{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b862fd7-75a8-4818-95c2-5375dde4b345",
   "metadata": {},
   "source": [
    "## Building the dataset that will be input into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda6c879-2ecc-475a-9f90-99bccea780bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop-3.2.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.7-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "2024-01-14 22:35:08,611 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-01-14 22:35:09,683 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2024-01-14 22:35:09,683 WARN util.Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "2024-01-14 22:35:09,683 WARN util.Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "2024-01-14 22:35:09,684 WARN util.Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "2024-01-14 22:35:11,505 WARN spark.ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"Building dataset\").config(\"hive.metastore.uris\", \"thrift://amok:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "spark.sql(\"USE 2023_11_02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54391b05-47b8-4d4b-b943-eda194771996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import lit,col\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "#from boruta import BorutaPy\n",
    "#from fredapi import Fred\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import csv\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from CreateDataset import get_features_all_stocks_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba39e2b3-07f5-4fd9-adf5-735d881bf565",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "2024-01-14 22:22:39,276 WARN scheduler.TaskSetManager: Stage 1 contains a task of very large size (2969 KiB). The maximum recommended task size is 1000 KiB.\n",
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 22:22:47,296 WARN scheduler.TaskSetManager: Stage 6 contains a task of very large size (2969 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 22:22:53,022 WARN scheduler.TaskSetManager: Stage 12 contains a task of very large size (2969 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "238211\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "#full_series_stocks = get_full_series_stocks(df) #this gets the stocks that have data since 2001\n",
    "#filtered_df = df[df['fsym_id'].isin(full_series_stocks)]\n",
    "df['Implosion_Start_Date'] = pd.to_datetime(df['Implosion_Start_Date'])\n",
    "df['Implosion_End_Date'] = pd.to_datetime(df['Implosion_End_Date'])\n",
    "df = get_features_all_stocks_seq(df, all_feats=False) #get stocks that have data in the ff_advanced_der_af for all years, not just prices (or not even)\n",
    "#for boruta, maybe treat each column separately 22*10 features?\n",
    "#df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1a2e0bd-1dac-455b-bca0-485707f1f52c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "| fsym_id|      date|\n",
      "+--------+----------+\n",
      "|B01DPB-R|1998-12-31|\n",
      "|B01DPB-R|1994-12-31|\n",
      "|B01DPB-R|1999-12-31|\n",
      "|B01DPB-R|2004-12-31|\n",
      "|B01DPB-R|2003-12-31|\n",
      "|B01DPB-R|2002-12-31|\n",
      "|B01DPB-R|2005-12-31|\n",
      "|B01DPB-R|1995-12-31|\n",
      "|B01DPB-R|1996-12-31|\n",
      "|B01DPB-R|1997-12-31|\n",
      "|B01DPB-R|2000-12-31|\n",
      "|B01DPB-R|2001-12-31|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT fsym_id, date FROM FF_ADVANCED_DER_AF WHERE fsym_id = 'B01DPB-R'\").show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d30d429c-7d3d-4325-bc80-2471074b13ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 16:15:36,148 WARN scheduler.TaskSetManager: Stage 18 contains a task of very large size (2969 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 24:=================================================>    (183 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(df.filter(df['label'] == 1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91e7c04c-6dc4-4ac2-a544-5e3c187cfaaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63645eaf-dfa5-43d5-ab36-e94c6ced8c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def train_test_split(df):\n",
    "    train, test = df.randomSplit([0.7,0.3])\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def confusion_matrix_pandas(df):\n",
    "    df = df.toPandas()\n",
    "    cm = confusion_matrix(df['label'], df['prediction'])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def model_testing(df):\n",
    "    df = df.filter(reduce(lambda acc, column: acc & (F.size(col(column)) == 22), df.columns[1:-1], lit(True)))\n",
    "\n",
    "    #need to decide whether to only include stocks that started from 2000, or include just from e.g. 2019\n",
    "    #temporary measure - replace with 0\n",
    "    #try imputer?\n",
    "    #look into masking\n",
    "    \n",
    "    print(\"Number of records: \", df.count())\n",
    "    features = df.columns[1:-1]\n",
    "    list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "    for f in features:\n",
    "        df = df.withColumn(f, list_to_vector_udf(f))\n",
    "    df.show(2)\n",
    "    train_df, test_df = train_test_split(df)\n",
    "    \n",
    "    vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features_vector\")\n",
    "    train_df = vector_assembler.transform(train_df)\n",
    "    test_df = vector_assembler.transform(test_df)\n",
    "    \n",
    "    lr = LogisticRegression(featuresCol=\"features_vector\", labelCol=\"label\")\n",
    "    rf = RandomForestClassifier(featuresCol='features_vector', labelCol='label')\n",
    "    gbt = GBTClassifier(featuresCol = 'features_vector', labelCol='label')\n",
    "    models = [rf]\n",
    "    model_names = ['random forest']\n",
    "    \n",
    "    for model, model_name in zip(models, model_names):\n",
    "        if model_name == 'random forest':\n",
    "            paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(model.numTrees, [100]) \\\n",
    "                .addGrid(model.maxDepth, [15]) \\\n",
    "                .addGrid(model.minInstancesPerNode, [5]) \\\n",
    "                .build()\n",
    "        elif model_name == 'logistic regression':\n",
    "            paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.1]) \\\n",
    "                .addGrid(lr.elasticNetParam, [0.5]) \\\n",
    "                .addGrid(lr.maxIter, [100]) \\\n",
    "                .build()\n",
    "        else:\n",
    "            paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(gbt.maxDepth, [10]) \\\n",
    "                .addGrid(gbt.maxBins, [32]) \\\n",
    "                .addGrid(gbt.maxIter, [20]) \\\n",
    "                .build()\n",
    "\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "        crossval = CrossValidator(estimator=model,\n",
    "                                  estimatorParamMaps=paramGrid,\n",
    "                                  evaluator=evaluator,\n",
    "                                  numFolds=5, parallelism=12)\n",
    "\n",
    "        cvModel = crossval.fit(train_df)\n",
    "\n",
    "        avg_metrics = cvModel.avgMetrics\n",
    "        print(model_name.upper())\n",
    "\n",
    "        for i, acc in enumerate(avg_metrics):\n",
    "            print(f\"Fold {i + 1} - Validation Accuracy: {acc}\")\n",
    "\n",
    "        best_model = cvModel.bestModel\n",
    "\n",
    "        predictions = best_model.transform(test_df)\n",
    "\n",
    "        confusion_matrix_pandas(predictions.select('label', 'prediction'))\n",
    "        \n",
    "def basic_test(df):\n",
    "    df = df.filter(reduce(lambda acc, column: acc & (F.size(col(column)) == 22), df.columns[1:-1], lit(True)))\n",
    "    print(\"Number of records: \", df.count())\n",
    "    features = df.columns[1:-1]\n",
    "    list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "    for f in features:\n",
    "        df = df.withColumn(f, list_to_vector_udf(f))\n",
    "    df.show(2)\n",
    "    train_df, test_df = train_test_split(df)\n",
    "    \n",
    "    vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features_vector\")\n",
    "    train_df = vector_assembler.transform(train_df)\n",
    "    test_df = vector_assembler.transform(test_df)\n",
    "    \n",
    "    lr = LogisticRegression(featuresCol=\"features_vector\", labelCol=\"label\")\n",
    "    rf = RandomForestClassifier(featuresCol='features_vector', labelCol='label')\n",
    "    gbt = GBTClassifier(featuresCol = 'features_vector', labelCol='label')\n",
    "    models = [lr, rf, gbt]\n",
    "    model_names = ['logistic regression']\n",
    "    \n",
    "    for model, model_name in zip(models, model_names):\n",
    "        if model_name == 'boosted trees':\n",
    "            paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [10])\n",
    "             .addGrid(gbt.maxIter, [50])\n",
    "             .build())\n",
    "        elif model_name == 'random forest':\n",
    "            paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [5]) #5,10,15\n",
    "             .addGrid(rf.numTrees, [20]) #20,50,100\n",
    "             .build())\n",
    "        else:\n",
    "            paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.maxIter, [10])  # Number of iterations #10,50,100\n",
    "             .addGrid(lr.regParam, [0.01])  # Regularization parameter #0.01,0.1,0.5\n",
    "             .addGrid(lr.elasticNetParam, [0.0])  # Elastic net parameter (0 for L2, 1 for L1) 0.0,0.5,1.0\n",
    "             .build())\n",
    "    \n",
    "\n",
    "\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "        \n",
    "        crossval = CrossValidator(estimator = model, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "        \n",
    "        model = crossval.fit(train_df)\n",
    "        \n",
    "        best_model = model.bestModel\n",
    "\n",
    "        print(model_name.upper())\n",
    "\n",
    "        predictions = best_model.transform(test_df)\n",
    "\n",
    "        confusion_matrix_pandas(predictions.select('label', 'prediction'))\n",
    "        \n",
    "        recall = evaluator.evaluate(predictions)\n",
    "        print(f\"Recall: {recall}\")\n",
    "    \n",
    "\n",
    "\n",
    "#basic_test(df)\n",
    "#test_pandas(df)\n",
    "#model_testing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9eb392cd-c08f-4884-bf9d-80ef508a15ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import csv\n",
    "    \n",
    "def model_training(df, classifier):\n",
    "    df = df.filter(reduce(lambda acc, column: acc & (F.size(col(column)) == 22), df.columns[1:-1], lit(True)))\n",
    "    print(\"Number of records: \", df.count())\n",
    "    \n",
    "    features = df.columns[1:-1]\n",
    "\n",
    "    list_to_vector_udf = F.udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "    for f in features:\n",
    "        df = df.withColumn(f, list_to_vector_udf(f))\n",
    " \n",
    "    train_df, test_df = train_test_split(df)\n",
    "\n",
    "    vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features_vector\")\n",
    "    train_df = vector_assembler.transform(train_df)\n",
    "    test_df = vector_assembler.transform(test_df)\n",
    "\n",
    "    if classifier == 'LogisticRegression':\n",
    "        param_space = {\n",
    "            'regParam': hp.uniform('regParam', 0.01, 1.0),\n",
    "            'elasticNetParam': hp.uniform('elasticNetParam', 0.0, 1.0)\n",
    "        }\n",
    "        classifier_instance = LogisticRegression(featuresCol=\"features_vector\", labelCol=\"label\")\n",
    "    elif classifier == 'RandomForest':\n",
    "        param_space = {\n",
    "            'maxBins': hp.quniform('maxBins', 16, 32, 1),\n",
    "            'maxDepth': hp.quniform('maxDepth', 20, 30, 1)\n",
    "        }\n",
    "        classifier_instance = RandomForestClassifier(featuresCol='features_vector', labelCol='label')\n",
    "    elif classifier == 'GBT':\n",
    "        param_space = {\n",
    "            'maxDepth' : hp.quniform(\"maxDepth\", 3, 18, 1),\n",
    "            'maxBins': hp.quniform('maxBins', 16, 32, 1)\n",
    "        }\n",
    "        classifier_instance = GBTClassifier(featuresCol='features_vector', labelCol='label')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported classifier\")\n",
    "    \n",
    "    initial_model = classifier_instance\n",
    "    initial_model = initial_model.fit(train_df)\n",
    "\n",
    "    def cross_val_train(params):\n",
    "        classifier_instance.setParams(**params)\n",
    "        #evaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n",
    "        evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', metricName='f1')\n",
    "        train, val = train_df.randomSplit([0.9,0.1])\n",
    "        curr_model = classifier_instance.fit(train)\n",
    "        predictions = curr_model.transform(val)\n",
    "        val_metric = evaluator.evaluate(predictions)\n",
    "        return curr_model, val_metric\n",
    "#         crossval = CrossValidator(estimator=classifier_instance,\n",
    "#                                   estimatorParamMaps=[params],\n",
    "#                                   evaluator=evaluator,\n",
    "#                                   numFolds=5, parallelism=12)\n",
    "        \n",
    "#         cv_model = crossval.fit(train_df)\n",
    "#         predictions = cv_model.transform(train_df)\n",
    "#         val_metric = evaluator.evaluate(predictions)\n",
    "#         return cv_model, val_metric\n",
    "    \n",
    "    \n",
    "    def objective(params):\n",
    "        model, metric = cross_val_train(params)\n",
    "        return -metric\n",
    "\n",
    "    # Find the best hyperparameters\n",
    "    best_params = fmin(fn=objective, space=param_space, algo=tpe.suggest, max_evals=5)\n",
    "    print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "    # Train the model with the best hyperparameters\n",
    "    best_model, final_metric = cross_val_train(best_params)\n",
    "    if classifier!='LogisticRegression':\n",
    "        feature_importances = best_model.featureImportances\n",
    "        print(\"Feature Importances:\")\n",
    "        for i in range(len(feature_importances)):\n",
    "            print(\"Feature {}: {}\".format(i, feature_importances[i]))\n",
    "            \n",
    "    \n",
    "    predictions = best_model.transform(test_df)\n",
    "    final_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "    report = {\n",
    "        'accuracy': final_evaluator.evaluate(predictions, {final_evaluator.metricName: \"accuracy\"}),\n",
    "        'f1': final_evaluator.evaluate(predictions, {final_evaluator.metricName: \"f1\"}),\n",
    "        'weightedPrecision': final_evaluator.evaluate(predictions, {final_evaluator.metricName: \"weightedPrecision\"}),\n",
    "        'weightedRecall': final_evaluator.evaluate(predictions, {final_evaluator.metricName: \"weightedRecall\"}),\n",
    "        'precisionByLabel': final_evaluator.evaluate(predictions, {final_evaluator.metricName: \"precisionByLabel\"}),\n",
    "        'recallByLabel': final_evaluator.evaluate(predictions, {final_evaluator.metricName: \"recallByLabel\"}),\n",
    "        'logLoss': final_evaluator.evaluate(predictions, {final_evaluator.metricName: \"logLoss\"}),\n",
    "        'hammingLoss': final_evaluator.evaluate(predictions, {final_evaluator.metricName: \"hammingLoss\"})\n",
    "    }\n",
    "    initial_model_test_metric = final_evaluator.evaluate(initial_model.transform(test_df), {final_evaluator.metricName: \"f1\"})\n",
    "    print(\"Initial Model F1: \", initial_model_test_metric)\n",
    "    print(report)\n",
    "    fp = f'{classifier}_seq.csv'\n",
    "    with open(fp, 'w', newline='') as csv_file:\n",
    "        csv_writer = csv.writer(csv_file)\n",
    "        csv_writer.writerow(report.keys())\n",
    "        csv_writer.writerow(report.values())\n",
    "\n",
    "    confusion_matrix_pandas(predictions.select('label', 'prediction'))\n",
    "    \n",
    "    \n",
    "    \n",
    "# model_training(df, 'LogisticRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8882a899-98b5-4b95-ba1c-5de2a137a0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-14 22:30:15,459 WARN scheduler.TaskSetManager: Stage 30 contains a task of very large size (2969 KiB). The maximum recommended task size is 1000 KiB.\n",
      "2024-01-14 22:30:37,082 WARN scheduler.TaskSetManager: Lost task 11.0 in stage 35.0 (TID 2332) (192.168.2.4 executor 24): org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 16384 bytes of memory, got 0\n",
      "\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)\n",
      "\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.<init>(UnsafeInMemorySorter.java:128)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.<init>(UnsafeExternalSorter.java:164)\n",
      "\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.create(UnsafeExternalSorter.java:131)\n",
      "\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.add(ExternalAppendOnlyUnsafeRowArray.scala:125)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:162)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:188)\n",
      "\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:122)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage75.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "2024-01-14 22:30:40,392 ERROR scheduler.TaskSetManager: Task 11 in stage 35.0 failed 4 times; aborting job\n",
      "2024-01-14 22:30:40,425 WARN scheduler.TaskSetManager: Lost task 15.3 in stage 35.0 (TID 2521) (192.168.2.22 executor 25): TaskKilled (Stage cancelled)\n",
      "2024-01-14 22:30:40,511 WARN scheduler.TaskSetManager: Lost task 23.2 in stage 35.0 (TID 2520) (192.168.2.4 executor 24): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o685.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 35.0 failed 4 times, most recent failure: Lost task 11.3 in stage 35.0 (TID 2518) (192.168.2.22 executor 25): org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 16384 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.<init>(UnsafeInMemorySorter.java:128)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.<init>(UnsafeExternalSorter.java:164)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.create(UnsafeExternalSorter.java:131)\n\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.add(ExternalAppendOnlyUnsafeRowArray.scala:125)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:162)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:188)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:122)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage75.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2297)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1120)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1102)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1524)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1512)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:183)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 16384 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.<init>(UnsafeInMemorySorter.java:128)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.<init>(UnsafeExternalSorter.java:164)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.create(UnsafeExternalSorter.java:131)\n\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.add(ExternalAppendOnlyUnsafeRowArray.scala:125)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:162)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:188)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:122)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage75.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 108\u001b[0m\n\u001b[1;32m    105\u001b[0m     plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mActual\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    106\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m--> 108\u001b[0m train_seqs, test_seqs \u001b[38;5;241m=\u001b[39m \u001b[43mnn_prepare_seqs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m nn_training(train_seqs, test_seqs)\n",
      "Cell \u001b[0;32mIn[5], line 8\u001b[0m, in \u001b[0;36mnn_prepare_seqs\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnn_prepare_seqs\u001b[39m(df):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# df = df.filter(reduce(lambda acc, column: acc & (F.size(col(column)) == 22), df.columns[1:-1], F.lit(True)))\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# print(\"Number of records: \", df.count())\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     features \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# df = df.toPandas()\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# train_df = train_df.toPandas()\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# test_df = test_df.toPandas()\u001b[39;00m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:484\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m name | Bob\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;28mint\u001b[39m(truncate), vertical))\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o685.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 11 in stage 35.0 failed 4 times, most recent failure: Lost task 11.3 in stage 35.0 (TID 2518) (192.168.2.22 executor 25): org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 16384 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.<init>(UnsafeInMemorySorter.java:128)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.<init>(UnsafeExternalSorter.java:164)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.create(UnsafeExternalSorter.java:131)\n\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.add(ExternalAppendOnlyUnsafeRowArray.scala:125)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:162)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:188)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:122)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage75.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2297)\n\tat org.apache.spark.rdd.RDD.$anonfun$reduce$1(RDD.scala:1120)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.reduce(RDD.scala:1102)\n\tat org.apache.spark.rdd.RDD.$anonfun$takeOrdered$1(RDD.scala:1524)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1512)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:183)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:772)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.memory.SparkOutOfMemoryError: Unable to acquire 16384 bytes of memory, got 0\n\tat org.apache.spark.memory.MemoryConsumer.throwOom(MemoryConsumer.java:157)\n\tat org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:97)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.<init>(UnsafeInMemorySorter.java:128)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.<init>(UnsafeExternalSorter.java:164)\n\tat org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.create(UnsafeExternalSorter.java:131)\n\tat org.apache.spark.sql.execution.ExternalAppendOnlyUnsafeRowArray.add(ExternalAppendOnlyUnsafeRowArray.scala:125)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.fetchNextPartition(WindowExec.scala:162)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:188)\n\tat org.apache.spark.sql.execution.window.WindowExec$$anon$1.next(WindowExec.scala:122)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage75.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def nn_prepare_seqs(df):\n",
    "    # df = df.filter(reduce(lambda acc, column: acc & (F.size(col(column)) == 22), df.columns[1:-1], F.lit(True)))\n",
    "    # print(\"Number of records: \", df.count())\n",
    "    df.show(1)\n",
    "    features = df.columns[1:-1]\n",
    "    # list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "    # df = df.toPandas()\n",
    "    \n",
    "    \n",
    "    \n",
    "    # for f in features:\n",
    "    #     df = df.withColumn(f, list_to_vector_udf(f))\n",
    "    # train_df, test_df = train_test_split(df)\n",
    "    # train_df = train_df.toPandas()\n",
    "    # test_df = test_df.toPandas()\n",
    "    \n",
    "    train_seqs = []\n",
    "    for stock_id, group in train_df.groupby('fsym_id'):\n",
    "        seq_feats = group[features]\n",
    "        label = group['label']\n",
    "        feat_length = len(seq_feats.iloc[0])\n",
    "        date_length = len(seq_feats.iloc[0,0])\n",
    "        seq_array = np.zeros((feat_length, date_length))\n",
    "        for i, sublist in enumerate(seq_feats.columns):\n",
    "            seq_array[i, :] = np.array(seq_feats[sublist].iloc[0])\n",
    "        seq_array = seq_array.T\n",
    "        train_seqs.append((seq_array, label))\n",
    "        \n",
    "    test_seqs = []\n",
    "    for stock_id, group in test_df.groupby('fsym_id'):\n",
    "        seq_feats = group[features]\n",
    "        label = group['label']\n",
    "        feat_length = len(seq_feats.iloc[0])\n",
    "        date_length = len(seq_feats.iloc[0,0])\n",
    "        seq_array = np.zeros((feat_length, date_length))\n",
    "        for i, sublist in enumerate(seq_feats.columns):\n",
    "            seq_array[i, :] = np.array(seq_feats[sublist].iloc[0])\n",
    "        seq_array = seq_array.T\n",
    "        test_seqs.append((seq_array, label))\n",
    "    \n",
    "    return train_seqs, test_seqs\n",
    "\n",
    "def plot_model_performance(mdl, loss, metric):\n",
    "    x = pd.DataFrame(mdl.history).reset_index()\n",
    "    x = pd.melt(x, id_vars='index')\n",
    "    x['validation'] = (x['variable'].str[:4] == 'val_').replace({True:'validation',False:'training'})\n",
    "    x['loss'] = (x['variable'].str[-4:] == 'loss').replace({True:loss,False:metric})\n",
    "    g = sns.FacetGrid(x, col='loss', hue='validation',sharey=False)\n",
    "    g.map(sns.lineplot, 'index','value')\n",
    "    g.add_legend()\n",
    "    return g\n",
    "\n",
    "def nn_training(train_seqs, test_seqs):\n",
    "    train_X, train_y = zip(*train_seqs)\n",
    "    test_X, test_y = zip(*test_seqs)\n",
    "\n",
    "    # Convert lists to numpy arrays\n",
    "    train_X = np.array(train_X)\n",
    "    train_y = np.array(train_y)\n",
    "    test_X = np.array(test_X)\n",
    "    test_y = np.array(test_y)\n",
    "\n",
    "    # Define the neural network model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(train_X.shape[1], train_X.shape[2])),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    loss_fn = keras.losses.BinaryCrossentropy()\n",
    "    optimizer = keras.optimizers.Adam(\n",
    "        learning_rate=0.005\n",
    "    )\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    fit_model = model.fit(train_X, train_y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "    # plot_model_performance(fit_model, 'bin_cross_entropy','accuracy')\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_acc = model.evaluate(test_X, test_y)\n",
    "    print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "    # Make predictions on new data\n",
    "    predictions = model.predict(test_X)\n",
    "    for i in range(len(predictions)):\n",
    "        predictions[i] = 1 if predictions[i] >= 0.5 else 0\n",
    "    print(predictions)\n",
    "    \n",
    "    # pred_df = pd.DataFrame()\n",
    "    # pred_df['prediction'] = predictions\n",
    "    # pred_df['label'] = test_y\n",
    "    # confusion_matrix_pandas(pred_df)\n",
    "    cm = confusion_matrix(test_y, predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "train_seqs, test_seqs = nn_prepare_seqs(df)\n",
    "nn_training(train_seqs, test_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d2c79-f857-4688-84be-7965097c3047",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# import torch\n",
    "# import torch.autograd as autograd\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# import seaborn as sns\n",
    "# from pylab import rcParams\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib import rc\n",
    "# import matplotlib\n",
    "# from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# from sklearn.metrics import classification_report, confusion_matrix\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# from multiprocessing import cpu_count\n",
    "\n",
    "\n",
    "\n",
    "# class SequenceModel(nn.Module):\n",
    "#     def __init__(self, n_features, n_classes, n_hidden=256, n_layers=3):\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.lstm = nn.LSTM(\n",
    "#             input_size = n_features,\n",
    "#             hidden_size = n_hidden,\n",
    "#             batch_first = True,\n",
    "#             num_layers = n_layers, # Stack LSTMs\n",
    "#             dropout = 0.2  # This model works on a lot of regularisation\n",
    "#         )\n",
    "\n",
    "#         self.classifier = nn.Linear(n_hidden, n_classes)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         self.lstm.flatten_parameters()  # For distrubuted training\n",
    "\n",
    "#         _, (hidden, _) = self.lstm(x)\n",
    "#         # We want the output from the last layer to go into the final\n",
    "#         # regressor linear layer\n",
    "#         out = hidden[-1] \n",
    "\n",
    "#         return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b910d24-3885-4d00-857b-90ca055539c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "def train_tree(train_df):\n",
    "    rf = RandomForestClassifier(featuresCol='features_vector', labelCol='label', maxDepth=maxDepth, numTrees=numTrees)\n",
    "    model = rf.fit(train_df)\n",
    "    preds = model.transform()\n",
    "    \n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "def train_with_hyperopt(params, df):\n",
    "    numTrees = int(params['numTrees'])\n",
    "    maxDepth = int(params['maxDepth'])\n",
    "\n",
    "    model, f1_score = train_rf(numTrees, maxDepth, df)\n",
    "    loss = - f1_score\n",
    "    return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "def prepare_df():\n",
    "    df = df.filter(reduce(lambda acc, column: acc & (F.size(col(column)) == 22), df.columns[1:-1], lit(True)))\n",
    "    print(\"Number of records: \", df.count())\n",
    "    features = df.columns[1:-1]\n",
    "    list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "    for f in features:\n",
    "        df = df.withColumn(f, list_to_vector_udf(f))\n",
    "    df.show(2)\n",
    "    train_df, test_df = train_test_split(df)\n",
    "    \n",
    "    vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features_vector\")\n",
    "    train_df = vector_assembler.transform(train_df)\n",
    "    test_df = vector_assembler.transform(test_df)\n",
    "    return train_df, test_df\n",
    "\n",
    "# train_df, test_df = prepare_df(df)\n",
    "# train_with_hyperopt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f034c3-4ed9-400e-a07b-6d2c6be018d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(filtered_df.filter(F.col('label')==1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b9373-fceb-4cf1-9c66-aab50d165c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.filter(F.col('label')==1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1698c017-00e4-4208-8305-518dd1746e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c12702-06f6-442b-9787-3d03ef354eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
