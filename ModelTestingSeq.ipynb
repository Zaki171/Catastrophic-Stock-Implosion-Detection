{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b862fd7-75a8-4818-95c2-5375dde4b345",
   "metadata": {},
   "source": [
    "## Building the dataset that will be input into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda6c879-2ecc-475a-9f90-99bccea780bd",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBuilding dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhive.metastore.uris\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthrift://bialobog:9083\u001b[39m\u001b[38;5;124m\"\u001b[39m, conf\u001b[38;5;241m=\u001b[39mSparkConf())\u001b[38;5;241m.\u001b[39mgetOrCreate() \\\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# for local metastore (your private, invidivual database) add the following config to spark session\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mUSE 2023_04_01\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: org.apache.hadoop.hive.ql.metadata.HiveException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"Building dataset\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "spark.sql(\"USE 2023_04_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54391b05-47b8-4d4b-b943-eda194771996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import lit,col\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "#from boruta import BorutaPy\n",
    "#from fredapi import Fred\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import csv\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from CreateDataset import get_features_all_stocks_seq, get_full_series_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba39e2b3-07f5-4fd9-adf5-735d881bf565",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "#full_series_stocks = get_full_series_stocks(df) #this gets the stocks that have data since 2001\n",
    "#filtered_df = df[df['fsym_id'].isin(full_series_stocks)]\n",
    "df = get_features_all_stocks_seq(df) #get stocks that have data in the ff_advanced_der_af for all years, not just prices (or not even)\n",
    "#for boruta, maybe treat each column separately 22*10 features?\n",
    "#df.show()\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63645eaf-dfa5-43d5-ab36-e94c6ced8c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def train_test_split(df):\n",
    "    train, test = df.randomSplit([0.7,0.3], 22)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def confusion_matrix_pandas(df):\n",
    "    df = df.toPandas()\n",
    "    cm = confusion_matrix(df['label'], df['prediction'])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def model_testing(df):\n",
    "    df = df.filter(reduce(lambda acc, column: acc & (F.size(col(column)) == 22), df.columns[1:-1], lit(True)))\n",
    "\n",
    "    # average_lengths = df.agg(*[(F.avg(F.size(col(column))).alias(f'avg_length_{column}')) for column in df.columns[1:-1]])\n",
    "    \n",
    "    # test = padded_df.select('ff_non_oper_exp').filter(col('fsym_id')=='RTTY5P-R').collect()[0]\n",
    "    # print(test['ff_non_oper_exp'], len(test['ff_non_oper_exp']))\n",
    "\n",
    "    #need to decide whether to only include stocks that started from 2000, or include just from e.g. 2019\n",
    "    #temporary measure - replace with 0\n",
    "    #try imputer?\n",
    "    #look into masking\n",
    "    print(\"Number of records: \", df.count())\n",
    "    features = df.columns[1:-1]\n",
    "    list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "    for f in features:\n",
    "        df = df.withColumn(f, list_to_vector_udf(f))\n",
    "    df.show(2)\n",
    "    train_df, test_df = train_test_split(df)\n",
    "    \n",
    "    vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features_vector\")\n",
    "    train_df = vector_assembler.transform(train_df)\n",
    "    test_df = vector_assembler.transform(test_df)\n",
    "    \n",
    "    lr = LogisticRegression(featuresCol=\"features_vector\", labelCol=\"label\")\n",
    "    rf = RandomForestClassifier(featuresCol='features_vector', labelCol='label')\n",
    "    gbt = GBTClassifier(featuresCol = 'features_vector', labelCol='label')\n",
    "    models = [lr, rf, gbt]\n",
    "    model_names = ['random forest']\n",
    "    \n",
    "    for model, model_name in zip(models, model_names):\n",
    "        if model_name == 'random forest'\n",
    "            paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(model.numTrees, [50, 100, 150]) \\\n",
    "                .addGrid(model.maxDepth, [5, 10, 15]) \\\n",
    "                .addGrid(model.minInstancesPerNode, [1, 5, 10]) \\\n",
    "                .build()\n",
    "        elif model_name == 'logistic regression':\n",
    "            paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.01, 0.1, 1.0]) \\\n",
    "                .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "                .addGrid(lr.maxIter, [50, 100, 150]) \\\n",
    "                .build()\n",
    "        \n",
    "        paramGrid = ParamGridBuilder() \\\n",
    "            .addGrid(gbt.maxDepth, [5, 10, 15]) \\\n",
    "            .addGrid(gbt.maxBins, [16, 32]) \\\n",
    "            .addGrid(gbt.maxIter, [10, 20]) \\\n",
    "            .build()\n",
    "\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "        crossval = CrossValidator(estimator=model,\n",
    "                                  estimatorParamMaps=paramGrid,\n",
    "                                  evaluator=evaluator,\n",
    "                                  numFolds=5, parallelism=)\n",
    "\n",
    "        cvModel = crossval.fit(train_df)\n",
    "\n",
    "        avg_metrics = cvModel.avgMetrics\n",
    "        print(model_name.upper())\n",
    "\n",
    "        for i, acc in enumerate(avg_metrics):\n",
    "            print(f\"Fold {i + 1} - Validation Accuracy: {acc}\")\n",
    "\n",
    "        best_model = cvModel.bestModel\n",
    "\n",
    "        predictions = best_model.transform(test_df)\n",
    "\n",
    "        confusion_matrix_pandas(predictions.select('label', 'prediction'))\n",
    "        \n",
    "def basic_test(df):\n",
    "    df = df.filter(reduce(lambda acc, column: acc & (F.size(col(column)) == 22), df.columns[1:-1], lit(True)))\n",
    "    print(\"Number of records: \", df.count())\n",
    "    features = df.columns[1:-1]\n",
    "    list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "    for f in features:\n",
    "        df = df.withColumn(f, list_to_vector_udf(f))\n",
    "    df.show(2)\n",
    "    train_df, test_df = train_test_split(df)\n",
    "    \n",
    "    vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features_vector\")\n",
    "    train_df = vector_assembler.transform(train_df)\n",
    "    test_df = vector_assembler.transform(test_df)\n",
    "    \n",
    "    lr = LogisticRegression(featuresCol=\"features_vector\", labelCol=\"label\")\n",
    "    rf = RandomForestClassifier(featuresCol='features_vector', labelCol='label')\n",
    "    gbt = GBTClassifier(featuresCol = 'features_vector', labelCol='label')\n",
    "    models = [lr, rf, gbt]\n",
    "    model_names = ['logistic regression']\n",
    "    \n",
    "    for model, model_name in zip(models, model_names):\n",
    "        if model_name == 'boosted trees':\n",
    "            paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [10])\n",
    "             .addGrid(gbt.maxIter, [50])\n",
    "             .build())\n",
    "        elif model_name == 'random forest':\n",
    "            paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [10]) #5,10,15\n",
    "             .addGrid(rf.numTrees, [50]) #20,50,100\n",
    "             .build())\n",
    "        else:\n",
    "            paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.maxIter, [10])  # Number of iterations #10,50,100\n",
    "             .addGrid(lr.regParam, [0.01])  # Regularization parameter #0.01,0.1,0.5\n",
    "             .addGrid(lr.elasticNetParam, [0.0])  # Elastic net parameter (0 for L2, 1 for L1) 0.0,0.5,1.0\n",
    "             .build())\n",
    "    \n",
    "\n",
    "\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "        \n",
    "        crossval = CrossValidator(estimator = model, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "        \n",
    "        model = crossval.fit(train_df)\n",
    "        \n",
    "        best_model = model.bestModel\n",
    "\n",
    "        print(model_name.upper())\n",
    "\n",
    "        predictions = best_model.transform(test_df)\n",
    "\n",
    "        confusion_matrix_pandas(predictions.select('label', 'prediction'))\n",
    "        \n",
    "        recall = evaluator.evaluate(predictions)\n",
    "        print(f\"Recall: {recall}\")\n",
    "    \n",
    "\n",
    "\n",
    "basic_test(df)\n",
    "#test_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7337e-6ca2-4212-b9d0-17953b8998d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def test_model_pandas(df1, model_name):\n",
    "    df1 = df1.toPandas()\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "\n",
    "    \n",
    "    print(\"Number of records: \", len(df))\n",
    "    X = df.drop(exclude_columns, axis=1)\n",
    "    y = df['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    print(X_train)\n",
    "    if model_name == 'LogisticRegression':\n",
    "        model = LogisticRegression()\n",
    "        param_grid = {\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'C': [0.01, 0.1, 1.0, 10.0],\n",
    "            'max_iter': [100, 200, 300]\n",
    "        }\n",
    "    elif model_name == 'SVM':\n",
    "        model = SVC()\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'kernel': ['linear', 'rbf', 'poly'],\n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "    elif model_name == 'RandomForest':\n",
    "        model = RandomForestClassifier()\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [None, 5, 10, 20]\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name\")\n",
    "\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=5, scoring='roc_auc')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    predictions = best_model.predict(X_test)\n",
    "\n",
    "    auc = roc_auc_score(y_test, predictions)\n",
    "\n",
    "    print(f\"Area under the ROC curve (AUC) for {model_name}: {auc}\")\n",
    "    print(f\"Best model hyperparameters for {model_name}:\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "#test_model_pandas(df, 'LogisticRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f034c3-4ed9-400e-a07b-6d2c6be018d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_df.filter(F.col('label')==1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b9373-fceb-4cf1-9c66-aab50d165c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.filter(F.col('label')==1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1698c017-00e4-4208-8305-518dd1746e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
