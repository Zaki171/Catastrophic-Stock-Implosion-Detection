{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe610780-dc3d-4ac8-9dd1-914ed92696c0",
   "metadata": {},
   "source": [
    "# Approach 1 - 5-year Rolling Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6802cbd1-16cf-4e4a-8137-9894bdec78e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"5yr_rolling_window\").config(\"hive.metastore.uris\", \"thrift://amok:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "spark.sql(\"USE 2023_11_02\")"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 35,
>>>>>>> Stashed changes
   "id": "2886cd22-2c06-4882-b434-e5ed726788c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 34,
>>>>>>> Stashed changes
   "id": "694d984f-e6b5-42b9-afb9-1c9d98a7fd3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import os\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "main_dir = os.path.dirname(curr_dir)\n",
    "print(main_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce18a8c5-d4ca-4953-b814-cc3e0d85ac50",
   "metadata": {},
   "source": [
    "## Extract Yearly Time Series"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 31,
>>>>>>> Stashed changes
   "id": "61704c74-e53c-424b-9a8f-78ea6f9d34ad",
   "metadata": {
    "tags": []
   },
<<<<<<< Updated upstream
   "outputs": [],
=======
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop-3.2.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.7-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "2024-04-15 22:34:04,253 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-04-15 22:34:06,960 WARN spark.ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "from CreateDataset import get_tabular_dataset, get_feature_col_names, get_not_null_cols\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "# import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "def get_df(fn, all_feats=True, prediction=False, imploded_only=False):\n",
    "    df = get_tabular_dataset(fn, all_feats=all_feats, imploded_only=imploded_only, prediction=prediction, null_thresh=0.25) # this returns dataset of yearly time series with 1 if implosion occurred during that year\n",
    "    \n",
    "    df =df.toPandas()\n",
    "\n",
    "    return df\n",
    "\n",
    "# df = get_df(f'{main_dir}/data/imploded_stocks_price.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14006533-6114-4b28-b538-e813a069dd79",
   "metadata": {},
   "source": [
    "### Adding the returns data, which is extracted from a different table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d2a373-bf2d-4718-9513-5bb01e70a01f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "price_data = pd.read_csv('price_data.csv')\n",
    "price_data=price_data.sort_values(by=['fsym_id','date'])\n",
    "price_data['year'] = pd.to_datetime(price_data['date']).dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d9de1-409e-4ca8-a295-b97c373239a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "price_df = price_data.groupby(['fsym_id', 'year'], as_index=False).first()\n",
    "price_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02febb5-093c-438c-85e0-e5a28171ecf2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['year'] = pd.to_datetime(df['date']).dt.year\n",
    "# price_data['year'] = pd.to_datetime(price_data['date']).dt.year\n",
    "df = pd.merge(df, price_df[['fsym_id','year','adj_price', 'Market_Value']], on=['fsym_id', 'year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b20c6f3-472e-4df8-aa13-dd4f9401d217",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df['normalized_adj_price'] = df.groupby('fsym_id')['adj_price'].transform(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "def apply_log_returns(group):\n",
    "    group['log_return'] = np.log(group['adj_price'] / group['adj_price'].shift(1))\n",
    "    group['log_mv_return'] = np.log(group['Market_Value'] / group['Market_Value'].shift(1))\n",
    "    return group\n",
    "\n",
    "df = df.sort_values(by=['fsym_id', 'date'])\n",
    "df = df.groupby('fsym_id').apply(apply_log_returns).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc98e78-1401-41b2-9d1f-d1a27cfb4495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(['adj_price'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a0bb8c-bc5e-4b5a-b5e5-0152004b9eeb",
   "metadata": {},
   "source": [
    "### Optional - add second table of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31d1bf8-f5a9-4e2a-b03e-bb1db3eb3e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5156fe5e-4e52-4331-8ef8-28a84a39a335",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66041bcb-b8c0-48f6-851a-de61b395701e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.sort_values(by=['fsym_id','date']).head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5791a9f3-0901-4213-8696-f5d34951edb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_more_feats(df):\n",
    "    temp_df = pd.read_csv(f'{main_dir}/data/imploded_stocks_price.csv', index_col=False)\n",
    "    temp_df['Implosion_Start_Date'] = pd.to_datetime(temp_df['Implosion_Start_Date'])\n",
    "    temp_df['Implosion_End_Date'] = pd.to_datetime(temp_df['Implosion_End_Date'])\n",
    "    \n",
    "    col_names = get_not_null_cols(temp_df, 0.2, \"FF_ADVANCED_AF\")\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    spark_df.createOrReplaceTempView('temp')\n",
    "    col_string = ', '.join('a.' + item for item in col_names)\n",
    "    q = f\"\"\"SELECT t.*, {col_string} FROM temp t INNER JOIN FF_ADVANCED_AF a ON a.fsym_id = t.fsym_id AND a.date = t.date\"\"\"\n",
    "    bigger_df = spark.sql(q)\n",
    "    bigger_df=bigger_df.toPandas()\n",
    "    return bigger_df\n",
    "\n",
    "# test_df = add_more_feats(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd5f82e-8a4a-4fd1-b0de-457811e0a63c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df=test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3211c51d-3ce6-4366-bb23-1978788cde73",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8002642e-1983-409f-840b-65b70537abcd",
   "metadata": {},
   "source": [
    "### Dropping Highly Correlated Features"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 32,
>>>>>>> Stashed changes
   "id": "79a0fd65-c039-4a2c-8719-e2a20b8404b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def correlation_matrix(df):\n",
    "    # df =df.toPandas()\n",
    "    corr_df = df.drop(['date','fsym_id'], axis=1)\n",
    "    corr_mat = corr_df.corr().abs()\n",
    "    mask = np.triu(np.ones_like(corr_mat))\n",
    "    plt.figure(figsize=(50, 40))\n",
    "    sns.heatmap(corr_mat, annot=True, cmap='coolwarm', fmt=\".2f\", mask=mask)\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('corr_matrix_tab.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Variable pairs with absolute correlation above 0.7:\")\n",
    "    corr_dict = {}\n",
    "    for i in range(len(corr_mat.columns)):\n",
    "        for j in range(i+1, len(corr_mat.columns)):\n",
    "            if abs(corr_mat.iloc[i, j]) >= 0.7:\n",
    "                print(f\"{corr_mat.columns[i]} - {corr_mat.columns[j]}: {corr_mat.iloc[i, j]}\")\n",
    "                if corr_mat.columns[i] not in corr_dict.keys():\n",
    "                    corr_dict[corr_mat.columns[i]] = [corr_mat.columns[j]]\n",
    "                else:\n",
    "                    corr_dict[corr_mat.columns[i]].append(corr_mat.columns[j])\n",
    "                    \n",
    "    for k,v in corr_dict.items():\n",
    "        if len(corr_dict[k]) >= 1:\n",
    "            for col in corr_dict[k]:\n",
    "                if col in df.columns:\n",
    "                    df=df.drop(col,axis=1)\n",
    "    \n",
    "\n",
    "    print(df.columns)\n",
    "    return df\n",
    "\n",
    "    \n",
    "                \n",
    "# df=correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49e18b7-5408-4be3-a934-d5f58ccb9b16",
   "metadata": {},
   "source": [
    "### Imputation (Required before Boruta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b54dc-ccbc-43c6-bd3b-2d1d97595ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "feats = df.drop(['fsym_id','date', 'label'], axis=1).columns.tolist()\n",
    "df[feats] = df.groupby('fsym_id')[feats].transform(lambda x : x.fillna(method='ffill'))\n",
    "df[feats] = df.groupby('fsym_id')[feats].transform(lambda x: x.fillna(x.median()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dabc3c-8f21-4ba4-90ce-2c6a5aa0fd89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5220d45b-c73c-4295-b156-102c9c17966a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=df.drop('year', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66977c59-6064-49a5-bedc-4dd7de5013f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=df.sort_values(by=['fsym_id', 'date'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add42762-1106-40bd-b627-52011c80b59c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "q = \"\"\"SELECT date, ff_accr_exp FROM FF_ADVANCED_DER_AF WHERE fsym_id = 'B00FG1-R'\"\"\"\n",
    "q_df=spark.sql(q)\n",
    "q_df.show() #sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204614b8-8285-4201-b5f0-91efd69068aa",
   "metadata": {},
   "source": [
    "### Convert to Prediction Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3f8175-b763-4444-be76-6d1d999013c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def convert_to_prediction(df):\n",
    "    df_temp = spark.createDataFrame(df)\n",
    "    ws = Window.partitionBy('fsym_id').orderBy(F.col('date').desc())\n",
    "    df_temp = df_temp.withColumn('label', F.lag(F.col('label')).over(ws))\n",
    "    df_temp= df_temp.filter(F.col('label').isNotNull())\n",
    "    return df_temp.toPandas()\n",
    "\n",
    "df = convert_to_prediction(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199a8e16-df48-4b9d-b332-c47b00635593",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['fsym_id']=='B00FG1-R'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95853bbf-9daf-4836-84b5-712a24a8045f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(df[df['label']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df647b-9633-404a-a793-8d0a89755cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d6fbd4-5457-4cd4-8041-919df824ea7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3078c4e0-fde8-4d99-9d35-d7cc77d8306f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ad229-ea06-4054-bf46-78883ccf460c",
   "metadata": {},
   "source": [
    "### Narrow Down Feature Space with Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ca7819-9e6b-4e82-a54f-a1e5c069dc3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Boruta import BorutaPy\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def boruta_fs(train_df, model_name):\n",
    "    feats = train_df.drop(['fsym_id', 'date', 'label'], axis=1).columns.tolist()\n",
    "    train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "    train_df['year'] = train_df['date'].dt.year\n",
    "    train_df[feats] = train_df.groupby(['year'])[feats].transform(lambda x: x.fillna(x.mean()))\n",
    "    train_df=train_df.drop('year', axis=1)\n",
    "    feats = train_df.drop(['fsym_id', 'date', 'label'], axis=1).columns.tolist()\n",
    "    \n",
    "    # inf_mask = np.isinf(train_df).any(axis=1)\n",
    "\n",
    "    infinity_threshold = 1e10  # Adjust as needed based on your data\n",
    "\n",
    "    close_to_inf_mask = (np.abs(train_df[feats+['label']]) > infinity_threshold).any(axis=1)\n",
    "\n",
    "    filter_mask = close_to_inf_mask\n",
    "\n",
    "    train_df = train_df[feats+['label']][~filter_mask]\n",
    "\n",
    "    X_train = train_df.drop(['label'], axis=1)\n",
    "    y_train = train_df['label']\n",
    "\n",
    "\n",
    "    if model_name == 'rf':\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        counter = Counter(y_train)\n",
    "        estimate = counter[0] / counter[1]\n",
    "        print('Estimate: %.3f' % estimate)\n",
    "        model = xgb.XGBClassifier(scale_pos_weight=estimate)\n",
    "        \n",
    "    feat_selector = BorutaPy(model, n_estimators='auto', verbose=0, random_state=1)\n",
    "    feat_selector.fit(X_train, y_train)\n",
    "    features = X_train.columns.tolist()\n",
    "    print(\"Number of features: \", len(features) )\n",
    "    feature_ranks = list(zip(features, feat_selector.ranking_, feat_selector.support_))\n",
    "    selected_features = []\n",
    "    for feat in feature_ranks:\n",
    "        print(f\"Feature: {feat[0]}, Rank: {feat[1]}, Keep: {feat[2]}\")\n",
    "        if feat[1] <= 10:\n",
    "            selected_features.append(feat[0])\n",
    "    print(\"Selected features: \", selected_features)\n",
    "    return selected_features\n",
    "\n",
    "rf_feats = boruta_fs(df, 'xgb')\n",
    "# gbt_feats = boruta_fs(df, 'gbt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be297627-0587-4ad1-ad08-95df76029a0e",
   "metadata": {},
   "source": [
    "### RFE (for verification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de68830d-ce8e-4bb7-9c45-a6c25d761832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel,RFE\n",
    "\n",
    "\n",
    "def select_from_model(train_df, model_name):\n",
    "    feats = train_df.drop(['fsym_id', 'date', 'label'], axis=1).columns.tolist()\n",
    "    train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "    train_df['year'] = train_df['date'].dt.year\n",
    "    train_df[feats] = train_df.groupby(['year'])[feats].transform(lambda x: x.fillna(x.mean()))\n",
    "    train_df=train_df.drop('year', axis=1)\n",
    "    X_train = train_df.drop(['fsym_id', 'date', 'label'], axis=1)\n",
    "    y_train = train_df['label']\n",
    " \n",
    "    counter = Counter(y_train)\n",
    "    estimate = counter[0] / counter[1]\n",
    "    print('Estimate: %.3f' % estimate)\n",
    "    model = xgb.XGBClassifier(scale_pos_weight=estimate)\n",
    "    # model.fit(X_train, y_train)\n",
    "    \n",
    "    selector = RFE(model)\n",
    "    selector = selector.fit(X_train, y_train)\n",
    "    feats = X_train.columns.tolist()\n",
    "    for i in range(len(feats)):\n",
    "        print(feats[i], selector.ranking_[i])\n",
    "\n",
    "# select_from_model(df, 'XGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165fec0f-30ee-4693-ab1d-59e04ef9f11d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Open a file in write mode\n",
    "with open('results_5yr/boruta_results.txt', 'w') as file:\n",
    "    # Iterate over the elements of the list\n",
    "    for item in rf_feats:\n",
    "        # Write each element to the file\n",
    "        file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221769de-e63b-49fd-8882-074c5073d380",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df[rf_feats + ['fsym_id', 'date', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050cf4a2-6ddd-4e20-8bb5-4c24050b9b80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(df.columns) #22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e32939-414c-46c4-95e9-ad3838b8d5b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['fsym_id'] == 'FL275K-R'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd77d9b-2a24-4cb5-8e2c-4b9dba17261f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.1)\n",
    "selected_features = selector.fit_transform(df.drop(['fsym_id', 'label', 'date'], axis=1))\n",
    "mask = selector.get_support()\n",
    "remaining_feats = df.drop(['fsym_id', 'label', 'date'], axis=1).columns[mask]\n",
    "print(remaining_feats)\n",
    "df = df[remaining_feats.tolist() + ['fsym_id', 'label', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27209754-b712-4005-800e-572203ac2081",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_groups(df):\n",
    "    num_dict = {}\n",
    "    for fsym_id, group in df.groupby('fsym_id'):\n",
    "        if len(group) not in num_dict.keys():\n",
    "            num_dict[len(group)] = 1\n",
    "        else:\n",
    "            num_dict[len(group)]+=1\n",
    "    print(num_dict)\n",
    "    \n",
    "count_groups(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d31756-faf1-49d2-8352-a6f67662c5b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.set_index('date')\n",
    "df=df.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b5c3e-1753-4a12-b423-acbb55a27089",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered_df = df.groupby('fsym_id').filter(lambda x: len(x) >= 5) #restrict implosions to stocks that have sufficient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4db59a8-d548-4737-af6c-1a0509d3a449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[df['fsym_id']=='HY3QYL-R']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b0fa3a-ad88-4720-b33f-5b8abbe40e8a",
   "metadata": {},
   "source": [
    "### TSFEL Aggregation (takes a while to run!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ab2c0-6aea-489a-918e-3d7323977eb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tsfel\n",
    "\n",
    "def create_windows(df):\n",
    "    cfg = tsfel.get_features_by_domain(json_path='features_stat_editedv2.json') \n",
    "    result_dfs = [] \n",
    "    for fsym_id, group in df.groupby('fsym_id'):\n",
    "        for i in range(5, len(group)+1):\n",
    "            window = group.iloc[i-5:i]\n",
    "            print(window.head())\n",
    "            X = tsfel.time_series_features_extractor(cfg, window.drop(['fsym_id', 'label'], axis=1), verbose=0)\n",
    "            X['fsym_id'] = window['fsym_id'].iloc[0]\n",
    "            X['label'] = window['label'].iloc[-1]\n",
    "            X['end_date'] = window.index[-1]\n",
    "            result_dfs.append(X)\n",
    "            \n",
    "    final_result = pd.concat(result_dfs, ignore_index=True)\n",
    "    final_result.reset_index(drop=True, inplace=True)\n",
    "    return final_result\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "new_df = create_windows(filtered_df) #might have to restrict implosions to stocks that have sufficient data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf2311-33bc-4359-b2c6-ae37972f6d67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=df.reset_index()\n",
    "df[(df['fsym_id'] == 'B00FG1-R') & (df['date'] >= '2015-01-01') & (df['date'] <= '2020-12-31')]['log_return'].min() #verify TSFEL working as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf72565-e5aa-4b80-a42c-e1ba64e7d19a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd36d3e-0ce5-4c71-ad49-0ab62e0811e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(df[df['label']==1]))\n",
    "print(len(new_df[new_df['label']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d47d09-66f7-41ca-9576-d48f5b7a937c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(new_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fde90a6-cdf1-4fb5-a366-397714ac9d2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df.replace([np.inf, -np.inf], 0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e635741-21ef-4a2b-95f8-250ee587613a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "selector = VarianceThreshold(threshold=0.1)\n",
    "selected_features = selector.fit_transform(new_df.drop(['fsym_id', 'label', 'end_date'], axis=1))\n",
    "mask = selector.get_support()\n",
    "remaining_feats = new_df.drop(['fsym_id', 'label', 'end_date'], axis=1).columns[mask]\n",
    "print(remaining_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af717c2c-3d16-4728-ab5d-c89a8798e525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df = new_df[remaining_feats.tolist() + ['fsym_id', 'label', 'end_date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8567db0d-d536-498c-8764-606907df48b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27740f40-0f70-42f3-8613-96a982866460",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5afaec6-2723-4817-8b3d-594f87f8ca2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df.isnull().sum().nlargest(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6d5690-7dc4-4540-9dc8-ee8c239bea8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "null_percentages = (new_df.isnull().sum() / len(df))\n",
    "columns_to_drop = null_percentages[null_percentages > 0.2].index\n",
    "print(len(columns_to_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b1f338-4e3c-4576-9149-c1ca5ee1b577",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_df = df.drop(columns = columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a7ec37-b769-40f5-8df3-8dca2b8a4451",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exclude_cols = ['fsym_id', 'label', 'end_date']\n",
    "feats = new_df.drop(exclude_cols, axis=1).columns\n",
    "feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4e7f16-5db0-4856-9407-7c506fc8f8c7",
   "metadata": {},
   "source": [
    "### Drop correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef13ad87-b99e-49f0-a8ac-b36fa81493cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "corr_features = tsfel.correlated_features(new_df[feats])\n",
    "new_df.drop(corr_features, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe86f1f1-e438-4e4d-937a-f7e248634dee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(new_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd9768-4d04-4fd3-8984-2f6a6187b6d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da80e1-c63c-4cdf-9379-e165e8181131",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_with_null = new_df[new_df.isnull().any(axis=1)]\n",
    "print(len(rows_with_null))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd491c40-8835-4457-82f2-227302765495",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feats = new_df.drop(['fsym_id', 'end_date', 'label'], axis=1).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fe39d5-f0dc-4e2f-abff-a0b37c31522d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_df[feats] = new_df.groupby('fsym_id')[feats].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c922f9f-bdd4-42aa-bc3a-7f6d8ae4048c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(new_df))\n",
    "print(len(new_df[new_df['label']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c89050-81ed-4f09-9a81-4f60fffa436f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_df.to_csv('df_temp_both_feats_over_10_yrs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9053a07-a185-49a7-803c-170ba396ba5d",
   "metadata": {},
   "source": [
    "#### Add Industry Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984b6cb9-4074-43ae-b138-4b6b6f899bfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "temp_spark_df = spark.createDataFrame(new_df)\n",
    "temp_spark_df.createOrReplaceTempView('temp_table')\n",
    "\n",
    "q = \"\"\"SELECT sc.fsym_id, e.factset_industry_desc FROM temp_table t\n",
    "    LEFT JOIN sym_coverage sc ON sc.fsym_id = t.fsym_id\n",
    "    LEFT JOIN ff_sec_entity_hist c on c.fsym_id=sc.fsym_security_id\n",
    "    LEFT JOIN sym_entity_sector d on d.factset_entity_id=c.factset_entity_id\n",
    "    LEFT JOIN factset_industry_map e on e.factset_industry_code=d.industry_code\"\"\"\n",
    "ind_df = spark.sql(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342d3864-09b0-47e2-9870-001a4c9d1878",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ind_df = ind_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415e46fa-e9e1-44ba-86b3-4680d68435eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ind_df=ind_df.drop_duplicates(subset=['fsym_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069edca-1171-4aa5-afa4-613642c45a83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_with_industry = pd.merge(new_df, ind_df, on='fsym_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cba1cd1-b771-4c96-80f1-0de5840d997a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_with_industry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480c483a-040d-4563-bfa6-3920c7ddc3ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(df_with_industry))\n",
    "print(len(new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f17ce-cc8d-40d6-9039-eed1ae3f32a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "one_hot_encoded = pd.get_dummies(df_with_industry['factset_industry_desc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7b6141-23ff-4d24-8d6f-0e5aabb3479a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "grouped_industry = df_with_industry.groupby('factset_industry_desc').size().sort_values(ascending=False)\n",
    "top_10 = grouped_industry.head(10).index.tolist()\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad7058-cbc8-4c20-af92-011cdbadfc0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for col in one_hot_encoded.columns.tolist():\n",
    "    if col not in top_10:\n",
    "        one_hot_encoded.drop(col, axis=1, inplace=True)\n",
    "one_hot_encoded.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6c9bed-a086-495e-a8df-e8e165815be6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_with_industry = pd.concat([df_with_industry, one_hot_encoded], axis=1)\n",
    "df_with_industry.drop('factset_industry_desc', axis=1, inplace=True)\n",
    "df_with_industry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82803e62-c8ed-430b-bedc-8bfab31ad28e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(df_with_industry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32db228-89d4-4b19-828f-18da9ab4d802",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_with_industry.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c51be5-9e7b-49c3-999f-bb14d9038d96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(df_with_industry[(df_with_industry['Pharmaceuticals: Major'] == 1) & (df_with_industry['label'] == 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a40799-adbd-4f36-8f25-bd7e5c236669",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_df.to_csv('5yr_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d241b9b7-81c6-42aa-8ee9-2205a172ec9d",
   "metadata": {},
   "source": [
    "### Read in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3938b7a-332b-4dc0-a86b-46da3564e4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "new_df = pd.read_csv('5yr_final.csv', index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19d3ac2e-5aae-4128-b996-d2c80657bef0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unemployment_Rate_Absolute energy</th>\n",
       "      <th>Unemployment_Rate_Max</th>\n",
       "      <th>Unemployment_Rate_Mean absolute deviation</th>\n",
       "      <th>Unemployment_Rate_Mean absolute diff</th>\n",
       "      <th>Unemployment_Rate_Mean diff</th>\n",
       "      <th>Unemployment_Rate_Median absolute deviation</th>\n",
       "      <th>Unemployment_Rate_Median absolute diff</th>\n",
       "      <th>Unemployment_Rate_Median diff</th>\n",
       "      <th>Unemployment_Rate_Min</th>\n",
       "      <th>Unemployment_Rate_Negative turning points</th>\n",
       "      <th>...</th>\n",
       "      <th>log_return_Median absolute deviation</th>\n",
       "      <th>log_return_Median absolute diff</th>\n",
       "      <th>log_return_Median diff</th>\n",
       "      <th>log_return_Min</th>\n",
       "      <th>log_return_Negative turning points</th>\n",
       "      <th>log_return_Positive turning points</th>\n",
       "      <th>log_return_Zero crossing rate</th>\n",
       "      <th>fsym_id</th>\n",
       "      <th>label</th>\n",
       "      <th>end_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>185.75</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.056</td>\n",
       "      <td>0.800</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.85</td>\n",
       "      <td>-0.85</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.056015</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.644663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140.15</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.744</td>\n",
       "      <td>0.650</td>\n",
       "      <td>-0.650</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.60</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121953</td>\n",
       "      <td>0.195910</td>\n",
       "      <td>0.056015</td>\n",
       "      <td>0.644663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>110.47</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.528</td>\n",
       "      <td>0.425</td>\n",
       "      <td>-0.425</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>3.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121953</td>\n",
       "      <td>0.200871</td>\n",
       "      <td>-0.004961</td>\n",
       "      <td>0.644663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>92.07</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-0.350</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.30</td>\n",
       "      <td>-0.30</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121953</td>\n",
       "      <td>0.116992</td>\n",
       "      <td>0.098184</td>\n",
       "      <td>0.644663</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>111.96</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0.880</td>\n",
       "      <td>1.050</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121953</td>\n",
       "      <td>0.205832</td>\n",
       "      <td>-0.018808</td>\n",
       "      <td>0.384651</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 411 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unemployment_Rate_Absolute energy  Unemployment_Rate_Max  \\\n",
       "0                             185.75                    7.9   \n",
       "1                             140.15                    6.7   \n",
       "2                             110.47                    5.6   \n",
       "3                              92.07                    5.0   \n",
       "4                             111.96                    6.7   \n",
       "\n",
       "   Unemployment_Rate_Mean absolute deviation  \\\n",
       "0                                      1.056   \n",
       "1                                      0.744   \n",
       "2                                      0.528   \n",
       "3                                      0.472   \n",
       "4                                      0.880   \n",
       "\n",
       "   Unemployment_Rate_Mean absolute diff  Unemployment_Rate_Mean diff  \\\n",
       "0                                 0.800                       -0.800   \n",
       "1                                 0.650                       -0.650   \n",
       "2                                 0.425                       -0.425   \n",
       "3                                 0.350                       -0.350   \n",
       "4                                 1.050                        0.500   \n",
       "\n",
       "   Unemployment_Rate_Median absolute deviation  \\\n",
       "0                                          0.9   \n",
       "1                                          0.6   \n",
       "2                                          0.6   \n",
       "3                                          0.5   \n",
       "4                                          0.5   \n",
       "\n",
       "   Unemployment_Rate_Median absolute diff  Unemployment_Rate_Median diff  \\\n",
       "0                                    0.85                          -0.85   \n",
       "1                                    0.60                          -0.60   \n",
       "2                                    0.45                          -0.45   \n",
       "3                                    0.30                          -0.30   \n",
       "4                                    0.45                          -0.25   \n",
       "\n",
       "   Unemployment_Rate_Min  Unemployment_Rate_Negative turning points  ...  \\\n",
       "0                    4.7                                        0.0  ...   \n",
       "1                    4.1                                        0.0  ...   \n",
       "2                    3.9                                        0.0  ...   \n",
       "3                    3.6                                        0.0  ...   \n",
       "4                    3.6                                        1.0  ...   \n",
       "\n",
       "   log_return_Median absolute deviation  log_return_Median absolute diff  \\\n",
       "0                              0.000000                         0.056015   \n",
       "1                              0.121953                         0.195910   \n",
       "2                              0.121953                         0.200871   \n",
       "3                              0.121953                         0.116992   \n",
       "4                              0.121953                         0.205832   \n",
       "\n",
       "   log_return_Median diff  log_return_Min  log_return_Negative turning points  \\\n",
       "0                0.000000        0.644663                                 1.0   \n",
       "1                0.056015        0.644663                                 1.0   \n",
       "2               -0.004961        0.644663                                 1.0   \n",
       "3                0.098184        0.644663                                 1.0   \n",
       "4               -0.018808        0.384651                                 1.0   \n",
       "\n",
       "   log_return_Positive turning points  log_return_Zero crossing rate  \\\n",
       "0                                 0.0                            0.0   \n",
       "1                                 0.0                            0.0   \n",
       "2                                 1.0                            0.0   \n",
       "3                                 1.0                            0.0   \n",
       "4                                 2.0                            0.0   \n",
       "\n",
       "    fsym_id  label    end_date  \n",
       "0  B00FG1-R      0  2016-12-31  \n",
       "1  B00FG1-R      0  2017-12-31  \n",
       "2  B00FG1-R      0  2018-12-31  \n",
       "3  B00FG1-R      0  2019-12-31  \n",
       "4  B00FG1-R      0  2020-12-31  \n",
       "\n",
       "[5 rows x 411 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad029f7c-252f-43d2-83d3-47b65e7ec0c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_with_industry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306286b9-cb4c-431f-a133-ae5e8bd3c5a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Final Class Imbalance: \")\n",
    "print(len(new_df[new_df['label']==1])/len(new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc46b75b-f7f6-48a3-a7d9-88d1605f39be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(df_with_industry))\n",
    "print(len(new_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817982e0-a1d8-4d9e-b25b-1e66f2a8e3db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# train_file = 'data_if/train_df_if_window.csv'\n",
    "\n",
    "# # Check if the files exist\n",
    "# if not (os.path.exists(train_file)):\n",
    "#     # Write DataFrames to files\n",
    "#     new_df.to_csv(train_file, index=False)\n",
    "# else:\n",
    "#     # Read DataFrames from existing files\n",
    "#     print(\"works\")\n",
    "#     new_df = pd.read_csv(train_file, index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab51b45-1fa3-47b0-ae49-0a442d1b7395",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf2e298-6802-4a86-bfa7-1a5188637f04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60fc89-3e77-498f-9240-0515a00d75bf",
   "metadata": {},
   "source": [
    "### For Dataset Stats Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff9d251-9af7-4fbe-81e1-1cab57120fd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tr_df = new_df[new_df['end_date'] < '2019-01-01']\n",
    "tes_df = new_df[new_df['end_date'] >= '2019-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba34655-8680-4b2a-94f2-8707417f1a47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(tr_df))\n",
    "print(len(tr_df[tr_df['label']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300cf89a-f1f0-484d-a3cb-2aae282e2678",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(tes_df))\n",
    "print(len(tes_df[tes_df['label']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a7dbbc-f67d-4101-b2cb-96d4bb71c2c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def write_results(report, mcc, best_params, filename):\n",
    "    existing_mcc = None\n",
    "    try:\n",
    "        with open(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                if line.startswith('MCC:'):\n",
    "                    existing_mcc = float(line.split(':')[1])\n",
    "                    break\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    if existing_mcc is None or mcc > existing_mcc:\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(report)\n",
    "            f.write('\\nMCC: ' + str(mcc))\n",
    "            f.write('\\nHyperparams: ' + str(best_params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33feb075-0818-4e87-905f-a492b71ffb77",
   "metadata": {},
   "source": [
    "# Model Training (PySpark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017d6eaf-5f46-4b79-aa15-e3938b7030f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "#import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import lit,col\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost.spark import SparkXGBClassifier\n",
    "#from boruta import BorutaPy\n",
    "#from fredapi import Fred\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import csv\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import  BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "import csv\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from itertools import chain\n",
    "import math\n",
    "from pyspark.ml.classification import FMClassifier\n",
    "from hyperopt import SparkTrials, Trials\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "def t_t_split(df):\n",
    "    split_date = \"2019-01-01\"\n",
    "\n",
    "    # Step 3: Filter DataFrame to create train and test sets\n",
    "    train_df = df.filter(col(\"end_date\") < split_date)\n",
    "    test_df = df.filter(col(\"end_date\") >= split_date)\n",
    "    train_label_1_count = train_df.filter(col(\"label\") == 1).count()\n",
    "    test_label_1_count = test_df.filter(col(\"label\") == 1).count()\n",
    "    \n",
    "    # Printing the counts\n",
    "    print(\"Length of train/test: \", train_df.count()/(train_df.count() + test_df.count()))\n",
    "    print(\"Train DataFrame - Number of rows where label=1:\", train_label_1_count)\n",
    "    print(\"Test DataFrame - Number of rows where label=1:\", test_label_1_count)\n",
    "    \n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def feat_analysis(model, features, classifier_name):\n",
    "    feature_importances = model.featureImportances\n",
    "    feature_importances = feature_importances.toArray()\n",
    "    sorted_idx = np.argsort(feature_importances)[::-1][:20]  # Selecting top 20 indices\n",
    "    sorted_features = [features[i] for i in sorted_idx]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(sorted_idx)), feature_importances[sorted_idx], align=\"center\")\n",
    "    plt.xticks(range(len(sorted_idx)), sorted_features, rotation=45, ha=\"right\")\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"Feature Importance\")\n",
    "    plt.title(f\"Top 20 Feature Importances ({classifier_name})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'results_5yr/{classifier_name}_feature_importances.png')\n",
    "    \n",
    "    \n",
    "def model_training(df, classifier):\n",
    "    \n",
    "    print(\"Number of records: \", df.count())\n",
    "    \n",
    "    features = [c for c in df.columns if c!='fsym_id' and c!='end_date' and c!='label']\n",
    "    \n",
    "    train_df, test_df = t_t_split(df)\n",
    "    val_df = train_df[(train_df['end_date'] < '2019-01-01') & (train_df['end_date'] >= '2017-01-01')]\n",
    "    train_df2 = train_df[(train_df['end_date'] < '2017-01-01') ]\n",
    "    print(\"val/train: \", val_df.count()/train_df.count())\n",
    "    \n",
    "        \n",
    "    def compute_weights(train_df):\n",
    "        y_collect = train_df.select(\"label\").groupBy(\"label\").count().collect()\n",
    "        unique_y = [x[\"label\"] for x in y_collect]\n",
    "        total_y = sum([x[\"count\"] for x in y_collect])\n",
    "        unique_y_count = len(y_collect)\n",
    "        bin_count = [x[\"count\"] for x in y_collect]\n",
    "\n",
    "        class_weights_spark = {i: ii for i, ii in zip(unique_y, total_y / (unique_y_count * np.array(bin_count)))}\n",
    "        print(class_weights_spark)\n",
    "        mapping_expr = F.create_map([F.lit(x) for x in chain(*class_weights_spark.items())])\n",
    "        train_df = train_df.withColumn(\"weight\", mapping_expr.getItem(F.col(\"label\")))\n",
    "        return train_df\n",
    "    \n",
    "    def mcc(predictions):\n",
    "        tp= predictions[(predictions.label == 1) & (predictions.prediction == 1)].count()\n",
    "       # True Negatives\n",
    "        tn= predictions[(predictions.label == 0) & (predictions.prediction == 0)].count()\n",
    "       # False Positives\n",
    "        fp= predictions[(predictions.label == 0) & (predictions.prediction == 1)].count()\n",
    "       # False Negatives\n",
    "        fn= predictions[(predictions.label == 1) & (predictions.prediction == 0)].count()\n",
    "        if ((tp + fp) * (tp + fn) * (fp + tn) * (tn + fn))==0:\n",
    "            return 0\n",
    "        MCC = (tp * tn - fp * fn) / math.sqrt((tp + fp) * (tp + fn) * (fp + tn) * (tn + fn))\n",
    "        return MCC\n",
    "\n",
    "        \n",
    "    train_df = compute_weights(train_df)\n",
    "    train_df2 = compute_weights(train_df2)\n",
    "    \n",
    "    vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features_vector\")\n",
    "    train_df = vector_assembler.transform(train_df)\n",
    "    test_df = vector_assembler.transform(test_df)\n",
    "    train_df2 = vector_assembler.transform(train_df2)\n",
    "    val_df = vector_assembler.transform(val_df)\n",
    "\n",
    "    if classifier == 'LogisticRegression':\n",
    "        param_space = {\n",
    "            'regParam': hp.uniform('regParam', 0.01, 1.0),\n",
    "            'elasticNetParam': hp.uniform('elasticNetParam', 0.0, 1.0)\n",
    "        }\n",
    "        classifier_instance = LogisticRegression(featuresCol=\"features_vector\", labelCol=\"label\", weightCol='weight')\n",
    "    elif classifier == 'RandomForest':\n",
    "        param_space = {\n",
    "            'maxBins': hp.quniform('maxBins', 16, 100, 1),\n",
    "            'maxDepth': hp.quniform('maxDepth', 3, 30, 1),\n",
    "            'minInstancesPerNode': hp.quniform('minInstancesPerNode', 1, 10, 1),\n",
    "            'minInfoGain': hp.uniform('minInfoGain', 0.0, 1.0)\n",
    "        }\n",
    "        classifier_instance = RandomForestClassifier(featuresCol='features_vector', labelCol='label', weightCol='weight', seed=42)\n",
    "    elif classifier == 'GBT':\n",
    "        param_space = {\n",
    "            'maxDepth' : hp.quniform(\"maxDepth\", 3, 30, 1),\n",
    "            'maxBins': hp.quniform('maxBins', 16, 100, 1),\n",
    "            'minInstancesPerNode': hp.quniform('minInstancesPerNode', 1, 10, 1),\n",
    "            'minInfoGain': hp.uniform('minInfoGain', 0.0, 1.0)\n",
    "            \n",
    "        }\n",
    "        classifier_instance = GBTClassifier(featuresCol='features_vector', labelCol='label', weightCol='weight', seed=42)\n",
    "        \n",
    "    elif classifier == 'MLP':\n",
    "        param_space = {\n",
    "            'layers': hp.choice('layers', [ [len(features), len(features)//2, 2]]),\n",
    "            'blockSize': hp.choice('blockSize', [128]),\n",
    "            'stepSize': hp.uniform('stepSize', 0.03, 0.3)\n",
    "        }\n",
    "        classifier_instance = MultilayerPerceptronClassifier(layers=[len(features), len(features)//2, 2], seed=123, featuresCol='features_vector', labelCol='label')\n",
    "        curr_model = classifier_instance.fit(train_df)\n",
    "        predictions = curr_model.transform(test_df)\n",
    "        val_metric = mcc(predictions)\n",
    "        print(val_metric)\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported classifier\")\n",
    "\n",
    "    \n",
    "\n",
    "    def cross_val_train(params):\n",
    "        classifier_instance.setParams(**params)\n",
    "        curr_model = classifier_instance.fit(train_df2)\n",
    "        predictions = curr_model.transform(val_df)\n",
    "        val_metric = mcc(predictions)\n",
    "        return curr_model, val_metric\n",
    "\n",
    "    \n",
    "    def objective(params):\n",
    "        print(params)\n",
    "        model, metric = cross_val_train(params)\n",
    "        return -metric\n",
    "\n",
    "    # Find the best hyperparameters\n",
    "    best_params = fmin(fn=objective, space=param_space, algo=tpe.suggest, max_evals=100, trials = Trials(),  early_stop_fn=no_progress_loss(15))\n",
    "    print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "    # Train the model with the best hyperparameters\n",
    "    # best_model, final_metric = cross_val_train(best_params)\n",
    "    classifier_instance.setParams(**best_params)\n",
    "    best_model = classifier_instance.fit(train_df)\n",
    "            \n",
    "    \n",
    "    predictions = best_model.transform(test_df)\n",
    "    true = predictions.select('label').toPandas()\n",
    "    preds = predictions.select('prediction').toPandas()\n",
    "    report = classification_report(true, preds)\n",
    "    mcc = matthews_corrcoef(true,preds)\n",
    "    \n",
    "    write_results(report, mcc, best_params, f'results_5yr/{classifier}_report_test')\n",
    "    \n",
    "    feat_analysis(best_model, features, classifier)\n",
    "    \n",
    "    cm = confusion_matrix(true, preds)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    return best_model, train_df, test_df\n",
    "    \n",
    "# model, train_df, test_df = model_training(spark_df, 'RandomForest')\n",
    "# features = [c for c in spark_df.columns if c!='fsym_id' and c!='end_date' and c!='label']\n",
    "# feat_analysis(model, features, 'RandomForest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b13bfc-458c-4c04-a013-b129e70b9bdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "new_df[new_df['label']==1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2764fd50-ca93-49b8-93e9-c1b8655a1f1a",
   "metadata": {},
   "source": [
    "# Model Training (scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5f8c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ad0b664-c1e3-4714-b5f5-a079b05e4276",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\zakit\\Documents\\final year project\\Stock-Implosion-Prediction-FYP\\fypvenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from sklearn import tree\n",
    "import shap\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from collections import Counter\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "from functools import reduce\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import numpy as np\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def feature_importances(model, features, model_name):\n",
    "    feature_importances = model.feature_importances_\n",
    "\n",
    "    sorted_idx = np.argsort(feature_importances)[::-1]  # Sorting in descending order\n",
    "    top_features = sorted_idx[:20]  # Selecting top 20 features\n",
    "    top_feature_importances = feature_importances[top_features]\n",
    "    top_sorted_features = [features[i] for i in top_features]\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(range(len(top_features)), top_feature_importances, align=\"center\")\n",
    "    plt.xticks(range(len(top_features)), top_sorted_features, rotation=45, ha=\"right\")\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"Feature Importance\")\n",
    "    plt.title(f\"Feature Importances for {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'results_5yr/{model_name}_feature_importances.png')\n",
    "    plt.show()\n",
    "\n",
    "    return top_sorted_features\n",
    "\n",
    "def perm_feat_importance(model, features, X_test, y_test, model_name):\n",
    "    perm_importance = permutation_importance(model, X_test, y_test)\n",
    "    sorted_idx = perm_importance.importances_mean.argsort()\n",
    "    top_features = sorted_idx[:20]\n",
    "    top_feature_importances = perm_importance.importances_mean[top_features]\n",
    "    top_sorted_features = [features[i] for i in top_features]\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.bar(range(len(top_features)), top_feature_importances, align=\"center\")\n",
    "    plt.xticks(range(len(top_features)), top_sorted_features, rotation=45, ha=\"right\")\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"Feature Importance\")\n",
    "    plt.title(f\"Permutation Feature Importances for {model_name}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'results_5yr/{model_name}_permutation_feature_importances.png')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_roc(y_test, y_probs):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "    auc_score = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, label=f'AUC = {auc_score:.2f}')\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Random')\n",
    "    plt.xlabel('False Positive Rate (FPR)')\n",
    "    plt.ylabel('True Positive Rate (TPR)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def model_testing(df, classifier):\n",
    "    seed = 42\n",
    "    df = df.set_index('end_date')\n",
    "    df = df.sort_index()\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "    train_df = df[df.index < '2017-01-01']\n",
    "    val_df = df[(df.index >= '2017-01-01') & (df.index < '2019-01-01')]\n",
    "    test_df = df[df.index >= '2019-01-01']\n",
    "    \n",
    "    X_train = train_df.drop(exclude_columns, axis=1)\n",
    "    y_train = train_df['label']\n",
    "    \n",
    "    X_val = val_df.drop(exclude_columns, axis=1)\n",
    "    y_val = val_df['label']\n",
    "    \n",
    "    X_test = test_df.drop(exclude_columns, axis=1)\n",
    "    y_test = test_df['label']\n",
    "    \n",
    "    full_train = df[df.index < '2019-01-01'].drop(exclude_columns, axis=1)\n",
    "    full_y = df[df.index < '2019-01-01']['label']\n",
    "    \n",
    "    print(f\"\"\"Class imbalances: Train: {len(train_df[train_df['label']==1])/len(train_df)}, Val: {len(val_df[val_df['label']==1])/len(val_df)},\n",
    "          Test: {len(test_df[test_df['label']==1])/len(test_df)} \"\"\")\n",
    "    \n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_test), y=y_test)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    print(class_weight_dict)\n",
    "    \n",
    "    if classifier == 'LogisticRegression':\n",
    "        param_space = {\n",
    "            'C': hp.uniform('C', 0.01, 1.0) }\n",
    "        classifier_instance = LogisticRegression(class_weight = class_weight_dict, solver='sag', random_state=seed) #stochastic average gradient\n",
    "        scaler = StandardScaler()\n",
    "        feats = X_train.columns\n",
    "        X_train[feats] = scaler.fit_transform(X_train[feats])\n",
    "        # X_test[feats] = scaler.transform(X_test[feats])\n",
    "        \n",
    "    elif classifier == 'RandomForest':\n",
    "        param_space = { \n",
    "            'n_estimators': hp.quniform('n_estimators', 100, 500, 1),\n",
    "            'max_depth': hp.quniform('max_depth', 5, 500, 1),\n",
    "            'max_features': hp.choice('max_features', [ 'sqrt','log2', None]),\n",
    "            'min_samples_leaf': hp.uniform('min_samples_leaf', 0, 0.5),\n",
    "            'min_samples_split' : hp.uniform ('min_samples_split', 0, 1)\n",
    "        }\n",
    "        classifier_instance = RandomForestClassifier(class_weight = class_weight_dict, random_state=seed)\n",
    "    elif classifier == 'XGB':\n",
    "        param_space = { 'n_estimators':hp.quniform('n_estimators',100,400,1),\n",
    "                        'max_depth':hp.quniform('max_depth',5,100,1),\n",
    "                       'eta': hp.quniform('eta', 0.025, 1, 0.025), #overfitting control\n",
    "                       'min_child_weight': hp.quniform('min_child_weight', 1, 50, 1),\n",
    "                        'subsample': hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "                        'gamma': hp.quniform('gamma', 0.5, 100, 0.05), #large gamma = more conservative\n",
    "                        'colsample_bytree': hp.quniform('colsample_bytree', 0.5, 1, 0.01),\n",
    "                       'scale_pos_weight' : hp.uniform('scale_pos_weight', 500, 1000)\n",
    "                      }\n",
    "        counter = Counter(y_train)\n",
    "        # estimate scale_pos_weight value\n",
    "        estimate = counter[0] / counter[1]\n",
    "        print('Estimate: %.3f' % estimate)\n",
    "        \n",
    "        classifier_instance = xgb.XGBClassifier(seed=seed, scale_pos_weight= estimate)\n",
    "    elif classifier == 'LGB':\n",
    "        param_space = {\n",
    "            'n_estimators': hp.quniform('n_estimators', 100, 400, 1),\n",
    "            'max_depth': hp.quniform('max_depth', 5, 200, 1),\n",
    "            'learning_rate': hp.uniform('learning_rate', 0.025, 0.5),\n",
    "            'subsample': hp.uniform('subsample', 0.5, 1),\n",
    "            'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1),\n",
    "            'reg_alpha': hp.uniform('reg_alpha', 0, 1),\n",
    "            'reg_lambda': hp.uniform('reg_lambda', 0, 1),\n",
    "            'scale_pos_weight' : hp.uniform('scale_pos_weight', 500, 1000)\n",
    "        }\n",
    "        \n",
    "        classifier_instance = lgb.LGBMClassifier(\n",
    "            random_state=seed, verbose=-1)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported classifier\")\n",
    "    \n",
    "    def set_params(classifier, params):\n",
    "        if classifier == 'RandomForest':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            # params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            # params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        elif classifier == 'GBT':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        elif classifier == 'XGB':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            # params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            # params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        elif classifier == 'LGB':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            # params['min_data_in_leaf'] = int(params['min_data_in_leaf'])\n",
    "            # params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            # params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        \n",
    "        else:\n",
    "            return params\n",
    "    \n",
    "    obj_scores = []\n",
    "        \n",
    "    def objective(params):\n",
    "        params = set_params(classifier, params)\n",
    "        classifier_instance.set_params(**params)\n",
    "        classifier_instance.fit(X_train, y_train)\n",
    "        # scores = cross_val_score(classifier_instance, full_train, full_y, cv=tscv, scoring='matthews_corrcoef')\n",
    "        y_pred = classifier_instance.predict(X_val)\n",
    "        # recall = recall_score(y_test, y_pred)\n",
    "        score = matthews_corrcoef(y_val, y_pred)\n",
    "        # obj_scores.append(scores.mean())\n",
    "        return -score\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    best_params = fmin(fn=objective, space=param_space, algo=tpe.suggest, max_evals=100, early_stop_fn=no_progress_loss(25))\n",
    "    print(best_params)\n",
    "    \n",
    "    best_params = set_params(classifier, best_params)\n",
    "    classifier_instance.set_params(**best_params)\n",
    "    classifier_instance.fit(full_train, full_y)\n",
    "    preds = classifier_instance.predict(X_test)\n",
    "    final_report = classification_report(y_test, preds)\n",
    "    print(final_report)\n",
    "    \n",
    "    preds_probs = classifier_instance.predict_proba(X_test)[:, 1]  # Probabilities for positive class\n",
    "    plot_roc(y_test, preds_probs)\n",
    "    \n",
    "    mcc = matthews_corrcoef(y_test, preds)\n",
    "    print(\"MCC: \", mcc)\n",
    "    write_results(final_report, mcc, best_params, f'results_5yr/{classifier}_report_test')\n",
    "    \n",
    "    return classifier_instance, X_train.columns.tolist(), X_test, y_test\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# model, feats, X_test, y_test = model_testing(new_df, 'XGB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf9d147-d72b-479e-9cb5-80cf34db7055",
   "metadata": {},
   "source": [
    "#### Save the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05673226-e388-4936-93c5-29ccd0ff8ff1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_filename = \"results_5yr/xgboost_best.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef5a542-4b40-47bd-b52f-ccf564ec1841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pickle.dump(model, open(model_filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f16461-9966-41cf-af16-0ce8eeea1fe9",
   "metadata": {},
   "source": [
    "#### Load and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b2f10fb-0ef5-48f5-9a9f-9a5b25addf83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_loaded = pickle.load(open(model_filename, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1d273a2-d788-4441-b421-308f49632c60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99     12523\n",
      "           1       0.19      0.61      0.28       114\n",
      "\n",
      "    accuracy                           0.97     12637\n",
      "   macro avg       0.59      0.79      0.64     12637\n",
      "weighted avg       0.99      0.97      0.98     12637\n",
      "\n",
      "0.32550268683052536\n"
     ]
    }
   ],
   "source": [
    "def test_model(df, model):\n",
    "    seed = 42\n",
    "    df = df.set_index('end_date')\n",
    "    df = df.sort_index()\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "    test_df = df[df.index >= '2019-01-01']\n",
    "    \n",
    "    X_test = test_df.drop(exclude_columns, axis=1)\n",
    "    y_test = test_df['label']\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    final_report = classification_report(y_test, preds)\n",
    "    print(final_report)\n",
    "    mcc = matthews_corrcoef(y_test, preds)\n",
    "    print(mcc)\n",
    "    \n",
    "    \n",
    "test_model(new_df, model_loaded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3db095-2523-43f6-a3d9-2c3fbf823b6c",
   "metadata": {},
   "source": [
    "### How many imploded stocks were detected, ignoring timing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46f28987-fe56-465c-ad9a-b0e135b7efb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4569\n",
      "114\n",
      "70\n",
      "132\n",
      "          preds  label\n",
      "fsym_id               \n",
      "B3L98G-R      2      0\n",
      "B55S8J-R      3      0\n",
      "B85SJP-R      3      0\n",
      "B9FB4Z-R      1      0\n",
      "BCM03C-R      1      0\n",
      "...         ...    ...\n",
      "RT40C3-R      1      0\n",
      "RV811W-R      2      0\n",
      "S0XSBX-R      3      0\n",
      "S19R21-R      1      0\n",
      "S5KZRM-R      1      0\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zakit\\AppData\\Local\\Temp\\ipykernel_23544\\751293077.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  test_df['preds'] = preds\n"
     ]
    }
   ],
   "source": [
    "def test_model(df, model):\n",
    "    seed = 42\n",
    "    df = df.set_index('end_date')\n",
    "    df = df.sort_index()\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "    test_df = df[df.index >= '2019-01-01']\n",
    "    \n",
    "    X_test = test_df.drop(exclude_columns, axis=1)\n",
    "    y_test = test_df['label']\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    test_df['preds'] = preds\n",
    "    \n",
    "    # print(test_df[test_df['fsym_id']=='BM31T8-R'])\n",
    "    \n",
    "    stock_df = test_df.groupby('fsym_id').agg({'preds': 'sum', 'label': 'sum'})\n",
    "    print(len(stock_df['preds']>1))\n",
    "    tp_stocks = stock_df[(stock_df['label']==1) & (stock_df['preds']>=1)]\n",
    "    fp_stocks = stock_df[(stock_df['label']==0) & (stock_df['preds']>=1)]\n",
    "    tn_stocks = stock_df[(stock_df['label']==0) & (stock_df['preds']==0)]\n",
    "    fn_stocks = stock_df[(stock_df['label']==1) & (stock_df['preds']==0)]\n",
    "    imploded_stocks = stock_df[stock_df['label']==1]\n",
    "    print(len(imploded_stocks))\n",
    "    print(len(tp_stocks))\n",
    "    print(len(fp_stocks))\n",
    "    print(fp_stocks.head(100))\n",
    "    \n",
    "test_model(new_df,model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff44d56-75a0-41e9-b99f-4c4b217c83e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# perm_feat_importance(model_loaded, feats, X_test, y_test, 'XGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc939c24-0e5d-44e3-a950-3eb98b3311ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feats = new_df.drop(['fsym_id', 'end_date', 'label'], axis=1).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caceffc6-f65d-4440-baf2-763184a42411",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# top_feats = feature_importances(model,feats, 'XGBbhbh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ffe0c-817f-4c2b-b5d9-fbc83a52743a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = new_df[top_feats+['end_date', 'fsym_id', 'label']]\n",
    "model2, feats2, X_test2, y_test2 = model_testing(df2, 'XGB')\n",
    "# top_feats2 = feature_importances(model2,feats2, 'XGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d4c78c-ad79-4177-baef-45dd2e74745d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def time_series_cross_val(df, model):\n",
    "    existing_model_params = model.get_params()\n",
    "    new_model = xgb.XGBClassifier(**existing_model_params)\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    exclude_columns = ['fsym_id', 'label', 'end_date']\n",
    "    \n",
    "    df=df.reset_index()\n",
    "        \n",
    "    df['end_date'] = pd.to_datetime(df['end_date'])# Initialize the preds column\n",
    "    \n",
    "    years = sorted(df['end_date'].dt.year.unique())\n",
    "    print(years)\n",
    "    \n",
    "    pred_imploded_per_year = {}\n",
    "    tps_ratio = {}\n",
    "    \n",
    "    for i in range(len(years)-1):\n",
    "        yr=years[i] \n",
    "        test_yr=years[i+1]\n",
    "        pred_yr=test_yr+1 #the year the implosion is actually predicted to occur\n",
    "        train_df = df[df['end_date'].dt.year<=yr]\n",
    "        X_train= train_df.drop(exclude_columns, axis=1)\n",
    "        y_train = train_df['label']\n",
    "        \n",
    "        test_df = df[df['end_date'].dt.year==test_yr]\n",
    "        X_test= test_df.drop(exclude_columns, axis=1)\n",
    "        y_test = test_df['label']\n",
    "        \n",
    "        new_model.fit(X_train, y_train)\n",
    "        y_pred = new_model.predict(X_test)\n",
    "        pred_imploded_per_year[pred_yr] = sum(y_pred==1)\n",
    "        if sum(y_test==1) == 0:\n",
    "            continue\n",
    "        tps_ratio[pred_yr] =  sum((y_pred == 1) & (y_test == 1))/ sum(y_test == 1)\n",
    "        \n",
    "    pred_imploded_df = pd.DataFrame(list(pred_imploded_per_year.items()), columns=['Year', 'Implosions'])\n",
    "    tps_df = pd.DataFrame(list(tps_ratio.items()), columns=['Year', 'Implosions'])\n",
    "    \n",
    "    df['year'] = df['end_date'].dt.year+1\n",
    "    real_imploded_df = df.groupby('year')['label'].sum()\n",
    "    real_imploded_df = pd.DataFrame({'Year': real_imploded_df.index, 'Implosions': real_imploded_df.values})\n",
    "    real_imploded_df = real_imploded_df[real_imploded_df['Year'] >= 2005]\n",
    "    \n",
    "    print(pred_imploded_per_year)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot predicted implosions\n",
    "    plt.plot(pred_imploded_df['Year'], pred_imploded_df['Implosions'], label='Predicted Implosions', marker='o')\n",
    "\n",
    "    # Plot real implosions\n",
    "    plt.plot(real_imploded_df['Year'], real_imploded_df['Implosions'], label='Real Implosions', marker='o')\n",
    "\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Implosions')\n",
    "    plt.title('Predicted vs Real Implosions per Year')\n",
    "    plt.legend()  # Show legend with labels\n",
    "    plt.grid(True)  # Add grid lines\n",
    "    plt.xticks(range(min(pred_imploded_df['Year']), max(pred_imploded_df['Year']) + 1))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(tps_df['Year'], tps_df['Implosions'], label='Proportion of Implosions Detected per Year', marker='o')\n",
    "    plt.xticks(range(min(tps_df['Year']), max(tps_df['Year']) + 1))\n",
    "    plt.show()\n",
    "    \n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     sns.barplot(x='Year', y='Implosions', data=pred_imploded_df)\n",
    "#     plt.xlabel('Year')\n",
    "#     plt.ylabel('Number of Implosions')\n",
    "#     plt.title('Number of Predicted Implosions per Year')\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "#     plt.figure(figsize=(12, 6))\n",
    "#     sns.barplot(x='Year', y='Implosions', data=real_imploded_df)\n",
    "#     plt.xlabel('Year')\n",
    "#     plt.ylabel('Number of Implosions')\n",
    "#     plt.title('Number of Real Implosions per Year')\n",
    "#     plt.show()\n",
    "    \n",
    "    \n",
    "        \n",
    "#         X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "#         y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "#         new_model.fit(X_train, y_train)\n",
    "#         preds = new_model.predict(X_test)\n",
    "        \n",
    "#         # Set the predictions in the result_df\n",
    "#         result_df.iloc[test_index, -1] = preds  # -1 refers to the last column 'preds'\n",
    "        \n",
    "        # cm = confusion_matrix(y_test, preds, labels=new_model.classes_)\n",
    "        # plt.figure(figsize=(8, 6))\n",
    "        # sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
    "        # plt.xlabel(\"Predicted\")\n",
    "        # plt.ylabel(\"Actual\")\n",
    "        # plt.title(\"Confusion Matrix\")\n",
    "        # plt.show()\n",
    "        \n",
    "# df_with_preds = time_series_cross_val(new_df, model_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a933c-00de-4ae5-9fa8-b2808fa4cb35",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f91649-1c5a-4f29-97a9-57280cb3efab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agg_funcs = {\n",
    "    'label': 'sum'  # Summing the 'label' column\n",
    "}\n",
    "\n",
    "mean_cols = [col for col in new_df.columns if col != 'label' and col != 'end_date' and col!='fsym_id']\n",
    "\n",
    "\n",
    "for col in mean_cols:\n",
    "    agg_funcs[col] = 'mean'  # Calculating mean for each column\n",
    "\n",
    "stocks_df = new_df.drop('end_date',axis=1).groupby('fsym_id').agg(agg_funcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9590ccbc-74db-43d3-8d51-3c08dcd66398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "z_scores = (stocks_df['ff_earn_yld_Mean'] - stocks_df['ff_earn_yld_Mean'].mean()) / stocks_df['ff_earn_yld_Mean'].std()\n",
    "threshold = 3\n",
    "stocks_df = stocks_df[abs(z_scores) <= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de3ac86-abbd-4958-bb95-47460cef555e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# # Assuming 'df' is your DataFrame and 'column_name' is the name of the column you want to plot\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "\n",
    "# plt.boxplot([stocks_df[stocks_df['label'] == 0]['ff_earn_yld_Mean'], \n",
    "#                 stocks_df[stocks_df['label'] == 1]['ff_earn_yld_Mean']],\n",
    "#                 labels=['Imploded', 'Non-Imploded'], vert=False, notch=True, patch_artist=True)\n",
    "\n",
    "# plt.xlabel('Label')  # Label for x-axis\n",
    "# plt.ylabel('Earnings Yields (Mean)')  # Label for y-axis\n",
    "# plt.title('Boxplot of Earnings Yields (Non-Implosion Years vs Implosion Years)')  # Title of the plot\n",
    "# plt.grid(True)  # Show grid\n",
    "# plt.legend()  \n",
    "# plt.savefig('results_5yr/earn_ylds_boxplot.png')# Show legend\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b062c02f-e8bf-425e-9e2f-e133a6206f39",
   "metadata": {},
   "source": [
    "## MLP Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e5d11-8da6-4a8c-ac9d-6b87385e76b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hyperopt import hp, fmin, tpe, Trials\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "\n",
    "def hyperopt_nn(df):\n",
    "    seed = 42\n",
    "    df = df.set_index('end_date')\n",
    "    df = df.sort_index()\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "    train_df = df[df.index < '2019-01-01']\n",
    "    test_df = df[df.index >= '2019-01-01']\n",
    "    \n",
    "    X_train = train_df.drop(exclude_columns, axis=1)\n",
    "    y_train = train_df['label']\n",
    "    \n",
    "    X_test = test_df.drop(exclude_columns, axis=1)\n",
    "    y_test = test_df['label']\n",
    "    \n",
    "    feats = X_train.columns\n",
    "    \n",
    "    space = {\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.0001), np.log(0.1)),\n",
    "        'batch_size': hp.choice('batch_size', [1024, 2048, 4096]),\n",
    "        'num_layers': hp.choice('num_layers', [2, 3, 4, 5]),\n",
    "        'num_neurons': hp.choice('num_neurons', [128, 256, 512, 1024, 2048, 4096]),\n",
    "        'dropout_rate': hp.uniform('dropout_rate', 0, 0.5),\n",
    "        'class_weight_0': hp.uniform('class_weight_0', 0.1, 5),  \n",
    "        'class_weight_1': hp.uniform('class_weight_1', 100, 800) \n",
    "    }\n",
    "\n",
    "    scaler=StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    X_train_df = pd.DataFrame(X_train, columns=feats)\n",
    "    X_test_df = pd.DataFrame(X_test, columns=feats)\n",
    "    # class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    # class_weight_dict = dict(enumerate(class_weights))\n",
    "    # print(class_weight_dict)\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "    # tscv = TimeSeriesSplit(n_splits=2)\n",
    "    \n",
    "    def time_series_cross_validation(params):\n",
    "        # Define the number of splits\n",
    "\n",
    "        metrics = []  # Store metrics for each fold\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Flatten(input_shape=(X_train.shape[1],)))\n",
    "\n",
    "        for _ in range(params['num_layers']):\n",
    "            model.add(tf.keras.layers.Dense(params['num_neurons'], activation='relu'))\n",
    "            model.add(tf.keras.layers.Dropout(params['dropout_rate']))\n",
    "\n",
    "        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "\n",
    "        class_weight_dict = {0: params['class_weight_0'], 1: params['class_weight_1']}\n",
    "        hist = model.fit(X_train, y_train, batch_size=params['batch_size'], epochs=1, class_weight=class_weight_dict,  verbose=0)\n",
    "\n",
    "        preds = model.predict(X_test, batch_size=params['batch_size'], verbose=0)\n",
    "        preds = np.where(preds >= 0.5, 1, 0)\n",
    "\n",
    "        metric = matthews_corrcoef(y_test, preds)\n",
    "        metrics.append(metric)\n",
    "        # obj_scores.append(scores.mean())\n",
    "        return model, metric\n",
    "\n",
    "        # return model, sum(metrics)/len(metrics)\n",
    "\n",
    "    def train_with_params(params):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Flatten(input_shape=(X_train.shape[1],)))\n",
    "\n",
    "        for _ in range(params['num_layers']):\n",
    "            model.add(tf.keras.layers.Dense(params['num_neurons'], activation='relu'))\n",
    "            model.add(tf.keras.layers.Dropout(params['dropout_rate']))\n",
    "\n",
    "        model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
    "        model.compile(optimizer=optimizer, loss='binary_crossentropy')\n",
    "\n",
    "        class_weight_dict = {0: params['class_weight_0'], 1: params['class_weight_1']}\n",
    "        hist = model.fit(X_train, y_train, batch_size=params['batch_size'], epochs=10, class_weight = class_weight_dict, callbacks = [early_stopping], verbose=0)\n",
    "        preds = model.predict(X_test, batch_size=params['batch_size'], verbose=0 )\n",
    "        for i in range(len(preds)):\n",
    "            preds[i] = 1 if preds[i] >=0.5 else 0\n",
    "            \n",
    "        metric = matthews_corrcoef(y_test, preds)\n",
    "        return model, metric\n",
    "    \n",
    "    \n",
    "    def objective(params):\n",
    "        model, metric = time_series_cross_validation(params)\n",
    "        return -metric\n",
    "\n",
    "\n",
    "\n",
    "    # Run hyperparameter optimization\n",
    "    trials = Trials()\n",
    "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "    \n",
    "    activation_map = {0: 'relu', 1: 'sigmoid'}\n",
    "    best_hyperparams = {\n",
    "        'learning_rate': best['learning_rate'],\n",
    "        'batch_size': [1024, 2048, 4096][best['batch_size']],\n",
    "        'num_layers': [2, 3, 4, 5][best['num_layers']],\n",
    "        'num_neurons': [128, 256, 512, 1024, 2048, 4096][best['num_neurons']],\n",
    "        'dropout_rate': best['dropout_rate'],\n",
    "        'class_weight_0': best['class_weight_0'],\n",
    "        'class_weight_1': best['class_weight_1']\n",
    "    }\n",
    "\n",
    "    print(\"Best hyperparameters:\", best_hyperparams)\n",
    "    best_model, best_metric = train_with_params(best_hyperparams)\n",
    "    print(f'Best metric: {best_metric}')\n",
    "    preds = best_model.predict(X_test, batch_size = best_hyperparams['batch_size'])\n",
    "    print(preds)\n",
    "    for i in range(len(preds)):\n",
    "        preds[i] = 1 if preds[i] >= 0.5 else 0\n",
    "    final_report = classification_report(y_test, preds)\n",
    "    mcc = matthews_corrcoef(y_test, preds)\n",
    "    \n",
    "    write_results(final_report, mcc, best_hyperparams, f'results_5yr/MLP_report')\n",
    "            \n",
    "    return best_model, X_train_df, X_test_df\n",
    "    \n",
    "model, X_train, X_test = hyperopt_nn(new_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f7c2a8-016a-41a2-a021-da8c92a4d9e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# new_df[new_df['fsym_id']=='VBKMCH-R'].head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ee099b-d1d9-473f-aecc-8c81f0f189d2",
   "metadata": {},
   "source": [
    "## SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8659b122-b7b5-4251-b4b3-cda7acfab10f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def shapley(df, model, model_name, plot_type):\n",
    "    exclude_columns = ['fsym_id', 'label', 'end_date']\n",
    "    train_df = df[df['end_date'] < '2019-01-01']\n",
    "    test_df = df[df['end_date'] >= '2019-01-01']\n",
    "    \n",
    "    test_df = remove_outliers_iqr(test_df, 'ff_earn_yld_Mean')\n",
    "    \n",
    "    X_train = train_df.drop(exclude_columns, axis=1)\n",
    "    y_train = train_df['label']\n",
    "    \n",
    "    # test_df = test_df[test_df['label']==1]\n",
    "    # test_df = test_df[test_df['fsym_id']=='VBKMCH-R']\n",
    "    # print(test_df.head())\n",
    "    \n",
    "    X_test = test_df.drop(exclude_columns, axis=1)\n",
    "    y_test = test_df['label']\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    print(preds)\n",
    "    \n",
    "    explainer = shap.Explainer(model)\n",
    "    # shap_values = explainer.shap_values(X_test)\n",
    "    shap_values = explainer(X_test)\n",
    "    print(shap_values.shape)\n",
    "    shap.initjs()\n",
    "    if plot_type == 'beeswarm':\n",
    "        shap.plots.beeswarm(shap_values, max_display=20, show=False)\n",
    "    elif plot_type == 'bar':\n",
    "        shap.plots.bar(shap_values, max_display=20, show=False)\n",
    "    elif plot_type == 'waterfall':\n",
    "        for i in range(len(shap_values)):\n",
    "            shap.plots.waterfall(shap_values[i])\n",
    "    elif plot_type == 'scatter':\n",
    "        shap.plots.scatter(shap_values[:, 'ff_earn_yld_Mean'], show=False)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'results_5yr/{model_name}_shap_{plot_type}_final.png')\n",
    "    \n",
    "    \n",
    "    \n",
    "    # shap.summary_plot(shap_values, X_test)\n",
    "    # shap.dependence_plot(\"ff_earn_yld_Mean\", shap_values, X_test,interaction_index=\"log_return_Min\")\n",
    "    # shap.decision_plot(explainer.expected_value, shap_values, X_test.columns)\n",
    "    \n",
    "    # print(shap_values.shape)\n",
    "    # shap.plots.beeswarm(shap_values)\n",
    "    # shap.summary_plot(shap_values,test,show=False)\n",
    "#     if plot_type == 'bar':\n",
    "#         shap.plots.bar(shap_values, max_display=20, show=False)\n",
    "#     elif plot_type == 'beeswarm':\n",
    "#         shap.plots.beeswarm(shap_values, max_display=20, show=False)\n",
    "        \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig(f'results_5yr/{model_name}_shap_{plot_type}_top20.png')\n",
    "    \n",
    "    \n",
    "# shapley(df2, model2, model_name='XGB', plot_type = 'beeswarm')\n",
    "shapley(new_df, model_loaded,  'XGB', 'scatter')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a13e92-a951-4a0c-9bd4-f4b38f0f39a8",
   "metadata": {},
   "source": [
    "## False Positives/Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9938e78b-a720-4512-b9ea-6a90ab073601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from CreateDataset import get_fund_data\n",
    "import math\n",
    "from datetime import timedelta\n",
    "\n",
    "def plotting_stocks_pandas(model, df):\n",
    "    df = df.set_index('end_date')\n",
    "    df = df.sort_index()\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "    test_df = df[df.index >= '2019-01-01']\n",
    "    \n",
    "    X_test = test_df.drop(exclude_columns, axis=1)\n",
    "    y_test = test_df['label']\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    test_df['preds'] = preds\n",
    "    \n",
    "    \n",
    "    imploded_stocks = test_df[(test_df['label'] == 0) & (test_df['preds'] == 1)]\n",
    "    # imploded_stocks2= imploded_stocks.reset_index()\n",
    "    # imploded_stocks2['year'] = pd.to_datetime(imploded_stocks2['end_date']).dt.year\n",
    "    # imploded_stocks2 = imploded_stocks2.groupby('year')['preds'].sum()\n",
    "    # print(imploded_stocks2.head(100))\n",
    "    \n",
    "    \n",
    "    \n",
    "    spark_df = spark.createDataFrame(imploded_stocks['fsym_id'].to_frame())\n",
    "    imp_prices = get_fund_data(spark_df)\n",
    "    \n",
    "    adj_pd = imp_prices.toPandas()\n",
    "    adj_pd['date'] = pd.to_datetime(adj_pd['date'])\n",
    "    list_to_plot = sorted(adj_pd['fsym_id'].unique().tolist())[:24]\n",
    "    \n",
    "    columns = 8\n",
    "    num_rows = math.ceil(len(list_to_plot) / columns)\n",
    "    fig, axs = plt.subplots(nrows=num_rows, ncols=columns, figsize=(35, 5*num_rows))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    i = 0\n",
    "    for t in list_to_plot:\n",
    "        temp_df = adj_pd[adj_pd['fsym_id']==t]\n",
    "        impl_period = imploded_stocks[imploded_stocks['fsym_id']==t]\n",
    "        impl_period = impl_period.reset_index()\n",
    "        impl_period = impl_period.iloc[0]\n",
    "        # print(impl_period)\n",
    "        axs[i].plot(temp_df['date'], temp_df['adj_price'], label=t)\n",
    "        axs[i].axvspan(pd.to_datetime(impl_period['end_date']), pd.to_datetime(impl_period['end_date'])+timedelta(weeks=52), alpha=0.5, color='red')\n",
    "        axs[i].legend()\n",
    "        #axs[i].text(0.5, -0.1, f'Volatility: {vol:.2f}', ha='center', transform=axs[i].transAxes)\n",
    "        i+=1\n",
    "        \n",
    "    for i in range(len(list_to_plot), num_rows * columns):\n",
    "        fig.delaxes(axs.flatten()[i])\n",
    "    \n",
    "        \n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('results_5yr/incorrect_implosions_detected_by_model.png')\n",
    "    \n",
    "# plotting_stocks_pandas(model_loaded, new_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5d1f6c-a765-4c89-b3a0-d7a0345b3ba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def shap_fps(model, df):\n",
    "    df = df.set_index('end_date')\n",
    "    df = df.sort_index()\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "    test_df = df[df.index >= '2019-01-01']\n",
    "    \n",
    "    X_test = test_df.drop(exclude_columns, axis=1)\n",
    "    y_test = test_df['label']\n",
    "    \n",
    "    preds = model.predict(X_test)\n",
    "    test_df['preds'] = preds\n",
    "    \n",
    "    fps = test_df[(test_df['label'] == 0) & (test_df['preds'] == 1)]\n",
    "    fps = fps.drop(exclude_columns+['preds'],axis=1)\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer.shap_values(fps)\n",
    "    shap.initjs()\n",
    "    # shap.plots.beeswarm(shap_values)\n",
    "    # shap.plots.scatter(shap_values[:, fps.columns.get_loc('ff_earn_yld_Mean')], color=shap_values)\n",
    "    # shap.dependence_plot(\"ff_earn_yld_Mean\", shap_values[1], fps,interaction_index=\"Age\")\n",
    "\n",
    "    \n",
    "# shap_fps(model_loaded, new_df)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 36,
>>>>>>> Stashed changes
   "id": "87d7c61e-6540-4299-a147-cea44bb8963e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "2024-04-15 22:35:07,579 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "iteritems is deprecated and will be removed in a future version. Use .items instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ff_accr_exp', 'ff_assets_com_eq', 'ff_assets_eq', 'ff_assets_gr', 'ff_assets_oth_tot', 'ff_assets_per_emp', 'ff_bps_gr', 'ff_capex_assets', 'ff_capex_ps_cf', 'ff_cash_curr_assets', 'ff_cash_div_cf', 'ff_cash_roce', 'ff_cf_ps_gr', 'ff_cf_sales', 'ff_cogs_sales', 'ff_com_eq_gr', 'ff_com_eq_tcap', 'ff_debt_com_eq', 'ff_debt_entrpr_val', 'ff_debt_eq', 'ff_debt_lt_cf', 'ff_debt_st_x_curr_port', 'ff_dfd_tax_assets_lt', 'ff_dil_adj', 'ff_div_yld', 'ff_div_yld_secs', 'ff_earn_yld', 'ff_ebit_oper_mgn', 'ff_ebit_oper_roa', 'ff_ebitda_oper_mgn', 'ff_eff_int_rate', 'ff_emp_gr', 'ff_entrpr_val_sales', 'ff_eps_basic_gr', 'ff_fix_assets_com_eq', 'ff_for_assets_pct', 'ff_for_sales_pct', 'ff_free_ps_cf', 'ff_gross_cf_debt', 'ff_inc_adj', 'ff_inc_sund', 'ff_inc_tax_curr', 'ff_inc_tax_dfd', 'ff_int_exp_oth', 'ff_intang_oth', 'ff_invest_cap', 'ff_invest_lt', 'ff_invest_st_tot', 'ff_ltd_com_eq', 'ff_ltd_tcap', 'ff_min_int_tcap', 'ff_mkt_val_gr', 'ff_mkt_val_public', 'ff_net_cf_debt', 'ff_net_inc_basic_aft_xord', 'ff_net_inc_basic_beft_xord', 'ff_net_inc_bef_xord_gr', 'ff_net_inc_dil', 'ff_net_inc_dil_aft_xord', 'ff_net_inc_per_emp', 'ff_net_mgn_gr', 'ff_non_oper_exp', 'ff_oper_cf_fix_chrg', 'ff_oper_inc_aft_unusual', 'ff_oper_inc_gr', 'ff_oper_inc_tcap', 'ff_oper_ps_net_cf', 'ff_pfd_stk_tcap', 'ff_receiv_curr_assets', 'ff_receiv_turn', 'ff_reinvest_rate', 'ff_roa_ptx', 'ff_roce', 'ff_roic', 'ff_sales_gr', 'ff_sales_per_emp', 'ff_sales_ps_gr', 'ff_sga_oth', 'ff_sga_sales', 'ff_shs_float', 'ff_tcap_assets', 'ff_tot_debt_tcap_std', 'ff_ut_gross_inc', 'ff_ut_non_oper_inc_oth', 'ff_ut_operation_exp', 'ff_wkcap', 'ff_wkcap_pct', 'ff_xord', 'ff_xord_disc', 'ff_invest_receiv_lt', 'ff_ebit_bef_unusual', 'ff_ebitda_bef_unusual', 'ff_eps_dil_aft_xord', 'ff_eps_dil_gr', 'ff_psales_dil', 'ff_std_debt', 'ff_tang_assets_debt', 'ff_net_inc_dil_bef_unusual', 'ff_bk_oper_inc_oth', 'ff_bk_oper_inc_tot', 'ff_bk_non_oper_inc', 'ff_commiss_inc_net', 'ff_cf_roic', 'ff_liabs_lease', 'ff_fcf_yld', 'ff_compr_inc', 'ff_compr_inc_tot', 'GDP', 'Unemployment_Rate', 'CPI']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    }
   ],
   "source": [
    "full_df = get_df(f'{main_dir}/data/imploded_stocks_price.csv')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": null,
=======
   "execution_count": 37,
>>>>>>> Stashed changes
   "id": "54d91c81-d173-4b6e-9e84-aca67a800a1a",
   "metadata": {
    "tags": []
   },
<<<<<<< Updated upstream
   "outputs": [],
=======
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ff_oper_ps_net_cf</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fsym_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>B00FG1-R</th>\n",
       "      <td>2.502020</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B01DPB-R</th>\n",
       "      <td>2.592441</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B01HWF-R</th>\n",
       "      <td>0.462230</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B04CB3-R</th>\n",
       "      <td>-0.175698</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B04HL7-R</th>\n",
       "      <td>0.910161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ff_oper_ps_net_cf  label\n",
       "fsym_id                           \n",
       "B00FG1-R           2.502020      0\n",
       "B01DPB-R           2.592441      0\n",
       "B01HWF-R           0.462230      0\n",
       "B04CB3-R          -0.175698      0\n",
       "B04HL7-R           0.910161      0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "earn_groups = full_df.groupby('fsym_id').agg({'ff_oper_ps_net_cf': 'mean', 'label': 'sum'})\n",
    "earn_groups.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba83ce00-bb4a-42a5-bec6-5571f24ff51d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_outliers_iqr(data, column):\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return data[(data[column] >= lower_bound) & (data[column] <= upper_bound)]\n",
    "\n",
    "def box_plots(data):\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df = remove_outliers_iqr(df, 'ff_oper_ps_net_cf')\n",
    "    # Filter data for label=1 and label=0\n",
    "    label_1_data = df[df['label'] == 1]['ff_oper_ps_net_cf']\n",
    "    label_0_data = df[df['label'] == 0]['ff_oper_ps_net_cf']\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.boxplot([label_1_data, \n",
    "                 label_0_data],\n",
    "                labels=['Imploded', 'Non-Imploded'], vert=False, notch=True, patch_artist=True)\n",
    "    \n",
    "    # Customizing appearance\n",
    "    plt.title('Average Operating Cash Flow per Share for Imploded and Non-Imploded Stocks')\n",
    "    plt.xlabel('Average Operating Cash Flow per Share')\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results_5yr/oper_ps_net_cf_boxplot.png')\n",
    "    \n",
    "\n",
    "\n",
    "box_plots(earn_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b00c43-e5b2-42da-a80d-a6e1c4ae61b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "def stat_test(data):\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df = remove_outliers_iqr(df, 'ff_earn_yld')\n",
    "    \n",
    "    label_1_data = df[df['label'] == 1]['ff_earn_yld']\n",
    "    label_0_data = df[df['label'] == 0]['ff_earn_yld']\n",
    "    t_statistic, p_value = stats.ttest_ind(label_1_data, label_0_data, equal_var=False)\n",
    "    print(f\"T-statistic: {t_statistic}\")\n",
    "    print(f\"P-value: {p_value}\")\n",
    "\n",
    "    alpha = 0.05\n",
    "    if p_value < alpha:\n",
    "        print(\"Reject null hypothesis: There is a statistically significant difference in mean earnings yield.\")\n",
    "    else:\n",
    "        print(\"Fail to reject null hypothesis: No statistically significant difference in mean earnings yield.\")\n",
    "        \n",
    "stat_test(earn_groups)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbec60f8-9f93-426b-9c41-44c36058e896",
   "metadata": {},
   "source": [
    "# Scrapped Code\n",
    "Some of these functions may still be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d021d05-5678-45c8-90a1-d7965bbe2a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "\n",
    "def plot_model_performance(mdl, loss, metric):\n",
    "    x = pd.DataFrame(mdl.history).reset_index()\n",
    "    x = pd.melt(x, id_vars='index')\n",
    "    x['validation'] = (x['variable'].str[:4] == 'val_').replace({True:'validation',False:'training'})\n",
    "    x['loss'] = (x['variable'].str[-4:] == 'loss').replace({True:loss,False:metric})\n",
    "    g = sns.FacetGrid(x, col='loss', hue='validation',sharey=False)\n",
    "    g.map(sns.lineplot, 'index','value')\n",
    "    g.add_legend()\n",
    "    return g\n",
    "\n",
    "def nn_training(df):\n",
    "    train_df, test_df = t_t_split(df)\n",
    "\n",
    "    train_df = train_df.toPandas()\n",
    "    train_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Drop rows containing NaN values\n",
    "    train_df.dropna(axis=0, how='any', inplace=True)\n",
    "    \n",
    "    test_df = test_df.toPandas()\n",
    "    train_X = train_df.drop(['fsym_id', 'label'], axis=1).values\n",
    "    train_y = np.array(train_df['label'])\n",
    "    test_X = test_df.drop(['fsym_id', 'label'], axis=1).values\n",
    "    test_y = np.array(test_df['label'])\n",
    "    print(np.sum(test_y==1))\n",
    "    print(train_X, train_y)\n",
    "    \n",
    "    class_labels = np.unique(train_y)\n",
    "    class_weights = compute_class_weight('balanced', classes=class_labels, y=train_y.flatten())\n",
    "    class_weight_dict = dict(zip(class_labels, class_weights))\n",
    "    print(class_weight_dict)\n",
    "    \n",
    "\n",
    "    # Define the neural network model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(train_X.shape[1],)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),  # Additional Dense layer\n",
    "        tf.keras.layers.Dropout(0.5),  # Dropout layer for regularization\n",
    "        tf.keras.layers.Dense(16, activation='relu'),  # Another Dense layer\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    # model = keras.Sequential()\n",
    "    # model.add(layers.Dense(16,activation=\"relu\",input_shape=(train_X.shape[1],)))\n",
    "    # model.add(layers.Dense(8,activation=\"tanh\"))\n",
    "    # model.add(layers.Dense(1))\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    loss_fn = keras.losses.BinaryCrossentropy()\n",
    "    optimizer = keras.optimizers.Adam(\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    # early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    fit_model = model.fit(train_X, train_y, epochs=50, batch_size=32, validation_split=0.1, class_weight = class_weight_dict)\n",
    "    plot_model_performance(fit_model, 'bin_cross_entropy','accuracy')\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_acc = model.evaluate(test_X, test_y)\n",
    "    print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "    # Make predictions on new data\n",
    "    predictions = model.predict(test_X)\n",
    "    for i in range(len(predictions)):\n",
    "        predictions[i] = 1 if predictions[i] >= 0.5 else 0\n",
    "    print(classification_report(predictions, test_y.flatten()))\n",
    "    \n",
    "    # pred_df = pd.DataFrame()\n",
    "    # pred_df['prediction'] = predictions\n",
    "    # pred_df['label'] = test_y\n",
    "    # confusion_matrix_pandas(pred_df)\n",
    "    cm = confusion_matrix(test_y, predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "nn_training(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1fc72f-006a-44b5-83fd-21ba1e382ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "# import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "\n",
    "def pct_change_df(df, big_string, table):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, t.Implosion_Start_Date, b.date, {big_string}\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN sym_ticker_region s ON s.fsym_id = t.fsym_id\n",
    "                LEFT JOIN {table} a ON s.fsym_id = a.fsym_id AND  YEAR(a.date) = YEAR(t.Implosion_Start_Date)\n",
    "                LEFT JOIN {table} b ON s.fsym_id = b.fsym_id AND  YEAR(b.date) = YEAR(t.Implosion_Start_Date)-1\n",
    "                ORDER BY t.fsym_id\n",
    "            \"\"\"\n",
    "    df1 = spark.sql(query1)\n",
    "    #print(df1.show())\n",
    "    df1 = df1.toPandas()\n",
    "    df1 = df1.drop(['fsym_id','Implosion_Start_Date','date'], axis=1)\n",
    "    \n",
    "    def remove_outliers(column):\n",
    "        Q1 = column.quantile(0.25)\n",
    "        Q3 = column.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return column[(column >= lower_bound) & (column <= upper_bound)]\n",
    "\n",
    "\n",
    "\n",
    "    df1 = df1.abs()\n",
    "    null_percentage = df1.isnull().sum() / len(df1)\n",
    "    columns_to_keep = null_percentage[null_percentage <= 0.3].index\n",
    "    df_nulls_removed = df1[columns_to_keep]\n",
    "    print(\"Columns kept: \", len(columns_to_keep)/len(df1.columns))\n",
    "    \n",
    "    df_no_outliers = df_nulls_removed.apply(remove_outliers)\n",
    "\n",
    "    \n",
    "    column_means_no_outliers = df_no_outliers.mean()\n",
    "    #column_means_no_outliers = column_means_no_outliers.dropna()\n",
    "    column_means_no_outliers = column_means_no_outliers.sort_values()\n",
    "    feats = column_means_no_outliers.tail(5)\n",
    "\n",
    "    print(\"Largest averages of differences between previous year and implosion year: \",feats)\n",
    "    return feats.index.tolist()\n",
    "    \n",
    "def avg_change_df(df, big_string, table):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, {big_string}\n",
    "                FROM temp_table t  \n",
    "                LEFT JOIN sym_ticker_region s ON s.fsym_id = t.fsym_id\n",
    "                LEFT JOIN {table} a ON s.fsym_id = a.fsym_id AND  YEAR(a.date) > YEAR(t.Implosion_Start_Date)\n",
    "                LEFT JOIN {table} b ON s.fsym_id = b.fsym_id AND  YEAR(b.date) < YEAR(t.Implosion_Start_Date)\n",
    "                GROUP BY t.fsym_id\n",
    "                ORDER BY t.fsym_id\n",
    "            \"\"\"\n",
    "    df1 = spark.sql(query1)\n",
    "    df1 = df1.toPandas()\n",
    "    df1 = df1.drop(['fsym_id'], axis=1)\n",
    "    \n",
    "    def remove_outliers(column):\n",
    "        Q1 = column.quantile(0.25)\n",
    "        Q3 = column.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return column[(column >= lower_bound) & (column <= upper_bound)]\n",
    "\n",
    "\n",
    "    df1 = df1.abs()\n",
    "    null_percentage = df1.isnull().sum() / len(df1)\n",
    "    columns_to_keep = null_percentage[null_percentage <= 0.3].index\n",
    "    df_nulls_removed = df1[columns_to_keep]\n",
    "    print(\"Columns kept: \", len(columns_to_keep)/len(df1.columns))\n",
    "    \n",
    "    df_no_outliers = df_nulls_removed.apply(remove_outliers)\n",
    "    \n",
    "    column_means_no_outliers = df_no_outliers.mean()\n",
    "    #column_means_no_outliers = column_means_no_outliers.dropna()\n",
    "    column_means_no_outliers = column_means_no_outliers.sort_values()\n",
    "    feats = column_means_no_outliers.tail(5)\n",
    "    print(\"Largest averages of differences in average before and after implosion date: \", feats)\n",
    "#     for feature in feats.index:\n",
    "#         before_implosion = df_no_outliers[feature][df_no_outliers.index.isin(df1[df1[feature].notnull() & (df1['date'] < df1['Implosion_Start_Date'])].index)]\n",
    "#         after_implosion = df_no_outliers[feature][df_no_outliers.index.isin(df1[df1[feature].notnull() & (df1['date'] > df1['Implosion_Start_Date'])].index)]\n",
    "        \n",
    "#         _, p_value = ttest_ind(before_implosion, after_implosion)\n",
    "        \n",
    "#         print(f\"T-test p-value for {feature}: {p_value}\")\n",
    "    return feats.index.tolist()\n",
    "\n",
    "def t_test():\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_metric_changes(filename, table):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df = df[df['Implosion_Start_Date'].notnull()]\n",
    "    df['Implosion_Start_Date'] = pd.to_datetime(df['Implosion_Start_Date']).dt.date\n",
    "    df['Implosion_End_Date'] = pd.to_datetime(df['Implosion_End_Date']).dt.date\n",
    "    cols = get_not_null_cols(df, table)\n",
    "    result_string = ', '.join('(a.' + item + '-b.' + item +')/b.'+item + ' AS ' + item for item in cols)\n",
    "    feats1 = pct_change_df(df, result_string, table) #change 1 year before\n",
    "    print(\"Features with greatest percentage change with year before implosion: \", feats1)\n",
    "    \n",
    "    result_string2 = ', '.join('(MEAN(a.' + item + ')-MEAN(b.' + item +'))/MEAN(b.'+item + ') AS ' + item for item in cols)\n",
    "    feats2 = avg_change_df(df, result_string2, table)\n",
    "    print(\"Features with greatest percentage change in mean before and after implosion\", feats2)\n",
    "    \n",
    "    write_features_file( list(set(feats1+feats2)) )\n",
    "\n",
    "\n",
    "get_metric_changes('imploded_stocks_price.csv', 'FF_ADVANCED_DER_AF')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7274d-d2b0-4016-a2b6-4f7633f51fe7",
   "metadata": {},
   "source": [
    "### Correlations with Market Value Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc63eb-7d98-4de8-b665-808509638bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from CreateDataset import get_feature_col_names, get_fund_data\n",
    "\n",
    "\n",
    "def corr_query(implosion_df, col_string, table): \n",
    "    df = get_fund_data(implosion_df)\n",
    "    df=df.withColumn('year', F.year('date'))\n",
    "    window_spec = Window.partitionBy('fsym_id', 'year').orderBy(col('date').desc())\n",
    "\n",
    "    df = df.withColumn('row_num', F.row_number().over(window_spec))\n",
    "\n",
    "    df = df.filter(col('row_num') == 1).orderBy('date') #should we compare correlations with market val?\n",
    "    #should we do quarterly?\n",
    "    \n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, t.adj_price, t.Market_Value, t.date, {col_string}\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id AND YEAR(t.date)=YEAR(a.date)\n",
    "                ORDER BY t.fsym_id, t.date\n",
    "            \"\"\"\n",
    " \n",
    "    q_df = spark.sql(query1)\n",
    "    #q_df.show()\n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy('date')\n",
    "    \n",
    "    q_df = q_df.withColumn(\"return_market_val\", (F.col('Market_Value') - F.lag('Market_Value').over(window_spec)) / F.lag('Market_Value').over(window_spec))\n",
    "    q_df = q_df.withColumn(\"return\", (F.col('adj_price') - F.lag('adj_price').over(window_spec)) / F.lag('adj_price').over(window_spec))\n",
    "    \n",
    "    return_columns = [c[2:] for c in col_string.split(\", \")]\n",
    "    mean_corrs = []\n",
    "    corr_vals = []\n",
    "    #I THINK U NEED TO GROUP BY DATE AND THEN CALCULATE CORRELATIONS\n",
    "\n",
    "    for column in return_columns:\n",
    "        return_col_name = f\"return_{column}\"\n",
    "        corr_col_name = f\"corr_with_{column}\"\n",
    "        q_df = q_df.withColumn(return_col_name, (F.col(column) - F.lag(column).over(window_spec)) / F.lag(column).over(window_spec))\n",
    "        q_df = q_df.withColumn(column, F.corr(return_col_name, 'return_market_val').over(window_spec)) #calculating correlations with market value return\n",
    "        q_df = q_df.drop(*[return_col_name])\n",
    "    q_df = q_df.drop(*['return_market_val', 'return'])\n",
    "    q_df = q_df.select(q_df.columns[4:])\n",
    "    mean_corrs = q_df.agg(*[F.mean(F.abs(F.col(column))).alias(column) for column in q_df.columns])\n",
    "    # mean_corrs.show()\n",
    "    \n",
    "    return mean_corrs.toPandas()\n",
    "\n",
    "def corr_analysis(table):\n",
    "    imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "    imp_df_price = imp_df_price.loc[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "    cols = get_not_null_cols(imp_df_price, 'FF_ADVANCED_DER_AF')\n",
    "    result_string = ', '.join('a.' + item for item in cols)\n",
    "    mean_corrs_df = corr_query(spark.createDataFrame(imp_df_price), result_string, 'FF_ADVANCED_DER_AF')\n",
    "    mean_corrs = mean_corrs_df.to_dict(orient='records')\n",
    "    sorted_corrs = dict(sorted(mean_corrs[0].items(), key=lambda item: item[1], reverse=True))\n",
    "    top_records = list(sorted_corrs.items())[:5]\n",
    "    top_10 = []\n",
    "    for r in top_records:\n",
    "        top_10.append(r[0])\n",
    "    print(top_10)\n",
    "    current_feature_list = get_feature_col_names()\n",
    "    new_feature_list = list(set(current_feature_list + top_10))\n",
    "    \n",
    "    write_features_file(new_feature_list)\n",
    "    \n",
    "    \n",
    "corr_analysis('FF_Advanced_Der_AF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3722ccc-0c58-4599-a464-6e153f4e1f13",
   "metadata": {},
   "source": [
    "### Adding the Extra Features From Literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586949c-01ca-4b25-88c1-1488355015e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "imp_df_price['Implosion_Start_Date'] = pd.to_datetime(imp_df_price['Implosion_Start_Date'])\n",
    "imp_df_price['Implosion_End_Date'] = pd.to_datetime(imp_df_price['Implosion_End_Date'])\n",
    "available_feats = get_not_null_cols(imp_df_price)\n",
    "extra_feats = ['ff_capex_assets', 'ff_gross_cf_debt', 'ff_mkt_val_gr']\n",
    "\n",
    "current_feats = get_feature_col_names()\n",
    "final_feats = list(set(current_feats + extra_feats))\n",
    "write_features_file(final_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca6a8a0-1c51-42e5-8a36-18044f9e9bc4",
   "metadata": {},
   "source": [
    "### Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203b078-0417-4933-afe8-4442a98809ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(all_feats=False, imploded_only=False):\n",
    "    df = get_tabular_dataset(all_feats=all_feats, imploded_only=imploded_only)\n",
    "    df = forward_fill(df)\n",
    "    print(\"Number of rows: \", df.count())\n",
    "    print(\"Number of positives: \", df.filter(F.col('label')==1).count())\n",
    "    df=df.fillna(0.0)\n",
    "    print(\"Number of rows after dropping nulls: \", df.count())\n",
    "    print(\"Number of positives after dropping nulls: \", df.filter(F.col('label')==1).count())\n",
    "    return df\n",
    "\n",
    "\n",
    "def forward_fill(df):\n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy('date')\n",
    "    feature_cols = df.columns[2:-1]\n",
    "    for c in feature_cols:\n",
    "        df = df.withColumn(\n",
    "            c, F.last(c, ignorenulls=True).over(window_spec)\n",
    "        )\n",
    "    return df.orderBy('fsym_id','date')\n",
    "\n",
    "df = get_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0e0a7-413f-4d65-9fec-31092302bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Boruta import BorutaPy\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "def boruta_fs(train_df, model_name): #HOW DOES BORUTA ACC WORK?\n",
    "    train_df = train_df.toPandas()\n",
    "    X_train = train_df.drop(['fsym_id', 'date', 'label'], axis=1)\n",
    "    y_train = train_df['label']\n",
    "    \n",
    "    if model_name == 'rf':\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        model = GradientBoostingClassifier\n",
    "    feat_selector = BorutaPy(model, n_estimators='auto', verbose=2, random_state=1)\n",
    "    feat_selector.fit(X_train, y_train)\n",
    "    features = X_train.columns.tolist()\n",
    "    print(\"Number of features: \", len(features) )\n",
    "    feature_ranks = list(zip(features, feat_selector.ranking_, feat_selector.support_))\n",
    "    selected_features = []\n",
    "    for feat in feature_ranks:\n",
    "        print(f\"Feature: {feat[0]}, Rank: {feat[1]}, Keep: {feat[2]}\")\n",
    "        if feat[1] <= 5:\n",
    "            selected_features.append(feat[0])\n",
    "    print(\"Selected features: \", selected_features)\n",
    "    return selected_features\n",
    "\n",
    "rf_feats = boruta_fs(df, 'rf')\n",
    "gbt_feats = boruta_fs(df, 'gbt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f1dd31-27ce-42b8-ac91-41d2ebf9876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_features = get_feature_col_names()\n",
    "# for f in boruta_features:\n",
    "#     if f in current_features:\n",
    "#         print(f)\n",
    "# final_features = list(set(boruta_features + current_features))\n",
    "# write_features_file(final_features) #in the feature selection pipeline, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c21d5-a37f-4211-a2dc-a32a14a41cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def correlation_matrix(df):\n",
    "    df =df.toPandas()\n",
    "    print(\"Converted to Pandas\")\n",
    "    corr_df = df.drop(['date','fsym_id'], axis=1)\n",
    "    corr_mat = corr_df.corr()\n",
    "    mask = np.triu(np.ones_like(corr_mat))\n",
    "    plt.figure(figsize=(50, 40))\n",
    "    sns.heatmap(corr_mat, annot=True, cmap='coolwarm', fmt=\".2f\", mask=mask)\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('corr_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Variable pairs with absolute correlation above 0.7:\")\n",
    "    for i in range(len(corr_mat.columns)):\n",
    "        for j in range(i+1, len(corr_mat.columns)):\n",
    "            if abs(corr_mat.iloc[i, j]) >= 0.7:\n",
    "                print(f\"{corr_mat.columns[i]} - {corr_mat.columns[j]}: {corr_mat.iloc[i, j]}\")\n",
    "                \n",
    "# correlation_matrix(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ea6a0-81ff-4b2b-9dd0-f6fff61d7fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('ff_div_yld_secs', 'ff_earn_yld', 'ff_roa_ptx', 'ff_net_inc_basic_aft_xord', 'ff_net_inc_dil', 'ff_oper_inc_aft_unusual', \n",
    "                        'ff_net_inc_dil_aft_xord', 'ff_net_inc_dil_bef_unusual', 'ff_ebit_bef_unusual', 'ff_eps_dil_gr', 'GDP', 'ff_bk_oper_inc_tot')\n",
    "feats = df.columns[2:-1]\n",
    "# write_features_file(feats)\n",
    "feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb6e21-44f3-4b26-b63b-a45b4ba8056a",
   "metadata": {},
   "source": [
    "### Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385f503-98bc-4b81-938c-27e8225f53b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_dates(imp_df_price):\n",
    "    price_data = get_fund_data(spark.createDataFrame(imp_df_price))\n",
    "    #cols = get_not_null_cols(imp_df_price, 'FF_ADVANCED_DER_AF')\n",
    "    #result_string = ', '.join('a.' + item for item in cols)\n",
    "    \n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy(col('p_date'))\n",
    "\n",
    "    price_data = price_data.withColumn('row_num', F.row_number().over(window_spec))\n",
    "    price_data.show()\n",
    "\n",
    "    price_data = price_data.filter(col('row_num') == 1).orderBy(col('p_date').desc())\n",
    "    price_data.show()\n",
    "    \n",
    "    start_dates = price_data.groupBy('year').count().orderBy('year')\n",
    "    years = [row['year'] for row in start_dates.collect()]\n",
    "    counts = [row['count'] for row in start_dates.collect()]\n",
    "    plt.bar(years, counts)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Start Dates Count per Year')\n",
    "    plt.show()\n",
    "    #start_dates.show(25)\n",
    "    \n",
    "def null_vals(imp_df_price, table):\n",
    "    price_data = get_fund_data(spark.createDataFrame(imp_df_price))\n",
    "    cols = get_not_null_cols(imp_df_price, table)\n",
    "    col_string = ', '.join('a.' + item for item in cols)\n",
    "    price_data.createOrReplaceTempView('temp_table')\n",
    "    null_counts = []\n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, t.split_adj_price, t.Market_Value, t.p_date, {col_string}\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id AND YEAR(t.p_date)=YEAR(a.date)\n",
    "                ORDER BY t.fsym_id, t.p_date\n",
    "            \"\"\"\n",
    "    full_df = spark.sql(query1)\n",
    "    for column in cols:\n",
    "        null_count = full_df.select(column).filter(col(column).isNull()).count()\n",
    "        null_counts.append((column, null_count))\n",
    "    null_counts_df = pd.DataFrame(null_counts, columns=['Column', 'Null Count'])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(null_counts_df['Column'], null_counts_df['Null Count'])\n",
    "    plt.xlabel('Column')\n",
    "    plt.ylabel('Null Count')\n",
    "    plt.title('Null Counts for Each Column')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # null_counts = price_data.groupBy('year').agg(F.sum(col('p_price').isNull().cast('int')).alias('null_count'))\n",
    "    # null_counts.show()\n",
    "    \n",
    "imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "imp_df_price_imploded = imp_df_price.loc[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "start_dates(imp_df_price)\n",
    "start_dates(imp_df_price_imploded)\n",
    "\n",
    "#null_vals(imp_df_price, 'FF_ADVANCED_DER_AF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345755a7-1bf1-442d-9418-576cb9688733",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "imp_df_test = imp_df_price[imp_df_price['fsym_id']=='H7CTYF-R']\n",
    "df = get_fund_data(spark.createDataFrame(imp_df_test))\n",
    "df.show(1000)\n",
    "imp_df_imp = imp_df_price[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "print(len(imp_df_imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730ca87-195a-49a3-87e3-60f22f57b015",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df_imp = imp_df_price[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "print(len(imp_df_imp))\n",
    "print(len(imp_df_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19776f2-22bb-4ab6-a748-cb9928010b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols():\n",
    "    df_metrics = ps.DataFrame(spark.sql(\"SELECT * FROM FF_BASIC_AF LIMIT 10\")) #get all the metrics\n",
    "    cols = []\n",
    "    for c in df_metrics.columns:\n",
    "        if df_metrics[c].dtype=='float64':#get all the metrics we can calculate correlations with\n",
    "            cols.append(c)\n",
    "    return cols\n",
    "\n",
    "#%change average of each feature plotted for pharmacy industry\n",
    "def industry_analysis():\n",
    "    stock_df = get_all_stocks_df()\n",
    "    #stock_df = pd.read_csv('imploded_stocks.csv')\n",
    "    #stock_df = spark.createDataFrame(stock_df)\n",
    "    cols = ['ff_gross_inc', 'ff_sales', 'FF_OPER_EXP_TOT', 'FF_CASH_ST']\n",
    "    col_string = ', '.join('a.' + item for item in cols)\n",
    "    stock_df.createOrReplaceTempView(\"temp_table\")\n",
    "    q = f\"\"\"SELECT e.factset_industry_desc, t.ticker_region, a.date, {col_string} FROM temp_table t\n",
    "    LEFT JOIN FF_BASIC_AF a ON a.fsym_id = t.fsym_id\n",
    "    LEFT JOIN sym_coverage sc ON sc.fsym_id = t.fsym_id\n",
    "    LEFT JOIN ff_sec_entity_hist c on c.fsym_id=sc.fsym_security_id\n",
    "    LEFT JOIN sym_entity_sector d on d.factset_entity_id=c.factset_entity_id\n",
    "    LEFT JOIN factset_industry_map e on e.factset_industry_code=d.industry_code\n",
    "    WHERE a.date >= \"2009-01-01\" AND e.factset_industry_desc=\"Regional Banks\"\n",
    "    ORDER BY t.ticker_region,a.date\"\"\"\n",
    "    ind_df = spark.sql(q)\n",
    "    #print(ind_df.show(10))\n",
    "    ind_df =ind_df.toPandas()\n",
    "    ind_df['date'] = pd.to_datetime(ind_df['date'])\n",
    "    new_cols = []\n",
    "    for column in cols:\n",
    "        ind_df[f'{column}_percentage_change'] = ind_df.groupby('ticker_region')[column].pct_change() * 100\n",
    "        ind_df[f'{column}_percentage_change'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        ind_df.drop(column, axis=1, inplace=True)\n",
    "        new_cols.append(f'{column}_percentage_change')\n",
    "    ind_df['year'] = ind_df['date'].dt.year\n",
    "    avg_pct_change = ind_df.groupby(['year'])[new_cols].mean().reset_index()\n",
    "    print(avg_pct_change.head(20))\n",
    "    num_rows = (len(new_cols) + 1) // 2  # Adjust the number of rows as needed\n",
    "    num_cols = 2\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n",
    "    for i,column in enumerate(new_cols):\n",
    "        row = i//num_cols\n",
    "        col = i % num_cols \n",
    "        axes[row,col].plot(avg_pct_change['year'], avg_pct_change[column])\n",
    "        axes[row, col].set_title(f'Avg {column} Percentage Change Over Time')\n",
    "        axes[row, col].set_xlabel('Year')\n",
    "        axes[row, col].set_ylabel(f'Avg {column} Percentage Change')\n",
    "        axes[row, col].grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#industry_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b54c9-5ddc-4459-afff-5247cc6b7b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_not_null_cols(df, table='FF_ADVANCED_DER_AF'):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query1 = f\"\"\"SELECT t.fsym_id, a.*\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id\n",
    "                ORDER BY t.fsym_id, a.date\n",
    "            \"\"\"\n",
    "    #we get all the available dates per stock, so these null values are only within the timeframe available\n",
    "    q_df = spark.sql(query1)\n",
    "    column_types = q_df.dtypes\n",
    "    null_pcts = []\n",
    "    for c, dtype in zip(q_df.columns, column_types):\n",
    "        if dtype[1] == 'double':\n",
    "            null_count = q_df.filter(F.col(c).isNull()).count()\n",
    "            null_pcts.append(null_count/q_df.count())\n",
    "\n",
    "\n",
    "    columns_to_drop = [col_name for col_name, null_pct, dtype in zip(q_df.columns, null_pcts, column_types) if null_pct > 0.2 or dtype[1]!='double']\n",
    "\n",
    "    q_df = q_df.drop(*columns_to_drop)\n",
    "\n",
    "    cols = q_df.columns\n",
    "    print(cols)\n",
    "\n",
    "    return cols\n",
    "    \n",
    "df = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "df = df.loc[df['Implosion_Start_Date'].notnull()]\n",
    "get_not_null_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d7b62c-becd-45ee-bcb2-8c8c59e8e0b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
