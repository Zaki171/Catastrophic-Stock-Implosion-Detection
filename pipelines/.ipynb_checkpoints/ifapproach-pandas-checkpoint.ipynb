{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe610780-dc3d-4ac8-9dd1-914ed92696c0",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6802cbd1-16cf-4e4a-8137-9894bdec78e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop-3.2.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.7-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "2024-02-09 19:24:07,826 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-02-09 19:24:09,028 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2024-02-09 19:24:10,921 WARN spark.ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Database(name='2023_11_01', description='FactSet data snapshot for 2023_11_01', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_01'),\n",
       " Database(name='2023_11_02', description='FactSet data snapshot for 2023_11_02', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_02'),\n",
       " Database(name='2023_11_03', description='FactSet data snapshot for 2023_11_03', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_03'),\n",
       " Database(name='2023_11_14', description='FactSet data snapshot for 2023_11_14', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_14'),\n",
       " Database(name='2023_11_19', description='FactSet data snapshot for 2023_11_19', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_19'),\n",
       " Database(name='2023_11_22', description='FactSet data snapshot for 2023_11_22', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_22'),\n",
       " Database(name='2024_01_25', description='FactSet data snapshot for 2024_01_25', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2024_01_25'),\n",
       " Database(name='2024_02_02', description='FactSet data snapshot for 2024_02_02', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2024_02_02'),\n",
       " Database(name='2024_02_03', description='FactSet data snapshot for 2024_02_03', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2024_02_03'),\n",
       " Database(name='default', description='Default Hive database', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"List available databases and tables\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7c46d2-5935-44d2-a7a2-37563aa77b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2886cd22-2c06-4882-b434-e5ed726788c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.sql(\"USE 2023_11_02\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd18af3-d36d-4088-aa8c-6c410c3322ae",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694d984f-e6b5-42b9-afb9-1c9d98a7fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_all_stocks_df():\n",
    "    query = f\"\"\"SELECT s.ticker_region, s.fsym_id FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_not_null_cols(df, table='FF_ADVANCED_DER_AF'):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query1 = f\"\"\"SELECT t.fsym_id AS fsym_id2, a.*\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id\n",
    "                WHERE a.date > '2000-01-01'\n",
    "                ORDER BY t.fsym_id, a.date\n",
    "            \"\"\"\n",
    "    #we get all the available dates per stock, so these null values are only within the timeframe available\n",
    "    q_df = spark.sql(query1)\n",
    "    q_df = q_df.drop('date', 'adjdate', 'fsym_id2', 'fsym_id')\n",
    "    num_rows = q_df.count()\n",
    "    column_types = q_df.dtypes\n",
    "    good_cols = []\n",
    "    selected_columns = [F.col(c) for c, c_type in zip(q_df.columns, column_types) if c_type[1] == 'double']\n",
    "    q_df = q_df.select(selected_columns)\n",
    "    count_df = q_df.select( [(F.count(F.when(F.isnan(c) | F.col(c).isNull(), c))/num_rows).alias(c) for c in q_df.columns])\n",
    "    count_dict = count_df.first().asDict()\n",
    "    filtered_keys = [key for key, value in count_dict.items() if value <= 0.25]\n",
    "    return filtered_keys\n",
    "#     for c, c_type in zip(q_df.columns, column_types):\n",
    "#         if c_type[1] == 'double':\n",
    "#             null_count = F.sum(F.when(F.isnan(F.col(c)) | F.col(c).isNull(), 1).otherwise(0))\n",
    "#             null_pct = (null_count / num_rows).alias(f\"{c}_null_pct\")\n",
    "#             q_df_agg = q_df.agg(null_pct)\n",
    "#             actual_pct = q_df_agg.collect()[0][0]\n",
    "#             if actual_pct < 0.25:\n",
    "#                 good_cols.append(c)\n",
    "            \n",
    "#     return good_cols\n",
    "\n",
    "\n",
    "def write_features_file(data_list, csv_file_path='features.csv'):\n",
    "    data_list = [data_list]\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row in data_list:\n",
    "            writer.writerow(row)\n",
    "    print(\"Features written: \", data_list[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c41e5b-c43f-41cc-a9bd-c0e2fe6019cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ztewari/Stock-Implosion-Prediction-FYP\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "main_dir = os.path.dirname(curr_dir)\n",
    "print(main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61704c74-e53c-424b-9a8f-78ea6f9d34ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "2024-02-09 19:24:41,818 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ff_assets_com_eq', 'ff_assets_eq', 'ff_assets_gr', 'ff_assets_oth_tot', 'ff_assets_per_emp', 'ff_bps_gr', 'ff_capex_assets', 'ff_capex_ps_cf', 'ff_cash_div_cf', 'ff_cash_roce', 'ff_cf_ps_gr', 'ff_cf_sales', 'ff_com_eq_gr', 'ff_com_eq_tcap', 'ff_debt_com_eq', 'ff_debt_entrpr_val', 'ff_debt_eq', 'ff_debt_lt_cf', 'ff_debt_st_x_curr_port', 'ff_dfd_tax_assets_lt', 'ff_dil_adj', 'ff_div_yld', 'ff_div_yld_secs', 'ff_earn_yld', 'ff_ebit_oper_roa', 'ff_entrpr_val_sales', 'ff_eps_basic_gr', 'ff_fix_assets_com_eq', 'ff_for_assets_pct', 'ff_for_sales_pct', 'ff_free_ps_cf', 'ff_gross_cf_debt', 'ff_inc_adj', 'ff_inc_sund', 'ff_inc_tax_curr', 'ff_inc_tax_dfd', 'ff_int_exp_oth', 'ff_invest_cap', 'ff_invest_lt', 'ff_invest_st_tot', 'ff_ltd_com_eq', 'ff_ltd_tcap', 'ff_min_int_tcap', 'ff_mkt_val_gr', 'ff_net_cf_debt', 'ff_net_inc_basic_aft_xord', 'ff_net_inc_basic_beft_xord', 'ff_net_inc_bef_xord_gr', 'ff_net_inc_dil', 'ff_net_inc_dil_aft_xord', 'ff_net_inc_per_emp', 'ff_net_mgn_gr', 'ff_non_oper_exp', 'ff_oper_cf_fix_chrg', 'ff_oper_inc_aft_unusual', 'ff_oper_inc_gr', 'ff_oper_inc_tcap', 'ff_oper_ps_net_cf', 'ff_pfd_stk_tcap', 'ff_reinvest_rate', 'ff_roa_ptx', 'ff_roce', 'ff_roic', 'ff_sales_gr', 'ff_sales_per_emp', 'ff_sales_ps_gr', 'ff_tcap_assets', 'ff_tot_debt_tcap_std', 'ff_ut_gross_inc', 'ff_ut_non_oper_inc_oth', 'ff_ut_operation_exp', 'ff_wkcap', 'ff_wkcap_pct', 'ff_xord', 'ff_xord_disc', 'ff_invest_receiv_lt', 'ff_ebit_bef_unusual', 'ff_ebitda_bef_unusual', 'ff_eps_dil_aft_xord', 'ff_eps_dil_gr', 'ff_psales_dil', 'ff_std_debt', 'ff_tang_assets_debt', 'ff_net_inc_dil_bef_unusual', 'ff_bk_oper_inc_oth', 'ff_bk_oper_inc_tot', 'ff_bk_non_oper_inc', 'ff_commiss_inc_net', 'ff_cf_roic', 'ff_liabs_lease', 'ff_fcf_yld', 'GDP', 'Unemployment_Rate', 'CPI']\n",
      "df retrieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from CreateDataset import get_tabular_dataset, get_feature_col_names, get_not_null_cols\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "# import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def plot_nulls(df):\n",
    "    null_counts = df.agg(*[\n",
    "    (1 - (F.count(c) / F.count('*'))).alias(c + '_nulls') for c in df.columns])\n",
    "    null_counts_pd = null_counts.toPandas().transpose()\n",
    "    null_counts_pd.columns = ['null_percentage']\n",
    "\n",
    "    # Plot the bar chart\n",
    "    # null_counts_pd.plot(kind='bar', legend=False, figsize=(20, 6))\n",
    "    # plt.title('Percentage of Null Values in Each Column')\n",
    "    # plt.ylabel('Percentage of Null Values')\n",
    "    # plt.xlabel('Columns')\n",
    "    # plt.show()\n",
    "    \n",
    "def forward_fill(df):\n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy('date')\n",
    "    feature_cols = df.columns[2:-1]\n",
    "    for c in feature_cols:\n",
    "        df = df.withColumn(\n",
    "            c, F.last(c, ignorenulls=True).over(window_spec)\n",
    "        )\n",
    "    return df.orderBy('fsym_id','date')\n",
    "\n",
    "def median_fill(df):\n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy('date')\n",
    "    feature_cols = df.columns[2:-1]\n",
    "    imputer = Imputer(strategy=\"median\", inputCols=feature_cols, outputCols=feature_cols)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for c in feature_cols:\n",
    "        median_value = df.approxQuantile(c, [0.5], 0.001)[0]\n",
    "        df = df.withColumn(\n",
    "            c, F.when(F.col(c).isNull(), median_value).otherwise(F.col(c))\n",
    "        )\n",
    "    return df.orderBy('fsym_id','date')\n",
    "\n",
    "\n",
    "def get_df(fn, all_feats=False, imploded_only=False, prediction=False):\n",
    "    df = get_tabular_dataset(fn, all_feats=all_feats, imploded_only=imploded_only, prediction=prediction, null_thresh=0.2)\n",
    "    print(\"df retrieved\")\n",
    "    \n",
    "    # null_counts_per_column = df.select([col(c).isNull().cast(\"int\").alias(c) for c in df.columns])\n",
    "    # total_nulls = null_counts_per_column.agg(*[F.sum(col(c)).alias(c) for c in null_counts_per_column.columns]).collect()\n",
    "    # print(total_nulls)\n",
    "    # df = forward_fill(df)\n",
    "    # print(\"done ffill\")\n",
    "    # null_counts_per_column = df.select([col(c).isNull().cast(\"int\").alias(c) for c in df.columns])\n",
    "    # total_nulls = null_counts_per_column.agg(*[F.sum(col(c)).alias(c) for c in null_counts_per_column.columns]).collect()\n",
    "    # print(total_nulls)\n",
    "    # print(\"Number of rows: \", df.count())\n",
    "    # print(\"Number of positives: \", df.filter(F.col('label')==1).count())\n",
    "    # plot_nulls(df)\n",
    "    # df=df.fillna(0.0)\n",
    "    # print(\"Number of rows after dropping nulls: \", df.count())\n",
    "    # print(\"Number of positives after dropping nulls: \", df.filter(F.col('label')==1).count())\n",
    "    # window_spec = Window.partitionBy('fsym_id')\n",
    "    # feats = df.columns[2:-1]\n",
    "    \n",
    "    df =df.toPandas()\n",
    "    # print(df.head(30))\n",
    "#     feats = df.columns[2:-1]\n",
    "#     for fsym_id, group in df.groupby('fsym_id'):\n",
    "#         for col in group.columns[2:-1]:\n",
    "#             group[col] = group[col].fillna(group[col].median())\n",
    "    \n",
    "#     print(df.head(10))\n",
    "    # feats = df.columns[2:-1]\n",
    "    # df[feats] = df.groupby('fsym_id')[feats].transform(lambda x: x.fillna(x.median()))\n",
    "    # print(df.head(30))\n",
    "    return df\n",
    "    \n",
    "def write_features_file(data_list, csv_file_path='features.csv'):\n",
    "    data_list = [data_list]\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row in data_list:\n",
    "            writer.writerow(row)\n",
    "    print(\"Features written: \", data_list[0])\n",
    "\n",
    "# df = get_df('imploded_stocks_price.csv')\n",
    "df = get_df(f'{main_dir}/data/imploded_stocks_dd.csv', all_feats =True, prediction=False, imploded_only=False)\n",
    "# plot_nulls(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a718ab0-6e68-4097-afa7-98e05292d766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df2 = median_fill(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2824646-aa80-496e-8907-91b7a25778ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fsym_id              108919\n",
       "date                 108919\n",
       "ff_assets_com_eq      99485\n",
       "ff_assets_eq         107358\n",
       "ff_assets_gr         101052\n",
       "                      ...  \n",
       "ff_fcf_yld            89984\n",
       "GDP                  108919\n",
       "Unemployment_Rate    108919\n",
       "CPI                  108919\n",
       "label                108919\n",
       "Length: 97, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79a0fd65-c039-4a2c-8719-e2a20b8404b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to Pandas\n",
      "Variable pairs with absolute correlation above 0.7:\n",
      "ff_assets_com_eq - ff_assets_eq: 0.9977740313964437\n",
      "ff_assets_com_eq - ff_debt_com_eq: 0.9986906415089507\n",
      "ff_assets_com_eq - ff_debt_eq: 0.9976965038670106\n",
      "ff_assets_com_eq - ff_fix_assets_com_eq: 0.9913763695552309\n",
      "ff_assets_com_eq - ff_ltd_com_eq: 0.9986322689029365\n",
      "ff_assets_eq - ff_debt_com_eq: 0.9997593361119921\n",
      "ff_assets_eq - ff_debt_eq: 0.9954708500773313\n",
      "ff_assets_eq - ff_fix_assets_com_eq: 0.9910501161676604\n",
      "ff_assets_eq - ff_ltd_com_eq: 0.9997506467572551\n",
      "ff_assets_per_emp - ff_sales_per_emp: 0.8009300830648081\n",
      "ff_capex_ps_cf - ff_free_ps_cf: 0.7939942740034843\n",
      "ff_capex_ps_cf - ff_oper_ps_net_cf: 0.7808066918006235\n",
      "ff_capex_ps_cf - ff_sales_ps_gr: 0.8809572217736148\n",
      "ff_capex_ps_cf - ff_eps_dil_aft_xord: 0.7177994928929655\n",
      "ff_cf_ps_gr - ff_eps_basic_gr: 0.9999997331876461\n",
      "ff_cf_ps_gr - ff_sales_ps_gr: 0.999999765855268\n",
      "ff_cf_ps_gr - ff_eps_dil_aft_xord: 0.8066004434182418\n",
      "ff_cf_ps_gr - ff_eps_dil_gr: 0.9999997330947785\n",
      "ff_com_eq_tcap - ff_pfd_stk_tcap: 0.7393617722989069\n",
      "ff_com_eq_tcap - ff_tot_debt_tcap_std: 0.723839169428472\n",
      "ff_debt_com_eq - ff_debt_eq: 0.9998338029952734\n",
      "ff_debt_com_eq - ff_fix_assets_com_eq: 0.9907494352338675\n",
      "ff_debt_com_eq - ff_ltd_com_eq: 0.9999794257624541\n",
      "ff_debt_eq - ff_fix_assets_com_eq: 0.9905162880318155\n",
      "ff_debt_eq - ff_ltd_com_eq: 0.9998290483441999\n",
      "ff_div_yld - ff_div_yld_secs: 0.9995782851768595\n",
      "ff_ebit_oper_roa - ff_roa_ptx: 0.9981690104984369\n",
      "ff_entrpr_val_sales - ff_psales_dil: 0.9999999995133038\n",
      "ff_eps_basic_gr - ff_sales_ps_gr: 0.9999999986580594\n",
      "ff_eps_basic_gr - ff_eps_dil_aft_xord: 0.8066014530458017\n",
      "ff_eps_basic_gr - ff_eps_dil_gr: 0.9999999999943289\n",
      "ff_fix_assets_com_eq - ff_ltd_com_eq: 0.9907570471563834\n",
      "ff_for_assets_pct - ff_for_sales_pct: 0.8106798044581712\n",
      "ff_free_ps_cf - ff_oper_ps_net_cf: 0.9983852798798394\n",
      "ff_free_ps_cf - ff_sales_ps_gr: 0.9675037338441513\n",
      "ff_free_ps_cf - ff_eps_dil_aft_xord: 0.9449990143503743\n",
      "ff_gross_cf_debt - ff_net_cf_debt: 0.9044808013541741\n",
      "ff_inc_tax_curr - ff_oper_inc_aft_unusual: 0.7441878520395006\n",
      "ff_inc_tax_curr - ff_ut_gross_inc: 0.7202958755247818\n",
      "ff_inc_tax_curr - ff_ebit_bef_unusual: 0.8080028153906894\n",
      "ff_inc_tax_curr - ff_ebitda_bef_unusual: 0.8032791475383593\n",
      "ff_inc_tax_curr - ff_net_inc_dil_bef_unusual: 0.7365826511369938\n",
      "ff_invest_cap - ff_ut_gross_inc: 0.7061496455175712\n",
      "ff_invest_cap - ff_ebit_bef_unusual: 0.7391078190548601\n",
      "ff_invest_cap - ff_ebitda_bef_unusual: 0.7691519675539163\n",
      "ff_invest_lt - ff_invest_receiv_lt: 1.0\n",
      "ff_ltd_tcap - ff_tot_debt_tcap_std: 0.8391597873772109\n",
      "ff_min_int_tcap - ff_wkcap_pct: 0.9016799953046604\n",
      "ff_net_inc_basic_aft_xord - ff_net_inc_basic_beft_xord: 0.9702205800912327\n",
      "ff_net_inc_basic_aft_xord - ff_net_inc_dil: 0.972327353058125\n",
      "ff_net_inc_basic_aft_xord - ff_net_inc_dil_aft_xord: 0.9993697825804119\n",
      "ff_net_inc_basic_aft_xord - ff_oper_inc_aft_unusual: 0.9097606269584083\n",
      "ff_net_inc_basic_aft_xord - ff_ut_gross_inc: 0.8435009502676005\n",
      "ff_net_inc_basic_aft_xord - ff_ebit_bef_unusual: 0.8370056498737242\n",
      "ff_net_inc_basic_aft_xord - ff_ebitda_bef_unusual: 0.7966298542694771\n",
      "ff_net_inc_basic_aft_xord - ff_net_inc_dil_bef_unusual: 0.92597154240844\n",
      "ff_net_inc_basic_beft_xord - ff_net_inc_dil: 0.9970272365932611\n",
      "ff_net_inc_basic_beft_xord - ff_net_inc_dil_aft_xord: 0.969236621677225\n",
      "ff_net_inc_basic_beft_xord - ff_oper_inc_aft_unusual: 0.9431458099258823\n",
      "ff_net_inc_basic_beft_xord - ff_ut_gross_inc: 0.8720777765746758\n",
      "ff_net_inc_basic_beft_xord - ff_ebit_bef_unusual: 0.8699487592005166\n",
      "ff_net_inc_basic_beft_xord - ff_ebitda_bef_unusual: 0.8263127016095722\n",
      "ff_net_inc_basic_beft_xord - ff_net_inc_dil_bef_unusual: 0.9514684905300393\n",
      "ff_net_inc_bef_xord_gr - ff_oper_inc_gr: 0.8399819104573022\n",
      "ff_net_inc_bef_xord_gr - ff_sales_gr: 0.9710377589928197\n",
      "ff_net_inc_dil - ff_net_inc_dil_aft_xord: 0.9727796378514042\n",
      "ff_net_inc_dil - ff_oper_inc_aft_unusual: 0.940381423678924\n",
      "ff_net_inc_dil - ff_ut_gross_inc: 0.8704328566544921\n",
      "ff_net_inc_dil - ff_ebit_bef_unusual: 0.8681345974102525\n",
      "ff_net_inc_dil - ff_ebitda_bef_unusual: 0.8246483245354339\n",
      "ff_net_inc_dil - ff_net_inc_dil_bef_unusual: 0.9549072580178767\n",
      "ff_net_inc_dil_aft_xord - ff_oper_inc_aft_unusual: 0.9094573107336443\n",
      "ff_net_inc_dil_aft_xord - ff_ut_gross_inc: 0.8429254502207775\n",
      "ff_net_inc_dil_aft_xord - ff_ebit_bef_unusual: 0.8368099375546646\n",
      "ff_net_inc_dil_aft_xord - ff_ebitda_bef_unusual: 0.7962359872405717\n",
      "ff_net_inc_dil_aft_xord - ff_net_inc_dil_bef_unusual: 0.9264574110268587\n",
      "ff_oper_inc_aft_unusual - ff_ut_gross_inc: 0.8921527379954305\n",
      "ff_oper_inc_aft_unusual - ff_ebit_bef_unusual: 0.9080302954245419\n",
      "ff_oper_inc_aft_unusual - ff_ebitda_bef_unusual: 0.8666645187838873\n",
      "ff_oper_inc_aft_unusual - ff_net_inc_dil_bef_unusual: 0.9125928119509755\n",
      "ff_oper_inc_gr - ff_sales_gr: 0.9708620705114914\n",
      "ff_oper_ps_net_cf - ff_sales_ps_gr: 0.9674250971349388\n",
      "ff_oper_ps_net_cf - ff_eps_dil_aft_xord: 0.9459148370201574\n",
      "ff_reinvest_rate - ff_roce: 0.824208124059292\n",
      "ff_sales_ps_gr - ff_eps_dil_aft_xord: 0.97552938156497\n",
      "ff_sales_ps_gr - ff_eps_dil_gr: 0.9999999986577801\n",
      "ff_ut_gross_inc - ff_ebit_bef_unusual: 0.9569454229264749\n",
      "ff_ut_gross_inc - ff_ebitda_bef_unusual: 0.9318650967526966\n",
      "ff_ut_gross_inc - ff_net_inc_dil_bef_unusual: 0.9537328142384244\n",
      "ff_xord - ff_xord_disc: 0.7596484400929154\n",
      "ff_ebit_bef_unusual - ff_ebitda_bef_unusual: 0.9703416892984836\n",
      "ff_ebit_bef_unusual - ff_net_inc_dil_bef_unusual: 0.9421259612541185\n",
      "ff_ebitda_bef_unusual - ff_net_inc_dil_bef_unusual: 0.9079356870278078\n",
      "ff_eps_dil_aft_xord - ff_eps_dil_gr: 0.806601248762903\n",
      "ff_bk_oper_inc_oth - ff_commiss_inc_net: 0.8195425861962115\n",
      "GDP - CPI: 0.8270119448078531\n",
      "Index(['fsym_id', 'date', 'ff_assets_com_eq', 'ff_assets_gr',\n",
      "       'ff_assets_oth_tot', 'ff_assets_per_emp', 'ff_bps_gr',\n",
      "       'ff_capex_assets', 'ff_capex_ps_cf', 'ff_cash_div_cf', 'ff_cash_roce',\n",
      "       'ff_cf_ps_gr', 'ff_cf_sales', 'ff_com_eq_gr', 'ff_com_eq_tcap',\n",
      "       'ff_debt_entrpr_val', 'ff_debt_lt_cf', 'ff_debt_st_x_curr_port',\n",
      "       'ff_dfd_tax_assets_lt', 'ff_dil_adj', 'ff_div_yld', 'ff_earn_yld',\n",
      "       'ff_ebit_oper_roa', 'ff_entrpr_val_sales', 'ff_for_assets_pct',\n",
      "       'ff_gross_cf_debt', 'ff_inc_adj', 'ff_inc_sund', 'ff_inc_tax_curr',\n",
      "       'ff_inc_tax_dfd', 'ff_int_exp_oth', 'ff_invest_cap', 'ff_invest_lt',\n",
      "       'ff_invest_st_tot', 'ff_ltd_tcap', 'ff_min_int_tcap', 'ff_mkt_val_gr',\n",
      "       'ff_net_inc_basic_aft_xord', 'ff_net_inc_bef_xord_gr',\n",
      "       'ff_net_inc_per_emp', 'ff_net_mgn_gr', 'ff_non_oper_exp',\n",
      "       'ff_oper_cf_fix_chrg', 'ff_oper_inc_tcap', 'ff_reinvest_rate',\n",
      "       'ff_roic', 'ff_tcap_assets', 'ff_ut_non_oper_inc_oth',\n",
      "       'ff_ut_operation_exp', 'ff_wkcap', 'ff_xord', 'ff_std_debt',\n",
      "       'ff_tang_assets_debt', 'ff_bk_oper_inc_oth', 'ff_bk_oper_inc_tot',\n",
      "       'ff_bk_non_oper_inc', 'ff_cf_roic', 'ff_liabs_lease', 'ff_fcf_yld',\n",
      "       'GDP', 'Unemployment_Rate', 'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def correlation_matrix(df):\n",
    "    # df =df.toPandas()\n",
    "    print(\"Converted to Pandas\")\n",
    "    corr_df = df.drop(['date','fsym_id'], axis=1)\n",
    "    corr_mat = corr_df.corr().abs()\n",
    "    mask = np.triu(np.ones_like(corr_mat))\n",
    "    plt.figure(figsize=(50, 40))\n",
    "    sns.heatmap(corr_mat, annot=True, cmap='coolwarm', fmt=\".2f\", mask=mask)\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('corr_matrix_tab.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Variable pairs with absolute correlation above 0.7:\")\n",
    "    corr_dict = {}\n",
    "    for i in range(len(corr_mat.columns)):\n",
    "        for j in range(i+1, len(corr_mat.columns)):\n",
    "            if abs(corr_mat.iloc[i, j]) >= 0.7:\n",
    "                print(f\"{corr_mat.columns[i]} - {corr_mat.columns[j]}: {corr_mat.iloc[i, j]}\")\n",
    "                if corr_mat.columns[i] not in corr_dict.keys():\n",
    "                    corr_dict[corr_mat.columns[i]] = [corr_mat.columns[j]]\n",
    "                else:\n",
    "                    corr_dict[corr_mat.columns[i]].append(corr_mat.columns[j])\n",
    "                    \n",
    "    for k,v in corr_dict.items():\n",
    "        if len(corr_dict[k]) >= 1:\n",
    "            for col in corr_dict[k]:\n",
    "                if col in df.columns:\n",
    "                    df=df.drop(col,axis=1)\n",
    "    \n",
    "                \n",
    "\n",
    "    print(df.columns)\n",
    "    return df\n",
    "\n",
    "    \n",
    "                \n",
    "df=correlation_matrix(df) #pandas now\n",
    "# df=correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f39b54dc-ccbc-43c6-bd3b-2d1d97595ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyterhub/lib/python3.10/site-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feats = df.columns[2:-1]\n",
    "df[feats] = df.groupby('fsym_id')[feats].transform(lambda x : x.fillna(method='ffill'))\n",
    "df[feats] = df.groupby('fsym_id')[feats].transform(lambda x: x.fillna(x.median()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da310543-4a40-4e70-8089-f4841414e9b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    }
   ],
   "source": [
    "# df = df.drop('ff_debt_com_eq', 'ff_debt_eq', '' 'ff_div_yld_secs', 'ff_earn_yld', 'ff_roa_ptx', 'ff_net_inc_basic_aft_xord', 'ff_net_inc_dil', 'ff_oper_inc_aft_unusual', \n",
    "#                         'ff_net_inc_dil_aft_xord', 'ff_net_inc_dil_bef_unusual', 'ff_ebit_bef_unusual', 'ff_eps_dil_gr', 'GDP', 'ff_bk_oper_inc_tot')\n",
    "feats = df.columns[2:-1]\n",
    "# write_features_file(feats)\n",
    "print(len(feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66977c59-6064-49a5-bedc-4dd7de5013f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fsym_id</th>\n",
       "      <th>date</th>\n",
       "      <th>ff_assets_com_eq</th>\n",
       "      <th>ff_assets_gr</th>\n",
       "      <th>ff_assets_oth_tot</th>\n",
       "      <th>ff_assets_per_emp</th>\n",
       "      <th>ff_bps_gr</th>\n",
       "      <th>ff_capex_assets</th>\n",
       "      <th>ff_capex_ps_cf</th>\n",
       "      <th>ff_cash_div_cf</th>\n",
       "      <th>...</th>\n",
       "      <th>ff_tang_assets_debt</th>\n",
       "      <th>ff_bk_oper_inc_oth</th>\n",
       "      <th>ff_bk_oper_inc_tot</th>\n",
       "      <th>ff_bk_non_oper_inc</th>\n",
       "      <th>ff_cf_roic</th>\n",
       "      <th>ff_liabs_lease</th>\n",
       "      <th>ff_fcf_yld</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Unemployment_Rate</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1.248053</td>\n",
       "      <td>7.348883</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>56.030954</td>\n",
       "      <td>78.273974</td>\n",
       "      <td>0.483765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>129.518779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>20.792882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.477425</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>1.016917</td>\n",
       "      <td>27.333701</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>56.273777</td>\n",
       "      <td>38.851847</td>\n",
       "      <td>0.483765</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>129.518779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.890</td>\n",
       "      <td>-32.489870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.477425</td>\n",
       "      <td>0.014049</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>1.016917</td>\n",
       "      <td>1213.345113</td>\n",
       "      <td>2.152</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>-492.849757</td>\n",
       "      <td>11.984993</td>\n",
       "      <td>1.472874</td>\n",
       "      <td>3321.316445</td>\n",
       "      <td>...</td>\n",
       "      <td>77.245294</td>\n",
       "      <td>-0.294125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.392</td>\n",
       "      <td>3.610653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-95.444700</td>\n",
       "      <td>0.006058</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>1.016917</td>\n",
       "      <td>7.348883</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>-48.051855</td>\n",
       "      <td>0.483800</td>\n",
       "      <td>0.060456</td>\n",
       "      <td>186.785493</td>\n",
       "      <td>...</td>\n",
       "      <td>70.526570</td>\n",
       "      <td>-1.071870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>75.952</td>\n",
       "      <td>19.546902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-11.974300</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>1.016917</td>\n",
       "      <td>76.903869</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>85.114102</td>\n",
       "      <td>2.179701</td>\n",
       "      <td>0.426602</td>\n",
       "      <td>238.208140</td>\n",
       "      <td>...</td>\n",
       "      <td>130.866314</td>\n",
       "      <td>-0.710952</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.144</td>\n",
       "      <td>21.835781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-22.886700</td>\n",
       "      <td>0.010414</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    fsym_id        date  ff_assets_com_eq  ff_assets_gr  ff_assets_oth_tot  \\\n",
       "0  B00FG1-R  2012-12-31          1.248053      7.348883              0.000   \n",
       "1  B00FG1-R  2013-12-31          1.016917     27.333701              0.000   \n",
       "2  B00FG1-R  2014-12-31          1.016917   1213.345113              2.152   \n",
       "3  B00FG1-R  2015-12-31          1.016917      7.348883              0.000   \n",
       "4  B00FG1-R  2016-12-31          1.016917     76.903869              0.000   \n",
       "\n",
       "   ff_assets_per_emp   ff_bps_gr  ff_capex_assets  ff_capex_ps_cf  \\\n",
       "0          10.576109   56.030954        78.273974        0.483765   \n",
       "1          10.576109   56.273777        38.851847        0.483765   \n",
       "2          10.576109 -492.849757        11.984993        1.472874   \n",
       "3          10.576109  -48.051855         0.483800        0.060456   \n",
       "4          10.576109   85.114102         2.179701        0.426602   \n",
       "\n",
       "   ff_cash_div_cf  ...  ff_tang_assets_debt  ff_bk_oper_inc_oth  \\\n",
       "0        0.000000  ...           129.518779            0.000000   \n",
       "1        0.000000  ...           129.518779            0.000000   \n",
       "2     3321.316445  ...            77.245294           -0.294125   \n",
       "3      186.785493  ...            70.526570           -1.071870   \n",
       "4      238.208140  ...           130.866314           -0.710952   \n",
       "\n",
       "   ff_bk_oper_inc_tot  ff_bk_non_oper_inc  ff_cf_roic  ff_liabs_lease  \\\n",
       "0                 0.0              -0.590   20.792882             0.0   \n",
       "1                 0.0              -8.890  -32.489870             0.0   \n",
       "2                 0.0              14.392    3.610653             0.0   \n",
       "3                 0.0              75.952   19.546902             0.0   \n",
       "4                 0.0              85.144   21.835781             0.0   \n",
       "\n",
       "   ff_fcf_yld       GDP  Unemployment_Rate  label  \n",
       "0   -8.477425  0.006181                7.9      0  \n",
       "1   -8.477425  0.014049                6.7      0  \n",
       "2  -95.444700  0.006058                5.6      0  \n",
       "3  -11.974300  0.001821                5.0      0  \n",
       "4  -22.886700  0.010414                4.7      0  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e576693-3d3c-4ba5-a6a4-fdcc04fca499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-10  # Set your desired epsilon value\n",
    "\n",
    "df.replace(0.0, epsilon, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6db4abf6-85c4-4a20-b6e1-ee45082bc5f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fsym_id</th>\n",
       "      <th>date</th>\n",
       "      <th>ff_assets_com_eq</th>\n",
       "      <th>ff_assets_gr</th>\n",
       "      <th>ff_assets_oth_tot</th>\n",
       "      <th>ff_assets_per_emp</th>\n",
       "      <th>ff_bps_gr</th>\n",
       "      <th>ff_capex_assets</th>\n",
       "      <th>ff_capex_ps_cf</th>\n",
       "      <th>ff_cash_div_cf</th>\n",
       "      <th>...</th>\n",
       "      <th>ff_tang_assets_debt</th>\n",
       "      <th>ff_bk_oper_inc_oth</th>\n",
       "      <th>ff_bk_oper_inc_tot</th>\n",
       "      <th>ff_bk_non_oper_inc</th>\n",
       "      <th>ff_cf_roic</th>\n",
       "      <th>ff_liabs_lease</th>\n",
       "      <th>ff_fcf_yld</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Unemployment_Rate</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1.248053</td>\n",
       "      <td>7.348883</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>56.030954</td>\n",
       "      <td>78.273974</td>\n",
       "      <td>0.483765</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>129.518779</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>-0.590</td>\n",
       "      <td>20.792882</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>-8.477425</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>1.016917</td>\n",
       "      <td>27.333701</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>56.273777</td>\n",
       "      <td>38.851847</td>\n",
       "      <td>0.483765</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>129.518779</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>-8.890</td>\n",
       "      <td>-32.489870</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>-8.477425</td>\n",
       "      <td>0.014049</td>\n",
       "      <td>6.7</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>1.016917</td>\n",
       "      <td>1213.345113</td>\n",
       "      <td>2.152000e+00</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>-492.849757</td>\n",
       "      <td>11.984993</td>\n",
       "      <td>1.472874</td>\n",
       "      <td>3.321316e+03</td>\n",
       "      <td>...</td>\n",
       "      <td>77.245294</td>\n",
       "      <td>-2.941250e-01</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>14.392</td>\n",
       "      <td>3.610653</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>-95.444700</td>\n",
       "      <td>0.006058</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>1.016917</td>\n",
       "      <td>7.348883</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>-48.051855</td>\n",
       "      <td>0.483800</td>\n",
       "      <td>0.060456</td>\n",
       "      <td>1.867855e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>70.526570</td>\n",
       "      <td>-1.071870e+00</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>75.952</td>\n",
       "      <td>19.546902</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>-11.974300</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>1.016917</td>\n",
       "      <td>76.903869</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>85.114102</td>\n",
       "      <td>2.179701</td>\n",
       "      <td>0.426602</td>\n",
       "      <td>2.382081e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>130.866314</td>\n",
       "      <td>-7.109520e-01</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>85.144</td>\n",
       "      <td>21.835781</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>-22.886700</td>\n",
       "      <td>0.010414</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    fsym_id       date  ff_assets_com_eq  ff_assets_gr  ff_assets_oth_tot  \\\n",
       "0  B00FG1-R 2012-12-31          1.248053      7.348883       1.000000e-10   \n",
       "1  B00FG1-R 2013-12-31          1.016917     27.333701       1.000000e-10   \n",
       "2  B00FG1-R 2014-12-31          1.016917   1213.345113       2.152000e+00   \n",
       "3  B00FG1-R 2015-12-31          1.016917      7.348883       1.000000e-10   \n",
       "4  B00FG1-R 2016-12-31          1.016917     76.903869       1.000000e-10   \n",
       "\n",
       "   ff_assets_per_emp   ff_bps_gr  ff_capex_assets  ff_capex_ps_cf  \\\n",
       "0          10.576109   56.030954        78.273974        0.483765   \n",
       "1          10.576109   56.273777        38.851847        0.483765   \n",
       "2          10.576109 -492.849757        11.984993        1.472874   \n",
       "3          10.576109  -48.051855         0.483800        0.060456   \n",
       "4          10.576109   85.114102         2.179701        0.426602   \n",
       "\n",
       "   ff_cash_div_cf  ...  ff_tang_assets_debt  ff_bk_oper_inc_oth  \\\n",
       "0    1.000000e-10  ...           129.518779        1.000000e-10   \n",
       "1    1.000000e-10  ...           129.518779        1.000000e-10   \n",
       "2    3.321316e+03  ...            77.245294       -2.941250e-01   \n",
       "3    1.867855e+02  ...            70.526570       -1.071870e+00   \n",
       "4    2.382081e+02  ...           130.866314       -7.109520e-01   \n",
       "\n",
       "   ff_bk_oper_inc_tot  ff_bk_non_oper_inc  ff_cf_roic  ff_liabs_lease  \\\n",
       "0        1.000000e-10              -0.590   20.792882    1.000000e-10   \n",
       "1        1.000000e-10              -8.890  -32.489870    1.000000e-10   \n",
       "2        1.000000e-10              14.392    3.610653    1.000000e-10   \n",
       "3        1.000000e-10              75.952   19.546902    1.000000e-10   \n",
       "4        1.000000e-10              85.144   21.835781    1.000000e-10   \n",
       "\n",
       "   ff_fcf_yld       GDP  Unemployment_Rate         label  \n",
       "0   -8.477425  0.006181                7.9  1.000000e-10  \n",
       "1   -8.477425  0.014049                6.7  1.000000e-10  \n",
       "2  -95.444700  0.006058                5.6  1.000000e-10  \n",
       "3  -11.974300  0.001821                5.0  1.000000e-10  \n",
       "4  -22.886700  0.010414                4.7  1.000000e-10  \n",
       "\n",
       "[5 rows x 62 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a3f8175-b763-4444-be76-6d1d999013c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = '2020-01-01'\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "train_df = df[df['date'] < split_date]\n",
    "test_df = df[df['date'] >= split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2a9a3e51-894e-4e9a-8836-aa837749fcdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n",
      "8437\n",
      "6626\n",
      "68\n",
      "485\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df.columns))\n",
    "print(len(train_df['fsym_id'].unique()))\n",
    "print(len(test_df['fsym_id'].unique()))\n",
    "print(len(test_df[test_df['label']==1]))\n",
    "print(len(train_df[train_df['label']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ccbe6b5-537a-4d03-bfe0-dea35492f651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tsfel\n",
    "\n",
    "def feature_extraction(df):\n",
    "    df = df.set_index('date')\n",
    "\n",
    "    \n",
    "    cfg = tsfel.get_features_by_domain(\"statistical\")\n",
    "    \n",
    "    result_dfs = []\n",
    "    for fsym_id, group_df in df.groupby('fsym_id'):\n",
    "        # Exclude 'fsym_id' column from group_df\n",
    "        # print(group_df.head())\n",
    "        # non_zero_cols = group_df.columns[(group_df != 0).any()]\n",
    "        # group_df = group_df[non_zero_cols]\n",
    "\n",
    "        if not group_df.empty:\n",
    "            try:\n",
    "                X = tsfel.time_series_features_extractor(cfg, group_df.drop(['fsym_id', 'label'], axis=1), verbose=0)\n",
    "                X['fsym_id'] = group_df['fsym_id'].iloc[0]\n",
    "                X['label'] = group_df['label'].sum()\n",
    "                result_dfs.append(X)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    final_result = pd.concat(result_dfs, ignore_index=True)\n",
    "    final_result.reset_index(drop=True, inplace=True)\n",
    "    return final_result\n",
    "\n",
    "train_df = feature_extraction(train_df)\n",
    "test_df = feature_extraction(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0bd36d3e-0ce5-4c71-ad49-0ab62e0811e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GDP_Absolute energy</th>\n",
       "      <th>GDP_Average power</th>\n",
       "      <th>GDP_ECDF Percentile Count_0</th>\n",
       "      <th>GDP_ECDF Percentile Count_1</th>\n",
       "      <th>GDP_ECDF Percentile_0</th>\n",
       "      <th>GDP_ECDF Percentile_1</th>\n",
       "      <th>GDP_ECDF_0</th>\n",
       "      <th>GDP_ECDF_1</th>\n",
       "      <th>GDP_ECDF_2</th>\n",
       "      <th>GDP_ECDF_3</th>\n",
       "      <th>...</th>\n",
       "      <th>ff_tcap_assets_ECDF_8</th>\n",
       "      <th>ff_tcap_assets_ECDF_9</th>\n",
       "      <th>ff_ut_non_oper_inc_oth_ECDF_8</th>\n",
       "      <th>ff_ut_non_oper_inc_oth_ECDF_9</th>\n",
       "      <th>ff_ut_operation_exp_ECDF_8</th>\n",
       "      <th>ff_ut_operation_exp_ECDF_9</th>\n",
       "      <th>ff_wkcap_ECDF_8</th>\n",
       "      <th>ff_wkcap_ECDF_9</th>\n",
       "      <th>ff_xord_ECDF_8</th>\n",
       "      <th>ff_xord_ECDF_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.011641</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>0.010414</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.020922</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.005892</td>\n",
       "      <td>0.013815</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002972</td>\n",
       "      <td>0.015643</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.005892</td>\n",
       "      <td>0.014006</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001926</td>\n",
       "      <td>0.014816</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>0.012435</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002442</td>\n",
       "      <td>0.016281</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.005728</td>\n",
       "      <td>0.013815</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2362 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   GDP_Absolute energy  GDP_Average power  GDP_ECDF Percentile Count_0  \\\n",
       "0             0.000815           0.011641                          1.0   \n",
       "1             0.001046           0.020922                          1.0   \n",
       "2             0.002972           0.015643                          4.0   \n",
       "3             0.001926           0.014816                          2.0   \n",
       "4             0.002442           0.016281                          3.0   \n",
       "\n",
       "   GDP_ECDF Percentile Count_1  GDP_ECDF Percentile_0  GDP_ECDF Percentile_1  \\\n",
       "0                          6.0               0.001821               0.010414   \n",
       "1                          4.0               0.005892               0.013815   \n",
       "2                         16.0               0.005892               0.014006   \n",
       "3                         11.0               0.001821               0.012435   \n",
       "4                         12.0               0.005728               0.013815   \n",
       "\n",
       "   GDP_ECDF_0  GDP_ECDF_1  GDP_ECDF_2  GDP_ECDF_3  ...  ff_tcap_assets_ECDF_8  \\\n",
       "0    0.125000    0.250000    0.375000    0.500000  ...                    NaN   \n",
       "1    0.166667    0.333333    0.500000    0.666667  ...                    NaN   \n",
       "2    0.050000    0.100000    0.150000    0.200000  ...               0.450000   \n",
       "3    0.071429    0.142857    0.214286    0.285714  ...               0.642857   \n",
       "4    0.062500    0.125000    0.187500    0.250000  ...               0.562500   \n",
       "\n",
       "   ff_tcap_assets_ECDF_9  ff_ut_non_oper_inc_oth_ECDF_8  \\\n",
       "0                    NaN                            NaN   \n",
       "1                    NaN                            NaN   \n",
       "2               0.500000                       0.450000   \n",
       "3               0.714286                       0.642857   \n",
       "4               0.625000                       0.562500   \n",
       "\n",
       "   ff_ut_non_oper_inc_oth_ECDF_9  ff_ut_operation_exp_ECDF_8  \\\n",
       "0                            NaN                         NaN   \n",
       "1                            NaN                         NaN   \n",
       "2                       0.500000                    0.450000   \n",
       "3                       0.714286                    0.642857   \n",
       "4                       0.625000                    0.562500   \n",
       "\n",
       "   ff_ut_operation_exp_ECDF_9  ff_wkcap_ECDF_8  ff_wkcap_ECDF_9  \\\n",
       "0                         NaN              NaN              NaN   \n",
       "1                         NaN              NaN              NaN   \n",
       "2                    0.500000         0.450000         0.500000   \n",
       "3                    0.714286         0.642857         0.714286   \n",
       "4                    0.625000         0.562500         0.625000   \n",
       "\n",
       "   ff_xord_ECDF_8  ff_xord_ECDF_9  \n",
       "0             NaN             NaN  \n",
       "1             NaN             NaN  \n",
       "2        0.450000        0.500000  \n",
       "3        0.642857        0.714286  \n",
       "4        0.562500        0.625000  \n",
       "\n",
       "[5 rows x 2362 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1037567-3afd-471b-8094-1e245cc7987c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GDP_Absolute energy</th>\n",
       "      <th>GDP_Average power</th>\n",
       "      <th>GDP_ECDF Percentile Count_0</th>\n",
       "      <th>GDP_ECDF Percentile Count_1</th>\n",
       "      <th>GDP_ECDF Percentile_0</th>\n",
       "      <th>GDP_ECDF Percentile_1</th>\n",
       "      <th>GDP_ECDF_0</th>\n",
       "      <th>GDP_ECDF_1</th>\n",
       "      <th>GDP_ECDF_2</th>\n",
       "      <th>GDP_ECDF_3</th>\n",
       "      <th>...</th>\n",
       "      <th>ff_oper_inc_tcap_ECDF_6</th>\n",
       "      <th>ff_reinvest_rate_ECDF_6</th>\n",
       "      <th>ff_roic_ECDF_6</th>\n",
       "      <th>ff_std_debt_ECDF_6</th>\n",
       "      <th>ff_tang_assets_debt_ECDF_6</th>\n",
       "      <th>ff_tcap_assets_ECDF_6</th>\n",
       "      <th>ff_ut_non_oper_inc_oth_ECDF_6</th>\n",
       "      <th>ff_ut_operation_exp_ECDF_6</th>\n",
       "      <th>ff_wkcap_ECDF_6</th>\n",
       "      <th>ff_xord_ECDF_6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.001934</td>\n",
       "      <td>0.048361</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.005728</td>\n",
       "      <td>0.017494</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.043756</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.005728</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.043756</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.005728</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002188</td>\n",
       "      <td>0.043756</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.005728</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001934</td>\n",
       "      <td>0.048361</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.005728</td>\n",
       "      <td>0.017494</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2185 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   GDP_Absolute energy  GDP_Average power  GDP_ECDF Percentile Count_0  \\\n",
       "0             0.001934           0.048361                          1.0   \n",
       "1             0.002188           0.043756                          1.0   \n",
       "2             0.002188           0.043756                          1.0   \n",
       "3             0.002188           0.043756                          1.0   \n",
       "4             0.001934           0.048361                          1.0   \n",
       "\n",
       "   GDP_ECDF Percentile Count_1  GDP_ECDF Percentile_0  GDP_ECDF Percentile_1  \\\n",
       "0                          4.0               0.005728               0.017494   \n",
       "1                          4.0               0.005728               0.017409   \n",
       "2                          4.0               0.005728               0.017409   \n",
       "3                          4.0               0.005728               0.017409   \n",
       "4                          4.0               0.005728               0.017494   \n",
       "\n",
       "   GDP_ECDF_0  GDP_ECDF_1  GDP_ECDF_2  GDP_ECDF_3  ...  \\\n",
       "0    0.200000    0.400000         0.6    0.800000  ...   \n",
       "1    0.166667    0.333333         0.5    0.666667  ...   \n",
       "2    0.166667    0.333333         0.5    0.666667  ...   \n",
       "3    0.166667    0.333333         0.5    0.666667  ...   \n",
       "4    0.200000    0.400000         0.6    0.800000  ...   \n",
       "\n",
       "   ff_oper_inc_tcap_ECDF_6  ff_reinvest_rate_ECDF_6  ff_roic_ECDF_6  \\\n",
       "0                      NaN                      NaN             NaN   \n",
       "1                      NaN                      NaN             NaN   \n",
       "2                      NaN                      NaN             NaN   \n",
       "3                      NaN                      NaN             NaN   \n",
       "4                      NaN                      NaN             NaN   \n",
       "\n",
       "   ff_std_debt_ECDF_6  ff_tang_assets_debt_ECDF_6  ff_tcap_assets_ECDF_6  \\\n",
       "0                 NaN                         NaN                    NaN   \n",
       "1                 NaN                         NaN                    NaN   \n",
       "2                 NaN                         NaN                    NaN   \n",
       "3                 NaN                         NaN                    NaN   \n",
       "4                 NaN                         NaN                    NaN   \n",
       "\n",
       "   ff_ut_non_oper_inc_oth_ECDF_6  ff_ut_operation_exp_ECDF_6  ff_wkcap_ECDF_6  \\\n",
       "0                            NaN                         NaN              NaN   \n",
       "1                            NaN                         NaN              NaN   \n",
       "2                            NaN                         NaN              NaN   \n",
       "3                            NaN                         NaN              NaN   \n",
       "4                            NaN                         NaN              NaN   \n",
       "\n",
       "   ff_xord_ECDF_6  \n",
       "0             NaN  \n",
       "1             NaN  \n",
       "2             NaN  \n",
       "3             NaN  \n",
       "4             NaN  \n",
       "\n",
       "[5 rows x 2185 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6cc4098-ebf6-4b62-a330-05af65838b21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# null_columns = train_df.columns[train_df.isnull().all()].tolist()\n",
    "# train_df2 = train_df.drop(null_columns, axis=1)\n",
    "# test_df2 = test_df.drop(null_columns, axis=1)\n",
    "common_columns = train_df.columns.intersection(test_df.columns)\n",
    "\n",
    "# Include only the common columns in each DataFrame\n",
    "train_df = train_df[common_columns]\n",
    "test_df = test_df[common_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a296c420-cdb2-4e09-ae15-99a7670fdae4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "null_columns = train_df.columns[train_df.isna().any()].tolist()\n",
    "train_df = train_df.drop(null_columns, axis=1)\n",
    "test_df = test_df.drop(null_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8567db0d-d536-498c-8764-606907df48b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.replace([np.inf, -np.inf], 0.0, inplace=True)\n",
    "test_df.replace([np.inf, -np.inf], 0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "27740f40-0f70-42f3-8613-96a982866460",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GDP_Absolute energy</th>\n",
       "      <th>GDP_Average power</th>\n",
       "      <th>GDP_ECDF Percentile Count_0</th>\n",
       "      <th>GDP_ECDF Percentile Count_1</th>\n",
       "      <th>GDP_ECDF Percentile_0</th>\n",
       "      <th>GDP_ECDF Percentile_1</th>\n",
       "      <th>GDP_ECDF_0</th>\n",
       "      <th>GDP_Entropy</th>\n",
       "      <th>GDP_Histogram_0</th>\n",
       "      <th>GDP_Histogram_1</th>\n",
       "      <th>...</th>\n",
       "      <th>ff_xord_Median</th>\n",
       "      <th>ff_xord_Median absolute deviation</th>\n",
       "      <th>ff_xord_Min</th>\n",
       "      <th>ff_xord_Peak to peak distance</th>\n",
       "      <th>ff_xord_Root mean square</th>\n",
       "      <th>ff_xord_Skewness</th>\n",
       "      <th>ff_xord_Standard deviation</th>\n",
       "      <th>ff_xord_Variance</th>\n",
       "      <th>fsym_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000253</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.015917</td>\n",
       "      <td>0.015917</td>\n",
       "      <td>0.015917</td>\n",
       "      <td>0.015917</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B3ZSGK-R</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000303</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B4Q7Y8-R</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000303</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>B6TRWR-R</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000303</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>BDP2V1-R</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000303</td>\n",
       "      <td>inf</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>0.017409</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>BYKV82-R</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1831 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   GDP_Absolute energy  GDP_Average power  GDP_ECDF Percentile Count_0  \\\n",
       "0             0.000253                inf                     0.015917   \n",
       "1             0.000303                inf                     0.017409   \n",
       "2             0.000303                inf                     0.017409   \n",
       "3             0.000303                inf                     0.017409   \n",
       "4             0.000303                inf                     0.017409   \n",
       "\n",
       "   GDP_ECDF Percentile Count_1  GDP_ECDF Percentile_0  GDP_ECDF Percentile_1  \\\n",
       "0                     0.015917               0.015917               0.015917   \n",
       "1                     0.017409               0.017409               0.017409   \n",
       "2                     0.017409               0.017409               0.017409   \n",
       "3                     0.017409               0.017409               0.017409   \n",
       "4                     0.017409               0.017409               0.017409   \n",
       "\n",
       "   GDP_ECDF_0  GDP_Entropy  GDP_Histogram_0  GDP_Histogram_1  ...  \\\n",
       "0         1.0          NaN              0.0              0.0  ...   \n",
       "1         1.0          NaN              0.0              0.0  ...   \n",
       "2         1.0          NaN              0.0              0.0  ...   \n",
       "3         1.0          NaN              0.0              0.0  ...   \n",
       "4         1.0          NaN              0.0              0.0  ...   \n",
       "\n",
       "   ff_xord_Median  ff_xord_Median absolute deviation   ff_xord_Min  \\\n",
       "0    1.000000e-10                                0.0  1.000000e-10   \n",
       "1    1.000000e-10                                0.0  1.000000e-10   \n",
       "2    1.000000e-10                                0.0  1.000000e-10   \n",
       "3    1.000000e-10                                0.0  1.000000e-10   \n",
       "4    1.000000e-10                                0.0  1.000000e-10   \n",
       "\n",
       "   ff_xord_Peak to peak distance  ff_xord_Root mean square  ff_xord_Skewness  \\\n",
       "0                            0.0              1.000000e-10               NaN   \n",
       "1                            0.0              1.000000e-10               NaN   \n",
       "2                            0.0              1.000000e-10               NaN   \n",
       "3                            0.0              1.000000e-10               NaN   \n",
       "4                            0.0              1.000000e-10               NaN   \n",
       "\n",
       "   ff_xord_Standard deviation  ff_xord_Variance   fsym_id         label  \n",
       "0                         0.0               0.0  B3ZSGK-R  1.000000e-10  \n",
       "1                         0.0               0.0  B4Q7Y8-R  1.000000e-10  \n",
       "2                         0.0               0.0  B6TRWR-R  1.000000e-10  \n",
       "3                         0.0               0.0  BDP2V1-R  1.000000e-10  \n",
       "4                         0.0               0.0  BYKV82-R  1.000000e+00  \n",
       "\n",
       "[5 rows x 1831 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "18bd9768-4d04-4fd3-8984-2f6a6187b6d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df[test_df['label']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19433828-fa32-4d67-815b-162686b458a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961\n",
      "961\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df.columns))\n",
    "print(len(test_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3c922f9f-bdd4-42aa-bc3a-7f6d8ae4048c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2362\n",
      "6862\n",
      "347\n",
      "12\n",
      "47\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df.columns))\n",
    "print(len(train_df))\n",
    "print(len(test_df))\n",
    "print(len(test_df[test_df['label']==1]))\n",
    "print(len(train_df[train_df['label']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8ba14ec-26eb-4d15-8f69-48c1f446e353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feats = train_df.columns[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73d9643f-ef4e-4eb0-8c37-88fd204bb19a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to Pandas\n",
      "{0: 0.5248085513720485, 1: 10.57717041800643}\n",
      " 19%|█▉        | 19/100 [04:02<17:15, 12.79s/trial, best loss: -0.4749228769989395]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       866\n",
      "           1       0.21      0.13      0.16        31\n",
      "\n",
      "    accuracy                           0.95       897\n",
      "   macro avg       0.59      0.56      0.57       897\n",
      "weighted avg       0.94      0.95      0.95       897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from sklearn import tree\n",
    "import shap\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from collections import Counter\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "from functools import reduce\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "def feature_importances(model, features):\n",
    "    feature_importances = model.feature_importances_\n",
    "\n",
    "    # print(\"Feature Importances:\")\n",
    "    # for feature, importance in zip(features, feature_importances):\n",
    "    #     print(f\"{feature}: {importance}\")\n",
    "\n",
    "    sorted_idx = np.argsort(feature_importances)\n",
    "    sorted_features = [features[i] for i in sorted_idx]\n",
    "    \n",
    "    half_len = (len(sorted_idx) // 4 ) * 3 # Calculate the index for the middle point\n",
    "\n",
    "    # Select the lowest 50% of features\n",
    "    selected_features = [features[i] for i in sorted_idx[:half_len]]\n",
    "\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.bar(range(len(feature_importances)), feature_importances[sorted_idx], align=\"center\")\n",
    "    plt.xticks(range(len(feature_importances)), sorted_features, rotation=45, ha=\"right\")\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"Feature Importance\")\n",
    "    plt.title(\"Feature Importances\")\n",
    "    plt.show()\n",
    "    return selected_features\n",
    "\n",
    "def model_testing(train_df, test_df, classifier):\n",
    "    seed = 42\n",
    "    print(\"Converted to Pandas\")\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "    X_train = train_df.drop(exclude_columns, axis=1)\n",
    "    y_train = train_df['label']\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    print(class_weight_dict)\n",
    "    \n",
    "    if classifier == 'LogisticRegression':\n",
    "        param_space = {\n",
    "            'C': hp.uniform('C', 0.01, 1.0) }\n",
    "        classifier_instance = LogisticRegression(class_weight = class_weight_dict, solver='sag', seed=42)\n",
    "        scaler = StandardScaler()\n",
    "        feats = X_train.columns\n",
    "        X_train[feats] = scaler.fit_transform(X_train[feats])\n",
    "        \n",
    "    elif classifier == 'RandomForest':\n",
    "        param_space = { \n",
    "            'n_estimators': hp.quniform('n_estimators', 100, 500, 1),\n",
    "            'max_depth': hp.quniform('max_depth', 5, 20, 1)\n",
    "        }\n",
    "        classifier_instance = RandomForestClassifier(class_weight = class_weight_dict, random_state=42)\n",
    "    elif classifier == 'GBT':\n",
    "        param_space = { 'n_estimators':hp.uniform('n_estimators',100,500),\n",
    "           'max_depth':hp.quniform('max_depth',5,20,1),\n",
    "           'min_samples_leaf':hp.quniform('min_samples_leaf',1,5,1),\n",
    "           'min_samples_split':hp.quniform('min_samples_split',2,6,1)}\n",
    "        classifier_instance = GradientBoostingClassifier(seed=42)\n",
    "    elif classifier == 'XGB':\n",
    "        param_space = { 'n_estimators':hp.quniform('n_estimators',100,500,1),\n",
    "           'max_depth':hp.quniform('max_depth',5,20,1)}\n",
    "        counter = Counter(y_train)\n",
    "        # estimate scale_pos_weight value\n",
    "        estimate = counter[0] / counter[1]\n",
    "        print('Estimate: %.3f' % estimate)\n",
    "        \n",
    "        classifier_instance = xgb.XGBClassifier(scale_pos_weight=estimate, seed=42)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported classifier\")\n",
    "    \n",
    "    def set_params(classifier, params):\n",
    "        if classifier == 'RandomForest':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            # params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            # params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        elif classifier == 'GBT':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        elif classifier == 'XGB':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            # params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            # params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        \n",
    "        else:\n",
    "            return params\n",
    "    \n",
    "    losses = []\n",
    "        \n",
    "    def objective(params):\n",
    "        params = set_params(classifier, params)\n",
    "        classifier_instance.set_params(**params)\n",
    "        \n",
    "        scores = cross_val_score(classifier_instance, X_train, y_train, cv=3, scoring='matthews_corrcoef')\n",
    "        score = -scores.mean()\n",
    "        losses.append(score)\n",
    "        return score\n",
    "\n",
    "    def report_average(*args):\n",
    "        report_list = list()\n",
    "        for report in args:\n",
    "            splited = [' '.join(x.split()) for x in report.split('\\n\\n')]\n",
    "            header = [x for x in splited[0].split(' ')]\n",
    "            data = np.array(splited[1].split(' ')).reshape(-1, len(header) + 1)\n",
    "            data = np.delete(data, 0, 1).astype(float)\n",
    "            rest = splited[2].split(' ')\n",
    "            accuarcy =np.array([0, 0, rest[1], rest[2]]).astype(float).reshape(-1, len(header))\n",
    "            macro_avg = np.array([rest[5:9]]).astype(float).reshape(-1, len(header))\n",
    "            weighted_avg = np.array([rest[11:]]).astype(float).reshape(-1, len(header))\n",
    "            #avg_total = np.array([x for x in avg]).astype(float).reshape(-1, len(header))\n",
    "            df = pd.DataFrame(np.concatenate((data, accuarcy,macro_avg,weighted_avg)), columns=header)\n",
    "            report_list.append(df)\n",
    "        res = reduce(lambda x, y: x.add(y, fill_value=0), report_list) / len(report_list)\n",
    "        res.to_csv(f'when_{classifier}_results.csv')\n",
    "        return res.rename(index={res.index[-3]: 'accuracy',res.index[-2]: 'macro_avg',res.index[-1]: 'weighted_avg'})\n",
    "    \n",
    "    \n",
    "    best_params = fmin(fn=objective, space=param_space, algo=tpe.suggest, max_evals=100, early_stop_fn=no_progress_loss(10))\n",
    "    \n",
    "    plt.plot(np.arange(len(losses)), losses)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss during Hyperopt Optimization')\n",
    "    plt.show()\n",
    "    \n",
    "    best_params = set_params(classifier, best_params)\n",
    "    classifier_instance.set_params(**best_params)\n",
    "    \n",
    "    classifier_instance.fit(X_train, y_train)\n",
    "    X_test = test_df.drop(exclude_columns, axis=1)\n",
    "    y_test = test_df['label']\n",
    "    preds = classifier_instance.predict(X_test)\n",
    "    \n",
    "    report = classification_report(y_test, preds)\n",
    "    print(report)\n",
    "    with open(f'report_{classifier}_if_pandas', \"w\") as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    return classifier_instance, X_train.columns.tolist(), X_train\n",
    "    \n",
    "    \n",
    "#     i = 0\n",
    "#     final_recall = None\n",
    "#     all_reports = []\n",
    "#     for train_index, test_index in tscv.split(X_train):\n",
    "#         x_train, x_test = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "#         Y_train, Y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        \n",
    "#         classifier_instance.fit(x_train, Y_train)\n",
    "        \n",
    "#         preds = classifier_instance.predict(x_test)\n",
    "#         report = classification_report(Y_test, preds)\n",
    "#         print(report)\n",
    "#         all_reports.append(report)\n",
    "#         # cm = confusion_matrix(Y_test, preds, labels=classifier_instance.classes_)\n",
    "#         # plt.figure(figsize=(8, 6))\n",
    "#         # sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
    "#         # plt.xlabel(\"Predicted\")\n",
    "#         # plt.ylabel(\"Actual\")\n",
    "#         # plt.title(\"Confusion Matrix\")\n",
    "#         # plt.show()\n",
    "#         # final_recall = recall_score(Y_test, preds, pos_label=1)\n",
    "#         # return recall_minority_class\n",
    "#     # return final_recall\n",
    "#     final_report = report_average(*all_reports)\n",
    "#     print(final_report)\n",
    "#     # print(\"MCC: \", matthews_corrcoef(true, preds))\n",
    "#     final_report.to_csv(f'report_{classifier}_if_pandas')\n",
    "#         # f.write(f\"\\n\\nMatthews Correlation Coefficient: {matthews_corrcoef(true, preds)}\")\n",
    "        \n",
    "#     return classifier_instance, X_train.columns.tolist(), X_train\n",
    "\n",
    "\n",
    "model, feats, X_train = model_testing(train_df, test_df, 'RandomForest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76602b7c-3a5e-4c5a-a903-9e20808a9e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_feats = feature_importances(model,feats, 'RandomForest')\n",
    "\n",
    "df2 = df.select(*top_feats, 'fsym_id', 'label')\n",
    "# df2 = df.drop(*feats_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc9139-0926-4acc-8b62-5db6cabcd2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model2, feats2, X_train2 = model_training_spark(df2, 'RandomForest')\n",
    "_ = feature_importances(model2,top_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ef5a58-f645-44a2-ba76-988859d95f93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "\n",
    "def shapley(model, train, test, model_name):\n",
    "    train = train.toPandas()\n",
    "    test=test.toPandas()\n",
    "    exclude_columns = ['fsym_id',  'label', 'features_vector']\n",
    "    X_train = train.drop(exclude_columns, axis=1)\n",
    "    X_test = test.drop(exclude_columns, axis=1)\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X_train)\n",
    "    shap.initjs()\n",
    "    print(shap_values.shape)\n",
    "    # shap.plots.waterfall(shap_values[0])\n",
    "    shap.summary_plot(shap_values, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name}_shap_det.png')\n",
    "    \n",
    "    \n",
    "# shapley(model, train_df, test_df, model_name='rf')\n",
    "    \n",
    "# shapley(model, train_df, test_df, model_name='rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1593aa9a-1445-4a89-abaa-99012fd63ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "\n",
    "def plot_model_performance(mdl, loss, metric):\n",
    "    x = pd.DataFrame(mdl.history).reset_index()\n",
    "    x = pd.melt(x, id_vars='index')\n",
    "    x['validation'] = (x['variable'].str[:4] == 'val_').replace({True:'validation',False:'training'})\n",
    "    x['loss'] = (x['variable'].str[-4:] == 'loss').replace({True:loss,False:metric})\n",
    "    g = sns.FacetGrid(x, col='loss', hue='validation',sharey=False)\n",
    "    g.map(sns.lineplot, 'index','value')\n",
    "    g.add_legend()\n",
    "    return g\n",
    "\n",
    "def nn_training(df):\n",
    "    train_df, test_df = t_t_split(df)\n",
    "\n",
    "    train_df = train_df.toPandas()\n",
    "    train_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Drop rows containing NaN values\n",
    "    train_df.dropna(axis=0, how='any', inplace=True)\n",
    "    \n",
    "    test_df = test_df.toPandas()\n",
    "    train_X = train_df.drop(['fsym_id', 'label'], axis=1).values\n",
    "    train_y = np.array(train_df['label'])\n",
    "    test_X = test_df.drop(['fsym_id', 'label'], axis=1).values\n",
    "    test_y = np.array(test_df['label'])\n",
    "    print(np.sum(test_y==1))\n",
    "    print(train_X, train_y)\n",
    "    \n",
    "    class_labels = np.unique(train_y)\n",
    "    class_weights = compute_class_weight('balanced', classes=class_labels, y=train_y.flatten())\n",
    "    class_weight_dict = dict(zip(class_labels, class_weights))\n",
    "    print(class_weight_dict)\n",
    "    \n",
    "\n",
    "    # Define the neural network model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(train_X.shape[1],)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),  # Additional Dense layer\n",
    "        tf.keras.layers.Dropout(0.5),  # Dropout layer for regularization\n",
    "        tf.keras.layers.Dense(16, activation='relu'),  # Another Dense layer\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    # model = keras.Sequential()\n",
    "    # model.add(layers.Dense(16,activation=\"relu\",input_shape=(train_X.shape[1],)))\n",
    "    # model.add(layers.Dense(8,activation=\"tanh\"))\n",
    "    # model.add(layers.Dense(1))\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    loss_fn = keras.losses.BinaryCrossentropy()\n",
    "    optimizer = keras.optimizers.Adam(\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    # early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    fit_model = model.fit(train_X, train_y, epochs=50, batch_size=32, validation_split=0.1, class_weight = class_weight_dict)\n",
    "    plot_model_performance(fit_model, 'bin_cross_entropy','accuracy')\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_acc = model.evaluate(test_X, test_y)\n",
    "    print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "    # Make predictions on new data\n",
    "    predictions = model.predict(test_X)\n",
    "    for i in range(len(predictions)):\n",
    "        predictions[i] = 1 if predictions[i] >= 0.5 else 0\n",
    "    print(classification_report(predictions, test_y.flatten()))\n",
    "    \n",
    "    # pred_df = pd.DataFrame()\n",
    "    # pred_df['prediction'] = predictions\n",
    "    # pred_df['label'] = test_y\n",
    "    # confusion_matrix_pandas(pred_df)\n",
    "    cm = confusion_matrix(test_y, predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "nn_training(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b557ef83-82be-4f6d-b70e-6f59fd115154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def anomaly_det(df):\n",
    "    \n",
    "    seed = 42\n",
    "    train_df, test_df = t_t_split(df)\n",
    "    train_df = train_df.toPandas()\n",
    "    test_df = test_df.toPandas()\n",
    "    print(\"Converted to Pandas\")\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "    \n",
    "    \n",
    "    features =train_df.columns[:-2]\n",
    "    print(len(features))\n",
    "    \n",
    "    num_pos = len(train_df[train_df['label']==1])\n",
    "    print(num_pos/len(train_df))\n",
    "    isol_for = IsolationForest(contamination=num_pos/len(train_df), random_state=42)\n",
    "    \n",
    "    isol_for.fit(train_df[features])\n",
    "\n",
    "    train_df['anomaly_scores'] = isol_for.decision_function(train_df[features])\n",
    "    train_df['anomaly'] = isol_for.predict(train_df[features])\n",
    "    train_df['preds'] = np.where(train_df['anomaly'] == 1, 0, 1)\n",
    "\n",
    "    test_df['anomaly_scores'] = isol_for.decision_function(test_df[features])\n",
    "    test_df['anomaly'] = isol_for.predict(test_df[features])\n",
    "    test_df['preds'] = np.where(test_df['anomaly'] == 1, 0, 1)\n",
    "    \n",
    "    print(f\"Classification Report: \")\n",
    "    print(classification_report(test_df['label'], test_df['preds']))\n",
    "    return test_df\n",
    "    \n",
    "    \n",
    "test_df_isol = anomaly_det(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9938e78b-a720-4512-b9ea-6a90ab073601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from CreateDataset import get_fund_data\n",
    "import math\n",
    "\n",
    "def plotting_stocks_pandas(df):\n",
    "    imploded_stocks = df[(df['label'] == 1) & (df['preds'] == 0)]\n",
    "    spark_df = spark.createDataFrame(imploded_stocks['fsym_id'].to_frame())\n",
    "    imp_prices = get_fund_data(spark_df)\n",
    "    \n",
    "    adj_pd = imp_prices.toPandas()\n",
    "    adj_pd['date'] = pd.to_datetime(adj_pd['date'])\n",
    "    list_to_plot = sorted(adj_pd['fsym_id'].unique().tolist())\n",
    "    \n",
    "    columns = 8\n",
    "    num_rows = math.ceil(len(list_to_plot) / columns)\n",
    "    fig, axs = plt.subplots(nrows=num_rows, ncols=columns, figsize=(35, 5*num_rows))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    i = 0\n",
    "    for t in list_to_plot:\n",
    "        temp_df = adj_pd[adj_pd['fsym_id']==t]\n",
    "        axs[i].plot(temp_df['date'], temp_df['adj_price'], label=t)\n",
    "\n",
    "        axs[i].legend()\n",
    "        #axs[i].text(0.5, -0.1, f'Volatility: {vol:.2f}', ha='center', transform=axs[i].transAxes)\n",
    "        i+=1\n",
    "        \n",
    "    for i in range(len(list_to_plot), num_rows * columns):\n",
    "        fig.delaxes(axs.flatten()[i])\n",
    "    \n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('implosions_not_detected_by_model.png')\n",
    "    \n",
    "# print(len(test_df_isol[test_df_isol['preds']==1]))\n",
    "# print(len(test_df_isol[test_df_isol['label']==1]))\n",
    "# plotting_stocks_pandas(test_df_isol)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de2078-0d47-4339-805d-67e3099f0a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Boruta import BorutaPy\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "def boruta_fs(train_df, model_name): #HOW DOES BORUTA ACC WORK?\n",
    "    train_df = train_df.toPandas()\n",
    "    X_train = train_df.drop(['fsym_id', 'date', 'label'], axis=1)\n",
    "    y_train = train_df['label']\n",
    "    \n",
    "    if model_name == 'rf':\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        model = GradientBoostingClassifier\n",
    "    feat_selector = BorutaPy(model, n_estimators='auto', verbose=1, random_state=1)\n",
    "    feat_selector.fit(X_train, y_train)\n",
    "    features = X_train.columns.tolist()\n",
    "    print(\"Number of features: \", len(features) )\n",
    "    feature_ranks = list(zip(features, feat_selector.ranking_, feat_selector.support_))\n",
    "    selected_features = []\n",
    "    for feat in feature_ranks:\n",
    "        print(f\"Feature: {feat[0]}, Rank: {feat[1]}, Keep: {feat[2]}\")\n",
    "        if feat[1] <= 5:\n",
    "            selected_features.append(feat[0])\n",
    "    print(\"Selected features: \", selected_features)\n",
    "    return selected_features\n",
    "\n",
    "rf_feats = boruta_fs(df, 'rf')\n",
    "# gbt_feats = boruta_fs(df, 'gbt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f66c89-d4c3-485b-93f5-c9fa821c0a82",
   "metadata": {},
   "source": [
    "### Investigating metrics that changed the most before and after implosions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f38521f-bf73-407e-a6f1-33f7a17eb635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1fc72f-006a-44b5-83fd-21ba1e382ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "# import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "\n",
    "def pct_change_df(df, big_string, table):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, t.Implosion_Start_Date, b.date, {big_string}\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN sym_ticker_region s ON s.fsym_id = t.fsym_id\n",
    "                LEFT JOIN {table} a ON s.fsym_id = a.fsym_id AND  YEAR(a.date) = YEAR(t.Implosion_Start_Date)\n",
    "                LEFT JOIN {table} b ON s.fsym_id = b.fsym_id AND  YEAR(b.date) = YEAR(t.Implosion_Start_Date)-1\n",
    "                ORDER BY t.fsym_id\n",
    "            \"\"\"\n",
    "    df1 = spark.sql(query1)\n",
    "    #print(df1.show())\n",
    "    df1 = df1.toPandas()\n",
    "    df1 = df1.drop(['fsym_id','Implosion_Start_Date','date'], axis=1)\n",
    "    \n",
    "    def remove_outliers(column):\n",
    "        Q1 = column.quantile(0.25)\n",
    "        Q3 = column.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return column[(column >= lower_bound) & (column <= upper_bound)]\n",
    "\n",
    "\n",
    "\n",
    "    df1 = df1.abs()\n",
    "    null_percentage = df1.isnull().sum() / len(df1)\n",
    "    columns_to_keep = null_percentage[null_percentage <= 0.3].index\n",
    "    df_nulls_removed = df1[columns_to_keep]\n",
    "    print(\"Columns kept: \", len(columns_to_keep)/len(df1.columns))\n",
    "    \n",
    "    df_no_outliers = df_nulls_removed.apply(remove_outliers)\n",
    "\n",
    "    \n",
    "    column_means_no_outliers = df_no_outliers.mean()\n",
    "    #column_means_no_outliers = column_means_no_outliers.dropna()\n",
    "    column_means_no_outliers = column_means_no_outliers.sort_values()\n",
    "    feats = column_means_no_outliers.tail(5)\n",
    "\n",
    "    print(\"Largest averages of differences between previous year and implosion year: \",feats)\n",
    "    return feats.index.tolist()\n",
    "    \n",
    "def avg_change_df(df, big_string, table):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, {big_string}\n",
    "                FROM temp_table t  \n",
    "                LEFT JOIN sym_ticker_region s ON s.fsym_id = t.fsym_id\n",
    "                LEFT JOIN {table} a ON s.fsym_id = a.fsym_id AND  YEAR(a.date) > YEAR(t.Implosion_Start_Date)\n",
    "                LEFT JOIN {table} b ON s.fsym_id = b.fsym_id AND  YEAR(b.date) < YEAR(t.Implosion_Start_Date)\n",
    "                GROUP BY t.fsym_id\n",
    "                ORDER BY t.fsym_id\n",
    "            \"\"\"\n",
    "    df1 = spark.sql(query1)\n",
    "    df1 = df1.toPandas()\n",
    "    df1 = df1.drop(['fsym_id'], axis=1)\n",
    "    \n",
    "    def remove_outliers(column):\n",
    "        Q1 = column.quantile(0.25)\n",
    "        Q3 = column.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return column[(column >= lower_bound) & (column <= upper_bound)]\n",
    "\n",
    "\n",
    "    df1 = df1.abs()\n",
    "    null_percentage = df1.isnull().sum() / len(df1)\n",
    "    columns_to_keep = null_percentage[null_percentage <= 0.3].index\n",
    "    df_nulls_removed = df1[columns_to_keep]\n",
    "    print(\"Columns kept: \", len(columns_to_keep)/len(df1.columns))\n",
    "    \n",
    "    df_no_outliers = df_nulls_removed.apply(remove_outliers)\n",
    "    \n",
    "    column_means_no_outliers = df_no_outliers.mean()\n",
    "    #column_means_no_outliers = column_means_no_outliers.dropna()\n",
    "    column_means_no_outliers = column_means_no_outliers.sort_values()\n",
    "    feats = column_means_no_outliers.tail(5)\n",
    "    print(\"Largest averages of differences in average before and after implosion date: \", feats)\n",
    "#     for feature in feats.index:\n",
    "#         before_implosion = df_no_outliers[feature][df_no_outliers.index.isin(df1[df1[feature].notnull() & (df1['date'] < df1['Implosion_Start_Date'])].index)]\n",
    "#         after_implosion = df_no_outliers[feature][df_no_outliers.index.isin(df1[df1[feature].notnull() & (df1['date'] > df1['Implosion_Start_Date'])].index)]\n",
    "        \n",
    "#         _, p_value = ttest_ind(before_implosion, after_implosion)\n",
    "        \n",
    "#         print(f\"T-test p-value for {feature}: {p_value}\")\n",
    "    return feats.index.tolist()\n",
    "\n",
    "def t_test():\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_metric_changes(filename, table):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df = df[df['Implosion_Start_Date'].notnull()]\n",
    "    df['Implosion_Start_Date'] = pd.to_datetime(df['Implosion_Start_Date']).dt.date\n",
    "    df['Implosion_End_Date'] = pd.to_datetime(df['Implosion_End_Date']).dt.date\n",
    "    cols = get_not_null_cols(df, table)\n",
    "    result_string = ', '.join('(a.' + item + '-b.' + item +')/b.'+item + ' AS ' + item for item in cols)\n",
    "    feats1 = pct_change_df(df, result_string, table) #change 1 year before\n",
    "    print(\"Features with greatest percentage change with year before implosion: \", feats1)\n",
    "    \n",
    "    result_string2 = ', '.join('(MEAN(a.' + item + ')-MEAN(b.' + item +'))/MEAN(b.'+item + ') AS ' + item for item in cols)\n",
    "    feats2 = avg_change_df(df, result_string2, table)\n",
    "    print(\"Features with greatest percentage change in mean before and after implosion\", feats2)\n",
    "    \n",
    "    write_features_file( list(set(feats1+feats2)) )\n",
    "\n",
    "\n",
    "get_metric_changes('imploded_stocks_price.csv', 'FF_ADVANCED_DER_AF')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7274d-d2b0-4016-a2b6-4f7633f51fe7",
   "metadata": {},
   "source": [
    "### Correlations with Market Value Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc63eb-7d98-4de8-b665-808509638bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from CreateDataset import get_feature_col_names, get_fund_data\n",
    "\n",
    "\n",
    "def corr_query(implosion_df, col_string, table): \n",
    "    df = get_fund_data(implosion_df)\n",
    "    df=df.withColumn('year', F.year('date'))\n",
    "    window_spec = Window.partitionBy('fsym_id', 'year').orderBy(col('date').desc())\n",
    "\n",
    "    df = df.withColumn('row_num', F.row_number().over(window_spec))\n",
    "\n",
    "    df = df.filter(col('row_num') == 1).orderBy('date') #should we compare correlations with market val?\n",
    "    #should we do quarterly?\n",
    "    \n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, t.adj_price, t.Market_Value, t.date, {col_string}\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id AND YEAR(t.date)=YEAR(a.date)\n",
    "                ORDER BY t.fsym_id, t.date\n",
    "            \"\"\"\n",
    " \n",
    "    q_df = spark.sql(query1)\n",
    "    #q_df.show()\n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy('date')\n",
    "    \n",
    "    q_df = q_df.withColumn(\"return_market_val\", (F.col('Market_Value') - F.lag('Market_Value').over(window_spec)) / F.lag('Market_Value').over(window_spec))\n",
    "    q_df = q_df.withColumn(\"return\", (F.col('adj_price') - F.lag('adj_price').over(window_spec)) / F.lag('adj_price').over(window_spec))\n",
    "    \n",
    "    return_columns = [c[2:] for c in col_string.split(\", \")]\n",
    "    mean_corrs = []\n",
    "    corr_vals = []\n",
    "    #I THINK U NEED TO GROUP BY DATE AND THEN CALCULATE CORRELATIONS\n",
    "\n",
    "    for column in return_columns:\n",
    "        return_col_name = f\"return_{column}\"\n",
    "        corr_col_name = f\"corr_with_{column}\"\n",
    "        q_df = q_df.withColumn(return_col_name, (F.col(column) - F.lag(column).over(window_spec)) / F.lag(column).over(window_spec))\n",
    "        q_df = q_df.withColumn(column, F.corr(return_col_name, 'return_market_val').over(window_spec)) #calculating correlations with market value return\n",
    "        q_df = q_df.drop(*[return_col_name])\n",
    "    q_df = q_df.drop(*['return_market_val', 'return'])\n",
    "    q_df = q_df.select(q_df.columns[4:])\n",
    "    mean_corrs = q_df.agg(*[F.mean(F.abs(F.col(column))).alias(column) for column in q_df.columns])\n",
    "    # mean_corrs.show()\n",
    "    \n",
    "    return mean_corrs.toPandas()\n",
    "\n",
    "def corr_analysis(table):\n",
    "    imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "    imp_df_price = imp_df_price.loc[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "    cols = get_not_null_cols(imp_df_price, 'FF_ADVANCED_DER_AF')\n",
    "    result_string = ', '.join('a.' + item for item in cols)\n",
    "    mean_corrs_df = corr_query(spark.createDataFrame(imp_df_price), result_string, 'FF_ADVANCED_DER_AF')\n",
    "    mean_corrs = mean_corrs_df.to_dict(orient='records')\n",
    "    sorted_corrs = dict(sorted(mean_corrs[0].items(), key=lambda item: item[1], reverse=True))\n",
    "    top_records = list(sorted_corrs.items())[:5]\n",
    "    top_10 = []\n",
    "    for r in top_records:\n",
    "        top_10.append(r[0])\n",
    "    print(top_10)\n",
    "    current_feature_list = get_feature_col_names()\n",
    "    new_feature_list = list(set(current_feature_list + top_10))\n",
    "    \n",
    "    write_features_file(new_feature_list)\n",
    "    \n",
    "    \n",
    "corr_analysis('FF_Advanced_Der_AF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3722ccc-0c58-4599-a464-6e153f4e1f13",
   "metadata": {},
   "source": [
    "### Adding the Extra Features From Literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586949c-01ca-4b25-88c1-1488355015e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "imp_df_price['Implosion_Start_Date'] = pd.to_datetime(imp_df_price['Implosion_Start_Date'])\n",
    "imp_df_price['Implosion_End_Date'] = pd.to_datetime(imp_df_price['Implosion_End_Date'])\n",
    "available_feats = get_not_null_cols(imp_df_price)\n",
    "extra_feats = ['ff_capex_assets', 'ff_gross_cf_debt', 'ff_mkt_val_gr']\n",
    "\n",
    "current_feats = get_feature_col_names()\n",
    "final_feats = list(set(current_feats + extra_feats))\n",
    "write_features_file(final_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca6a8a0-1c51-42e5-8a36-18044f9e9bc4",
   "metadata": {},
   "source": [
    "### Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203b078-0417-4933-afe8-4442a98809ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(all_feats=False, imploded_only=False):\n",
    "    df = get_tabular_dataset(all_feats=all_feats, imploded_only=imploded_only)\n",
    "    df = forward_fill(df)\n",
    "    print(\"Number of rows: \", df.count())\n",
    "    print(\"Number of positives: \", df.filter(F.col('label')==1).count())\n",
    "    df=df.fillna(0.0)\n",
    "    print(\"Number of rows after dropping nulls: \", df.count())\n",
    "    print(\"Number of positives after dropping nulls: \", df.filter(F.col('label')==1).count())\n",
    "    return df\n",
    "\n",
    "\n",
    "def forward_fill(df):\n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy('date')\n",
    "    feature_cols = df.columns[2:-1]\n",
    "    for c in feature_cols:\n",
    "        df = df.withColumn(\n",
    "            c, F.last(c, ignorenulls=True).over(window_spec)\n",
    "        )\n",
    "    return df.orderBy('fsym_id','date')\n",
    "\n",
    "df = get_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0e0a7-413f-4d65-9fec-31092302bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Boruta import BorutaPy\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "def boruta_fs(train_df, model_name): #HOW DOES BORUTA ACC WORK?\n",
    "    train_df = train_df.toPandas()\n",
    "    X_train = train_df.drop(['fsym_id', 'date', 'label'], axis=1)\n",
    "    y_train = train_df['label']\n",
    "    \n",
    "    if model_name == 'rf':\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        model = GradientBoostingClassifier\n",
    "    feat_selector = BorutaPy(model, n_estimators='auto', verbose=2, random_state=1)\n",
    "    feat_selector.fit(X_train, y_train)\n",
    "    features = X_train.columns.tolist()\n",
    "    print(\"Number of features: \", len(features) )\n",
    "    feature_ranks = list(zip(features, feat_selector.ranking_, feat_selector.support_))\n",
    "    selected_features = []\n",
    "    for feat in feature_ranks:\n",
    "        print(f\"Feature: {feat[0]}, Rank: {feat[1]}, Keep: {feat[2]}\")\n",
    "        if feat[1] <= 5:\n",
    "            selected_features.append(feat[0])\n",
    "    print(\"Selected features: \", selected_features)\n",
    "    return selected_features\n",
    "\n",
    "rf_feats = boruta_fs(df, 'rf')\n",
    "gbt_feats = boruta_fs(df, 'gbt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f1dd31-27ce-42b8-ac91-41d2ebf9876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_features = get_feature_col_names()\n",
    "# for f in boruta_features:\n",
    "#     if f in current_features:\n",
    "#         print(f)\n",
    "# final_features = list(set(boruta_features + current_features))\n",
    "# write_features_file(final_features) #in the feature selection pipeline, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c21d5-a37f-4211-a2dc-a32a14a41cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def correlation_matrix(df):\n",
    "    df =df.toPandas()\n",
    "    print(\"Converted to Pandas\")\n",
    "    corr_df = df.drop(['date','fsym_id'], axis=1)\n",
    "    corr_mat = corr_df.corr()\n",
    "    mask = np.triu(np.ones_like(corr_mat))\n",
    "    plt.figure(figsize=(50, 40))\n",
    "    sns.heatmap(corr_mat, annot=True, cmap='coolwarm', fmt=\".2f\", mask=mask)\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('corr_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Variable pairs with absolute correlation above 0.7:\")\n",
    "    for i in range(len(corr_mat.columns)):\n",
    "        for j in range(i+1, len(corr_mat.columns)):\n",
    "            if abs(corr_mat.iloc[i, j]) >= 0.7:\n",
    "                print(f\"{corr_mat.columns[i]} - {corr_mat.columns[j]}: {corr_mat.iloc[i, j]}\")\n",
    "                \n",
    "# correlation_matrix(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ea6a0-81ff-4b2b-9dd0-f6fff61d7fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('ff_div_yld_secs', 'ff_earn_yld', 'ff_roa_ptx', 'ff_net_inc_basic_aft_xord', 'ff_net_inc_dil', 'ff_oper_inc_aft_unusual', \n",
    "                        'ff_net_inc_dil_aft_xord', 'ff_net_inc_dil_bef_unusual', 'ff_ebit_bef_unusual', 'ff_eps_dil_gr', 'GDP', 'ff_bk_oper_inc_tot')\n",
    "feats = df.columns[2:-1]\n",
    "# write_features_file(feats)\n",
    "feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb6e21-44f3-4b26-b63b-a45b4ba8056a",
   "metadata": {},
   "source": [
    "### Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385f503-98bc-4b81-938c-27e8225f53b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_dates(imp_df_price):\n",
    "    price_data = get_fund_data(spark.createDataFrame(imp_df_price))\n",
    "    #cols = get_not_null_cols(imp_df_price, 'FF_ADVANCED_DER_AF')\n",
    "    #result_string = ', '.join('a.' + item for item in cols)\n",
    "    \n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy(col('p_date'))\n",
    "\n",
    "    price_data = price_data.withColumn('row_num', F.row_number().over(window_spec))\n",
    "    price_data.show()\n",
    "\n",
    "    price_data = price_data.filter(col('row_num') == 1).orderBy(col('p_date').desc())\n",
    "    price_data.show()\n",
    "    \n",
    "    start_dates = price_data.groupBy('year').count().orderBy('year')\n",
    "    years = [row['year'] for row in start_dates.collect()]\n",
    "    counts = [row['count'] for row in start_dates.collect()]\n",
    "    plt.bar(years, counts)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Start Dates Count per Year')\n",
    "    plt.show()\n",
    "    #start_dates.show(25)\n",
    "    \n",
    "def null_vals(imp_df_price, table):\n",
    "    price_data = get_fund_data(spark.createDataFrame(imp_df_price))\n",
    "    cols = get_not_null_cols(imp_df_price, table)\n",
    "    col_string = ', '.join('a.' + item for item in cols)\n",
    "    price_data.createOrReplaceTempView('temp_table')\n",
    "    null_counts = []\n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, t.split_adj_price, t.Market_Value, t.p_date, {col_string}\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id AND YEAR(t.p_date)=YEAR(a.date)\n",
    "                ORDER BY t.fsym_id, t.p_date\n",
    "            \"\"\"\n",
    "    full_df = spark.sql(query1)\n",
    "    for column in cols:\n",
    "        null_count = full_df.select(column).filter(col(column).isNull()).count()\n",
    "        null_counts.append((column, null_count))\n",
    "    null_counts_df = pd.DataFrame(null_counts, columns=['Column', 'Null Count'])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(null_counts_df['Column'], null_counts_df['Null Count'])\n",
    "    plt.xlabel('Column')\n",
    "    plt.ylabel('Null Count')\n",
    "    plt.title('Null Counts for Each Column')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # null_counts = price_data.groupBy('year').agg(F.sum(col('p_price').isNull().cast('int')).alias('null_count'))\n",
    "    # null_counts.show()\n",
    "    \n",
    "imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "imp_df_price_imploded = imp_df_price.loc[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "start_dates(imp_df_price)\n",
    "start_dates(imp_df_price_imploded)\n",
    "\n",
    "#null_vals(imp_df_price, 'FF_ADVANCED_DER_AF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345755a7-1bf1-442d-9418-576cb9688733",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "imp_df_test = imp_df_price[imp_df_price['fsym_id']=='H7CTYF-R']\n",
    "df = get_fund_data(spark.createDataFrame(imp_df_test))\n",
    "df.show(1000)\n",
    "imp_df_imp = imp_df_price[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "print(len(imp_df_imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730ca87-195a-49a3-87e3-60f22f57b015",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df_imp = imp_df_price[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "print(len(imp_df_imp))\n",
    "print(len(imp_df_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19776f2-22bb-4ab6-a748-cb9928010b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols():\n",
    "    df_metrics = ps.DataFrame(spark.sql(\"SELECT * FROM FF_BASIC_AF LIMIT 10\")) #get all the metrics\n",
    "    cols = []\n",
    "    for c in df_metrics.columns:\n",
    "        if df_metrics[c].dtype=='float64':#get all the metrics we can calculate correlations with\n",
    "            cols.append(c)\n",
    "    return cols\n",
    "\n",
    "#%change average of each feature plotted for pharmacy industry\n",
    "def industry_analysis():\n",
    "    stock_df = get_all_stocks_df()\n",
    "    #stock_df = pd.read_csv('imploded_stocks.csv')\n",
    "    #stock_df = spark.createDataFrame(stock_df)\n",
    "    cols = ['ff_gross_inc', 'ff_sales', 'FF_OPER_EXP_TOT', 'FF_CASH_ST']\n",
    "    col_string = ', '.join('a.' + item for item in cols)\n",
    "    stock_df.createOrReplaceTempView(\"temp_table\")\n",
    "    q = f\"\"\"SELECT e.factset_industry_desc, t.ticker_region, a.date, {col_string} FROM temp_table t\n",
    "    LEFT JOIN FF_BASIC_AF a ON a.fsym_id = t.fsym_id\n",
    "    LEFT JOIN sym_coverage sc ON sc.fsym_id = t.fsym_id\n",
    "    LEFT JOIN ff_sec_entity_hist c on c.fsym_id=sc.fsym_security_id\n",
    "    LEFT JOIN sym_entity_sector d on d.factset_entity_id=c.factset_entity_id\n",
    "    LEFT JOIN factset_industry_map e on e.factset_industry_code=d.industry_code\n",
    "    WHERE a.date >= \"2009-01-01\" AND e.factset_industry_desc=\"Regional Banks\"\n",
    "    ORDER BY t.ticker_region,a.date\"\"\"\n",
    "    ind_df = spark.sql(q)\n",
    "    #print(ind_df.show(10))\n",
    "    ind_df =ind_df.toPandas()\n",
    "    ind_df['date'] = pd.to_datetime(ind_df['date'])\n",
    "    new_cols = []\n",
    "    for column in cols:\n",
    "        ind_df[f'{column}_percentage_change'] = ind_df.groupby('ticker_region')[column].pct_change() * 100\n",
    "        ind_df[f'{column}_percentage_change'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        ind_df.drop(column, axis=1, inplace=True)\n",
    "        new_cols.append(f'{column}_percentage_change')\n",
    "    ind_df['year'] = ind_df['date'].dt.year\n",
    "    avg_pct_change = ind_df.groupby(['year'])[new_cols].mean().reset_index()\n",
    "    print(avg_pct_change.head(20))\n",
    "    num_rows = (len(new_cols) + 1) // 2  # Adjust the number of rows as needed\n",
    "    num_cols = 2\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n",
    "    for i,column in enumerate(new_cols):\n",
    "        row = i//num_cols\n",
    "        col = i % num_cols \n",
    "        axes[row,col].plot(avg_pct_change['year'], avg_pct_change[column])\n",
    "        axes[row, col].set_title(f'Avg {column} Percentage Change Over Time')\n",
    "        axes[row, col].set_xlabel('Year')\n",
    "        axes[row, col].set_ylabel(f'Avg {column} Percentage Change')\n",
    "        axes[row, col].grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#industry_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700de0e-2f78-48aa-8c16-e12e167d67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#YOU'VE DONE WORST CHANGES NOW FIND OUT WHICH ONES DECREASE CONSISTENTLY\n",
    "#ALSO FIGURE OUT MEANS BEFORE PERIOD AND AFTER PERIOD USING QUARTERLY AND COMPARE DIFF\n",
    "#FINALLY WITH A HUGE LIST USE BORUTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b54c9-5ddc-4459-afff-5247cc6b7b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_not_null_cols(df, table='FF_ADVANCED_DER_AF'):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query1 = f\"\"\"SELECT t.fsym_id, a.*\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id\n",
    "                ORDER BY t.fsym_id, a.date\n",
    "            \"\"\"\n",
    "    #we get all the available dates per stock, so these null values are only within the timeframe available\n",
    "    q_df = spark.sql(query1)\n",
    "    column_types = q_df.dtypes\n",
    "    null_pcts = []\n",
    "    for c, dtype in zip(q_df.columns, column_types):\n",
    "        if dtype[1] == 'double':\n",
    "            null_count = q_df.filter(F.col(c).isNull()).count()\n",
    "            null_pcts.append(null_count/q_df.count())\n",
    "\n",
    "\n",
    "    columns_to_drop = [col_name for col_name, null_pct, dtype in zip(q_df.columns, null_pcts, column_types) if null_pct > 0.2 or dtype[1]!='double']\n",
    "\n",
    "    q_df = q_df.drop(*columns_to_drop)\n",
    "\n",
    "    cols = q_df.columns\n",
    "    print(cols)\n",
    "\n",
    "    return cols\n",
    "    \n",
    "df = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "df = df.loc[df['Implosion_Start_Date'].notnull()]\n",
    "get_not_null_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d7b62c-becd-45ee-bcb2-8c8c59e8e0b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4997ed0d-ad2e-4ec5-9b4b-7818b3a857f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
