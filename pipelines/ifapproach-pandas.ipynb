{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe610780-dc3d-4ac8-9dd1-914ed92696c0",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6802cbd1-16cf-4e4a-8137-9894bdec78e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop-3.2.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.7-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "2024-02-15 20:45:55,666 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-02-15 20:45:58,564 WARN spark.ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Database(name='2023_11_01', description='FactSet data snapshot for 2023_11_01', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_01'),\n",
       " Database(name='2023_11_02', description='FactSet data snapshot for 2023_11_02', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_02'),\n",
       " Database(name='2023_11_03', description='FactSet data snapshot for 2023_11_03', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_03'),\n",
       " Database(name='2023_11_14', description='FactSet data snapshot for 2023_11_14', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_14'),\n",
       " Database(name='2023_11_19', description='FactSet data snapshot for 2023_11_19', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_19'),\n",
       " Database(name='2023_11_22', description='FactSet data snapshot for 2023_11_22', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_22'),\n",
       " Database(name='2024_01_25', description='FactSet data snapshot for 2024_01_25', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2024_01_25'),\n",
       " Database(name='2024_02_02', description='FactSet data snapshot for 2024_02_02', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2024_02_02'),\n",
       " Database(name='2024_02_03', description='FactSet data snapshot for 2024_02_03', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2024_02_03'),\n",
       " Database(name='default', description='Default Hive database', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"List available databases and tables\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7c46d2-5935-44d2-a7a2-37563aa77b57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2886cd22-2c06-4882-b434-e5ed726788c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.sql(\"USE 2023_11_02\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd18af3-d36d-4088-aa8c-6c410c3322ae",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694d984f-e6b5-42b9-afb9-1c9d98a7fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_all_stocks_df():\n",
    "    query = f\"\"\"SELECT s.ticker_region, s.fsym_id FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_not_null_cols(df, table='FF_ADVANCED_DER_AF'):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query1 = f\"\"\"SELECT t.fsym_id AS fsym_id2, a.*\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id\n",
    "                WHERE a.date > '2000-01-01'\n",
    "                ORDER BY t.fsym_id, a.date\n",
    "            \"\"\"\n",
    "    #we get all the available dates per stock, so these null values are only within the timeframe available\n",
    "    q_df = spark.sql(query1)\n",
    "    q_df = q_df.drop('date', 'adjdate', 'fsym_id2', 'fsym_id')\n",
    "    num_rows = q_df.count()\n",
    "    column_types = q_df.dtypes\n",
    "    good_cols = []\n",
    "    selected_columns = [F.col(c) for c, c_type in zip(q_df.columns, column_types) if c_type[1] == 'double']\n",
    "    q_df = q_df.select(selected_columns)\n",
    "    count_df = q_df.select( [(F.count(F.when(F.isnan(c) | F.col(c).isNull(), c))/num_rows).alias(c) for c in q_df.columns])\n",
    "    count_dict = count_df.first().asDict()\n",
    "    filtered_keys = [key for key, value in count_dict.items() if value <= 0.25]\n",
    "    return filtered_keys\n",
    "#     for c, c_type in zip(q_df.columns, column_types):\n",
    "#         if c_type[1] == 'double':\n",
    "#             null_count = F.sum(F.when(F.isnan(F.col(c)) | F.col(c).isNull(), 1).otherwise(0))\n",
    "#             null_pct = (null_count / num_rows).alias(f\"{c}_null_pct\")\n",
    "#             q_df_agg = q_df.agg(null_pct)\n",
    "#             actual_pct = q_df_agg.collect()[0][0]\n",
    "#             if actual_pct < 0.25:\n",
    "#                 good_cols.append(c)\n",
    "            \n",
    "#     return good_cols\n",
    "\n",
    "\n",
    "def write_features_file(data_list, csv_file_path='features.csv'):\n",
    "    data_list = [data_list]\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row in data_list:\n",
    "            writer.writerow(row)\n",
    "    print(\"Features written: \", data_list[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c41e5b-c43f-41cc-a9bd-c0e2fe6019cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ztewari/Stock-Implosion-Prediction-FYP\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "main_dir = os.path.dirname(curr_dir)\n",
    "print(main_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61704c74-e53c-424b-9a8f-78ea6f9d34ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "2024-02-15 20:46:24,555 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ff_assets_com_eq', 'ff_assets_eq', 'ff_assets_gr', 'ff_assets_oth_tot', 'ff_assets_per_emp', 'ff_bps_gr', 'ff_capex_assets', 'ff_capex_ps_cf', 'ff_cash_div_cf', 'ff_cash_roce', 'ff_cf_ps_gr', 'ff_cf_sales', 'ff_com_eq_gr', 'ff_com_eq_tcap', 'ff_debt_com_eq', 'ff_debt_entrpr_val', 'ff_debt_eq', 'ff_debt_lt_cf', 'ff_debt_st_x_curr_port', 'ff_dfd_tax_assets_lt', 'ff_dil_adj', 'ff_div_yld', 'ff_div_yld_secs', 'ff_earn_yld', 'ff_ebit_oper_roa', 'ff_entrpr_val_sales', 'ff_eps_basic_gr', 'ff_fix_assets_com_eq', 'ff_for_assets_pct', 'ff_for_sales_pct', 'ff_free_ps_cf', 'ff_gross_cf_debt', 'ff_inc_adj', 'ff_inc_sund', 'ff_inc_tax_curr', 'ff_inc_tax_dfd', 'ff_int_exp_oth', 'ff_invest_cap', 'ff_invest_lt', 'ff_invest_st_tot', 'ff_ltd_com_eq', 'ff_ltd_tcap', 'ff_min_int_tcap', 'ff_net_cf_debt', 'ff_net_inc_basic_aft_xord', 'ff_net_inc_basic_beft_xord', 'ff_net_inc_bef_xord_gr', 'ff_net_inc_dil', 'ff_net_inc_dil_aft_xord', 'ff_net_inc_per_emp', 'ff_non_oper_exp', 'ff_oper_cf_fix_chrg', 'ff_oper_inc_aft_unusual', 'ff_oper_inc_gr', 'ff_oper_inc_tcap', 'ff_oper_ps_net_cf', 'ff_pfd_stk_tcap', 'ff_reinvest_rate', 'ff_roa_ptx', 'ff_roce', 'ff_roic', 'ff_sales_gr', 'ff_sales_per_emp', 'ff_sales_ps_gr', 'ff_tcap_assets', 'ff_tot_debt_tcap_std', 'ff_ut_gross_inc', 'ff_ut_non_oper_inc_oth', 'ff_ut_operation_exp', 'ff_wkcap', 'ff_wkcap_pct', 'ff_xord', 'ff_xord_disc', 'ff_invest_receiv_lt', 'ff_ebit_bef_unusual', 'ff_ebitda_bef_unusual', 'ff_eps_dil_aft_xord', 'ff_eps_dil_gr', 'ff_psales_dil', 'ff_std_debt', 'ff_tang_assets_debt', 'ff_net_inc_dil_bef_unusual', 'ff_bk_oper_inc_oth', 'ff_bk_oper_inc_tot', 'ff_bk_non_oper_inc', 'ff_commiss_inc_net', 'ff_cf_roic', 'ff_liabs_lease', 'ff_fcf_yld', 'GDP', 'Unemployment_Rate', 'CPI']\n",
      "df retrieved\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from CreateDataset import get_tabular_dataset, get_feature_col_names, get_not_null_cols\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "# import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def plot_nulls(df):\n",
    "    null_counts = df.agg(*[\n",
    "    (1 - (F.count(c) / F.count('*'))).alias(c + '_nulls') for c in df.columns])\n",
    "    null_counts_pd = null_counts.toPandas().transpose()\n",
    "    null_counts_pd.columns = ['null_percentage']\n",
    "\n",
    "    # Plot the bar chart\n",
    "    # null_counts_pd.plot(kind='bar', legend=False, figsize=(20, 6))\n",
    "    # plt.title('Percentage of Null Values in Each Column')\n",
    "    # plt.ylabel('Percentage of Null Values')\n",
    "    # plt.xlabel('Columns')\n",
    "    # plt.show()\n",
    "    \n",
    "def forward_fill(df):\n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy('date')\n",
    "    feature_cols = df.columns[2:-1]\n",
    "    for c in feature_cols:\n",
    "        df = df.withColumn(\n",
    "            c, F.last(c, ignorenulls=True).over(window_spec)\n",
    "        )\n",
    "    return df.orderBy('fsym_id','date')\n",
    "\n",
    "def median_fill(df):\n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy('date')\n",
    "    feature_cols = df.columns[2:-1]\n",
    "    imputer = Imputer(strategy=\"median\", inputCols=feature_cols, outputCols=feature_cols)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for c in feature_cols:\n",
    "        median_value = df.approxQuantile(c, [0.5], 0.001)[0]\n",
    "        df = df.withColumn(\n",
    "            c, F.when(F.col(c).isNull(), median_value).otherwise(F.col(c))\n",
    "        )\n",
    "    return df.orderBy('fsym_id','date')\n",
    "\n",
    "\n",
    "def get_df(fn, all_feats=False, imploded_only=False, prediction=False):\n",
    "    df = get_tabular_dataset(fn, all_feats=all_feats, imploded_only=imploded_only, prediction=prediction, null_thresh=0.2)\n",
    "    print(\"df retrieved\")\n",
    "    \n",
    "    # null_counts_per_column = df.select([col(c).isNull().cast(\"int\").alias(c) for c in df.columns])\n",
    "    # total_nulls = null_counts_per_column.agg(*[F.sum(col(c)).alias(c) for c in null_counts_per_column.columns]).collect()\n",
    "    # print(total_nulls)\n",
    "    # df = forward_fill(df)\n",
    "    # print(\"done ffill\")\n",
    "    # null_counts_per_column = df.select([col(c).isNull().cast(\"int\").alias(c) for c in df.columns])\n",
    "    # total_nulls = null_counts_per_column.agg(*[F.sum(col(c)).alias(c) for c in null_counts_per_column.columns]).collect()\n",
    "    # print(total_nulls)\n",
    "    # print(\"Number of rows: \", df.count())\n",
    "    # print(\"Number of positives: \", df.filter(F.col('label')==1).count())\n",
    "    # plot_nulls(df)\n",
    "    # df=df.fillna(0.0)\n",
    "    # print(\"Number of rows after dropping nulls: \", df.count())\n",
    "    # print(\"Number of positives after dropping nulls: \", df.filter(F.col('label')==1).count())\n",
    "    # window_spec = Window.partitionBy('fsym_id')\n",
    "    # feats = df.columns[2:-1]\n",
    "    \n",
    "    df =df.toPandas()\n",
    "    # print(df.head(30))\n",
    "#     feats = df.columns[2:-1]\n",
    "#     for fsym_id, group in df.groupby('fsym_id'):\n",
    "#         for col in group.columns[2:-1]:\n",
    "#             group[col] = group[col].fillna(group[col].median())\n",
    "    \n",
    "#     print(df.head(10))\n",
    "    # feats = df.columns[2:-1]\n",
    "    # df[feats] = df.groupby('fsym_id')[feats].transform(lambda x: x.fillna(x.median()))\n",
    "    # print(df.head(30))\n",
    "    return df\n",
    "    \n",
    "def write_features_file(data_list, csv_file_path='features.csv'):\n",
    "    data_list = [data_list]\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row in data_list:\n",
    "            writer.writerow(row)\n",
    "    print(\"Features written: \", data_list[0])\n",
    "\n",
    "# df = get_df('imploded_stocks_price.csv')\n",
    "df = get_df(f'{main_dir}/data/imploded_stocks_price.csv', all_feats =True, prediction=False, imploded_only=False)\n",
    "# plot_nulls(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a718ab0-6e68-4097-afa7-98e05292d766",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df2 = median_fill(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2824646-aa80-496e-8907-91b7a25778ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fsym_id              114188\n",
       "date                 114188\n",
       "ff_assets_com_eq     104196\n",
       "ff_assets_eq         112572\n",
       "ff_assets_gr         105840\n",
       "                      ...  \n",
       "ff_fcf_yld            93814\n",
       "GDP                  114188\n",
       "Unemployment_Rate    114188\n",
       "CPI                  114188\n",
       "label                114188\n",
       "Length: 95, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79a0fd65-c039-4a2c-8719-e2a20b8404b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to Pandas\n",
      "Variable pairs with absolute correlation above 0.7:\n",
      "ff_assets_com_eq - ff_assets_eq: 0.9978250764422232\n",
      "ff_assets_com_eq - ff_debt_com_eq: 0.9987191723613513\n",
      "ff_assets_com_eq - ff_debt_eq: 0.9977479665873509\n",
      "ff_assets_com_eq - ff_fix_assets_com_eq: 0.7122312115262303\n",
      "ff_assets_com_eq - ff_ltd_com_eq: 0.9986613816702117\n",
      "ff_assets_eq - ff_debt_com_eq: 0.9997635827287565\n",
      "ff_assets_eq - ff_debt_eq: 0.9955327930238194\n",
      "ff_assets_eq - ff_fix_assets_com_eq: 0.7122673485504473\n",
      "ff_assets_eq - ff_ltd_com_eq: 0.9997545283319508\n",
      "ff_assets_gr - ff_com_eq_gr: 0.9999999946425615\n",
      "ff_assets_gr - ff_debt_lt_cf: 0.998596478816726\n",
      "ff_assets_gr - ff_debt_st_x_curr_port: 0.9567388654974968\n",
      "ff_assets_gr - ff_inc_tax_curr: 0.9878313980536448\n",
      "ff_assets_gr - ff_inc_tax_dfd: 0.9651312315862938\n",
      "ff_assets_gr - ff_int_exp_oth: 0.9979472092194701\n",
      "ff_assets_gr - ff_invest_cap: 0.9984958977197079\n",
      "ff_assets_gr - ff_net_inc_basic_aft_xord: 0.998166217848079\n",
      "ff_assets_gr - ff_net_inc_basic_beft_xord: 0.9983063733611037\n",
      "ff_assets_gr - ff_net_inc_bef_xord_gr: 0.9999999946600269\n",
      "ff_assets_gr - ff_oper_inc_aft_unusual: 0.9825284986108163\n",
      "ff_assets_gr - ff_oper_inc_gr: 0.9999999999989583\n",
      "ff_assets_gr - ff_sales_gr: 0.9999999999997006\n",
      "ff_assets_gr - ff_sales_ps_gr: 0.9999975988959504\n",
      "ff_assets_gr - ff_ut_gross_inc: 0.992236228831377\n",
      "ff_assets_gr - ff_ut_non_oper_inc_oth: 0.9793186402426861\n",
      "ff_assets_gr - ff_ut_operation_exp: 0.9982798568010404\n",
      "ff_assets_gr - ff_wkcap: 0.9683317806475453\n",
      "ff_assets_gr - ff_ebit_bef_unusual: 0.979274742507503\n",
      "ff_assets_gr - ff_ebitda_bef_unusual: 0.9854579029587607\n",
      "ff_assets_oth_tot - ff_ut_non_oper_inc_oth: 0.7025742654357691\n",
      "ff_assets_per_emp - ff_sales_per_emp: 0.7978037783804918\n",
      "ff_bps_gr - ff_capex_ps_cf: 0.8432126741323365\n",
      "ff_bps_gr - ff_free_ps_cf: 0.8370302427503025\n",
      "ff_capex_ps_cf - ff_free_ps_cf: 0.9930571331187666\n",
      "ff_cash_roce - ff_roce: 0.7021959857433429\n",
      "ff_cf_ps_gr - ff_eps_basic_gr: 0.999999733178853\n",
      "ff_cf_ps_gr - ff_sales_ps_gr: 0.9999997658550582\n",
      "ff_cf_ps_gr - ff_eps_dil_gr: 0.9999997330859859\n",
      "ff_com_eq_gr - ff_debt_lt_cf: 0.998607230178459\n",
      "ff_com_eq_gr - ff_debt_st_x_curr_port: 0.9568096478250078\n",
      "ff_com_eq_gr - ff_inc_tax_curr: 0.987853931465376\n",
      "ff_com_eq_gr - ff_inc_tax_dfd: 0.9657441664349757\n",
      "ff_com_eq_gr - ff_int_exp_oth: 0.9979503580651761\n",
      "ff_com_eq_gr - ff_invest_cap: 0.9984999485566035\n",
      "ff_com_eq_gr - ff_net_inc_basic_aft_xord: 0.9982352150533934\n",
      "ff_com_eq_gr - ff_net_inc_basic_beft_xord: 0.9983751163946707\n",
      "ff_com_eq_gr - ff_net_inc_bef_xord_gr: 0.9999999999955977\n",
      "ff_com_eq_gr - ff_oper_inc_aft_unusual: 0.9829701079012435\n",
      "ff_com_eq_gr - ff_oper_inc_gr: 0.9999999999933609\n",
      "ff_com_eq_gr - ff_sales_gr: 0.9999999999996045\n",
      "ff_com_eq_gr - ff_sales_ps_gr: 0.9999975988961689\n",
      "ff_com_eq_gr - ff_ut_gross_inc: 0.9922890096565856\n",
      "ff_com_eq_gr - ff_ut_non_oper_inc_oth: 0.9794401789278498\n",
      "ff_com_eq_gr - ff_ut_operation_exp: 0.9982947337312524\n",
      "ff_com_eq_gr - ff_wkcap: 0.968341038978859\n",
      "ff_com_eq_gr - ff_ebit_bef_unusual: 0.9793609110587942\n",
      "ff_com_eq_gr - ff_ebitda_bef_unusual: 0.9855200393025181\n",
      "ff_com_eq_tcap - ff_pfd_stk_tcap: 0.7377716620633383\n",
      "ff_com_eq_tcap - ff_tot_debt_tcap_std: 0.7253614865400894\n",
      "ff_debt_com_eq - ff_debt_eq: 0.9998376294265129\n",
      "ff_debt_com_eq - ff_fix_assets_com_eq: 0.713101278140345\n",
      "ff_debt_com_eq - ff_ltd_com_eq: 0.9999795416268792\n",
      "ff_debt_eq - ff_fix_assets_com_eq: 0.712977471181954\n",
      "ff_debt_eq - ff_ltd_com_eq: 0.9998327282179491\n",
      "ff_debt_lt_cf - ff_debt_st_x_curr_port: 0.9527266853720879\n",
      "ff_debt_lt_cf - ff_inc_tax_curr: 0.9863940014258664\n",
      "ff_debt_lt_cf - ff_inc_tax_dfd: 0.9641008302758485\n",
      "ff_debt_lt_cf - ff_int_exp_oth: 0.996181272096403\n",
      "ff_debt_lt_cf - ff_invest_cap: 0.9967273089256887\n",
      "ff_debt_lt_cf - ff_net_inc_basic_aft_xord: 0.9971804210520332\n",
      "ff_debt_lt_cf - ff_net_inc_basic_beft_xord: 0.9973228760367551\n",
      "ff_debt_lt_cf - ff_net_inc_bef_xord_gr: 0.9986675942858406\n",
      "ff_debt_lt_cf - ff_oper_inc_aft_unusual: 0.9829991848961007\n",
      "ff_debt_lt_cf - ff_oper_inc_gr: 0.998682267331993\n",
      "ff_debt_lt_cf - ff_sales_gr: 0.9985975291023149\n",
      "ff_debt_lt_cf - ff_sales_ps_gr: 0.9986595184518555\n",
      "ff_debt_lt_cf - ff_ut_gross_inc: 0.9919205024429271\n",
      "ff_debt_lt_cf - ff_ut_non_oper_inc_oth: 0.9787375608637943\n",
      "ff_debt_lt_cf - ff_ut_operation_exp: 0.996643916858028\n",
      "ff_debt_lt_cf - ff_wkcap: 0.9683792932336278\n",
      "ff_debt_lt_cf - ff_ebit_bef_unusual: 0.9799953192525535\n",
      "ff_debt_lt_cf - ff_ebitda_bef_unusual: 0.9830838052657128\n",
      "ff_debt_st_x_curr_port - ff_inc_tax_curr: 0.9542081193553009\n",
      "ff_debt_st_x_curr_port - ff_inc_tax_dfd: 0.9263760548605796\n",
      "ff_debt_st_x_curr_port - ff_int_exp_oth: 0.9664886010368652\n",
      "ff_debt_st_x_curr_port - ff_invest_cap: 0.965430184350539\n",
      "ff_debt_st_x_curr_port - ff_invest_lt: 0.7365188821767011\n",
      "ff_debt_st_x_curr_port - ff_net_inc_basic_aft_xord: 0.9513324694437476\n",
      "ff_debt_st_x_curr_port - ff_net_inc_basic_beft_xord: 0.9514053109277523\n",
      "ff_debt_st_x_curr_port - ff_net_inc_bef_xord_gr: 0.9586086276858503\n",
      "ff_debt_st_x_curr_port - ff_oper_inc_aft_unusual: 0.926742595671505\n",
      "ff_debt_st_x_curr_port - ff_oper_inc_gr: 0.9590068900319817\n",
      "ff_debt_st_x_curr_port - ff_sales_gr: 0.956761812676619\n",
      "ff_debt_st_x_curr_port - ff_sales_ps_gr: 0.9579168855129984\n",
      "ff_debt_st_x_curr_port - ff_ut_gross_inc: 0.9375190562939958\n",
      "ff_debt_st_x_curr_port - ff_ut_non_oper_inc_oth: 0.9774473334821917\n",
      "ff_debt_st_x_curr_port - ff_ut_operation_exp: 0.9567749144610729\n",
      "ff_debt_st_x_curr_port - ff_wkcap: 0.9007548675719652\n",
      "ff_debt_st_x_curr_port - ff_invest_receiv_lt: 0.7365188821767011\n",
      "ff_debt_st_x_curr_port - ff_ebit_bef_unusual: 0.9140801613997208\n",
      "ff_debt_st_x_curr_port - ff_ebitda_bef_unusual: 0.9595318590573741\n",
      "ff_div_yld - ff_div_yld_secs: 0.9996300207360205\n",
      "ff_earn_yld - ff_eps_dil_aft_xord: 0.8265939949705117\n",
      "ff_earn_yld - ff_fcf_yld: 0.9999999999999772\n",
      "ff_ebit_oper_roa - ff_roa_ptx: 0.9974698472686245\n",
      "ff_entrpr_val_sales - ff_psales_dil: 0.9999999994222479\n",
      "ff_eps_basic_gr - ff_sales_ps_gr: 0.9999999986497526\n",
      "ff_eps_basic_gr - ff_eps_dil_gr: 0.9999999999943289\n",
      "ff_fix_assets_com_eq - ff_ltd_com_eq: 0.7132712588391372\n",
      "ff_gross_cf_debt - ff_net_cf_debt: 0.9041463165876684\n",
      "ff_inc_tax_curr - ff_inc_tax_dfd: 0.958757286364105\n",
      "ff_inc_tax_curr - ff_int_exp_oth: 0.9878644643904323\n",
      "ff_inc_tax_curr - ff_invest_cap: 0.9903509842822421\n",
      "ff_inc_tax_curr - ff_net_inc_basic_aft_xord: 0.9798658988522642\n",
      "ff_inc_tax_curr - ff_net_inc_basic_beft_xord: 0.980010167888745\n",
      "ff_inc_tax_curr - ff_net_inc_bef_xord_gr: 0.9880960547307183\n",
      "ff_inc_tax_curr - ff_oper_inc_aft_unusual: 0.949225348731626\n",
      "ff_inc_tax_curr - ff_oper_inc_gr: 0.9880048963308867\n",
      "ff_inc_tax_curr - ff_sales_gr: 0.9878454431258998\n",
      "ff_inc_tax_curr - ff_sales_ps_gr: 0.987922137222321\n",
      "ff_inc_tax_curr - ff_ut_gross_inc: 0.9663746674063549\n",
      "ff_inc_tax_curr - ff_ut_non_oper_inc_oth: 0.9786429163790041\n",
      "ff_inc_tax_curr - ff_ut_operation_exp: 0.9916585524172411\n",
      "ff_inc_tax_curr - ff_wkcap: 0.9548999434069648\n",
      "ff_inc_tax_curr - ff_ebit_bef_unusual: 0.9422680951012912\n",
      "ff_inc_tax_curr - ff_ebitda_bef_unusual: 0.9946494504777564\n",
      "ff_inc_tax_curr - ff_net_inc_dil_bef_unusual: 0.7347944426573996\n",
      "ff_inc_tax_dfd - ff_int_exp_oth: 0.9646567370719723\n",
      "ff_inc_tax_dfd - ff_invest_cap: 0.9641905339311989\n",
      "ff_inc_tax_dfd - ff_net_inc_basic_aft_xord: 0.9624763318270793\n",
      "ff_inc_tax_dfd - ff_net_inc_basic_beft_xord: 0.9630879840175735\n",
      "ff_inc_tax_dfd - ff_net_inc_bef_xord_gr: 0.9659694000834634\n",
      "ff_inc_tax_dfd - ff_oper_inc_aft_unusual: 0.9531829079677403\n",
      "ff_inc_tax_dfd - ff_oper_inc_gr: 0.965596468193575\n",
      "ff_inc_tax_dfd - ff_sales_gr: 0.9651001713125549\n",
      "ff_inc_tax_dfd - ff_sales_ps_gr: 0.9658120820646711\n",
      "ff_inc_tax_dfd - ff_ut_gross_inc: 0.9545452186004078\n",
      "ff_inc_tax_dfd - ff_ut_non_oper_inc_oth: 0.951105984157626\n",
      "ff_inc_tax_dfd - ff_ut_operation_exp: 0.9638365915012347\n",
      "ff_inc_tax_dfd - ff_wkcap: 0.9375320335379075\n",
      "ff_inc_tax_dfd - ff_ebit_bef_unusual: 0.9467080520487877\n",
      "ff_inc_tax_dfd - ff_ebitda_bef_unusual: 0.9511014583104127\n",
      "ff_int_exp_oth - ff_invest_cap: 0.9985623769891562\n",
      "ff_int_exp_oth - ff_net_inc_basic_aft_xord: 0.9954538887683894\n",
      "ff_int_exp_oth - ff_net_inc_basic_beft_xord: 0.9954539061024144\n",
      "ff_int_exp_oth - ff_net_inc_bef_xord_gr: 0.9980684201872637\n",
      "ff_int_exp_oth - ff_oper_inc_aft_unusual: 0.9773601562683871\n",
      "ff_int_exp_oth - ff_oper_inc_gr: 0.9980002481482838\n",
      "ff_int_exp_oth - ff_sales_gr: 0.9979506563334894\n",
      "ff_int_exp_oth - ff_sales_ps_gr: 0.9979945878748657\n",
      "ff_int_exp_oth - ff_ut_gross_inc: 0.9871385255913788\n",
      "ff_int_exp_oth - ff_ut_non_oper_inc_oth: 0.9792929111061326\n",
      "ff_int_exp_oth - ff_ut_operation_exp: 0.996899705574836\n",
      "ff_int_exp_oth - ff_wkcap: 0.9606703756461838\n",
      "ff_int_exp_oth - ff_ebit_bef_unusual: 0.9716726933465913\n",
      "ff_int_exp_oth - ff_ebitda_bef_unusual: 0.9878175589134494\n",
      "ff_invest_cap - ff_net_inc_basic_aft_xord: 0.9949437961799954\n",
      "ff_invest_cap - ff_net_inc_basic_beft_xord: 0.9950758472641344\n",
      "ff_invest_cap - ff_net_inc_bef_xord_gr: 0.9985763364225965\n",
      "ff_invest_cap - ff_oper_inc_aft_unusual: 0.9749426759997841\n",
      "ff_invest_cap - ff_oper_inc_gr: 0.9985591192736897\n",
      "ff_invest_cap - ff_sales_gr: 0.9985011616790652\n",
      "ff_invest_cap - ff_sales_ps_gr: 0.9985203371671587\n",
      "ff_invest_cap - ff_ut_gross_inc: 0.9859276324976871\n",
      "ff_invest_cap - ff_ut_non_oper_inc_oth: 0.980467188781829\n",
      "ff_invest_cap - ff_ut_operation_exp: 0.998194706543007\n",
      "ff_invest_cap - ff_wkcap: 0.9634111021442774\n",
      "ff_invest_cap - ff_ebit_bef_unusual: 0.969494379534718\n",
      "ff_invest_cap - ff_ebitda_bef_unusual: 0.9910141493526714\n",
      "ff_invest_lt - ff_ut_non_oper_inc_oth: 0.9047298720058948\n",
      "ff_invest_lt - ff_invest_receiv_lt: 1.0\n",
      "ff_ltd_tcap - ff_tot_debt_tcap_std: 0.8355425685424382\n",
      "ff_min_int_tcap - ff_wkcap_pct: 0.895426439607433\n",
      "ff_net_inc_basic_aft_xord - ff_net_inc_basic_beft_xord: 0.9998925783230418\n",
      "ff_net_inc_basic_aft_xord - ff_net_inc_bef_xord_gr: 0.9982563056613942\n",
      "ff_net_inc_basic_aft_xord - ff_net_inc_dil: 0.9725444960433146\n",
      "ff_net_inc_basic_aft_xord - ff_net_inc_dil_aft_xord: 0.9993815299431917\n",
      "ff_net_inc_basic_aft_xord - ff_oper_inc_aft_unusual: 0.9909891372766136\n",
      "ff_net_inc_basic_aft_xord - ff_oper_inc_gr: 0.9981977260580955\n",
      "ff_net_inc_basic_aft_xord - ff_sales_gr: 0.9981670774316562\n",
      "ff_net_inc_basic_aft_xord - ff_sales_ps_gr: 0.9982306236130636\n",
      "ff_net_inc_basic_aft_xord - ff_ut_gross_inc: 0.996752506274425\n",
      "ff_net_inc_basic_aft_xord - ff_ut_non_oper_inc_oth: 0.9733887752164511\n",
      "ff_net_inc_basic_aft_xord - ff_ut_operation_exp: 0.9948282334573422\n",
      "ff_net_inc_basic_aft_xord - ff_wkcap: 0.96760445837179\n",
      "ff_net_inc_basic_aft_xord - ff_ebit_bef_unusual: 0.987711515504632\n",
      "ff_net_inc_basic_aft_xord - ff_ebitda_bef_unusual: 0.975393401860335\n",
      "ff_net_inc_basic_aft_xord - ff_net_inc_dil_bef_unusual: 0.926579877275503\n",
      "ff_net_inc_basic_beft_xord - ff_net_inc_bef_xord_gr: 0.9983951663852767\n",
      "ff_net_inc_basic_beft_xord - ff_net_inc_dil: 0.997071943033356\n",
      "ff_net_inc_basic_beft_xord - ff_net_inc_dil_aft_xord: 0.9695079885483134\n",
      "ff_net_inc_basic_beft_xord - ff_oper_inc_aft_unusual: 0.9910235639795718\n",
      "ff_net_inc_basic_beft_xord - ff_oper_inc_gr: 0.9983332271261117\n",
      "ff_net_inc_basic_beft_xord - ff_sales_gr: 0.9983083549429457\n",
      "ff_net_inc_basic_beft_xord - ff_sales_ps_gr: 0.9983261167761156\n",
      "ff_net_inc_basic_beft_xord - ff_ut_gross_inc: 0.9968239337082837\n",
      "ff_net_inc_basic_beft_xord - ff_ut_non_oper_inc_oth: 0.9734687354524723\n",
      "ff_net_inc_basic_beft_xord - ff_ut_operation_exp: 0.9949638926099867\n",
      "ff_net_inc_basic_beft_xord - ff_wkcap: 0.9672642087726553\n",
      "ff_net_inc_basic_beft_xord - ff_ebit_bef_unusual: 0.9877594562431371\n",
      "ff_net_inc_basic_beft_xord - ff_ebitda_bef_unusual: 0.9754914135177616\n",
      "ff_net_inc_basic_beft_xord - ff_net_inc_dil_bef_unusual: 0.9518738436782097\n",
      "ff_net_inc_bef_xord_gr - ff_oper_inc_aft_unusual: 0.9832243258346858\n",
      "ff_net_inc_bef_xord_gr - ff_oper_inc_gr: 0.9999999999997824\n",
      "ff_net_inc_bef_xord_gr - ff_sales_gr: 0.9999999999994417\n",
      "ff_net_inc_bef_xord_gr - ff_sales_ps_gr: 0.9999977221780612\n",
      "ff_net_inc_bef_xord_gr - ff_ut_gross_inc: 0.9924293170434579\n",
      "ff_net_inc_bef_xord_gr - ff_ut_non_oper_inc_oth: 0.9810230765756607\n",
      "ff_net_inc_bef_xord_gr - ff_ut_operation_exp: 0.9983708378583065\n",
      "ff_net_inc_bef_xord_gr - ff_wkcap: 0.9695150839485323\n",
      "ff_net_inc_bef_xord_gr - ff_ebit_bef_unusual: 0.9797894661177983\n",
      "ff_net_inc_bef_xord_gr - ff_ebitda_bef_unusual: 0.9859384843600933\n",
      "ff_net_inc_dil - ff_net_inc_dil_aft_xord: 0.9729873173808263\n",
      "ff_net_inc_dil - ff_oper_inc_aft_unusual: 0.9397286436415643\n",
      "ff_net_inc_dil - ff_ut_gross_inc: 0.868604508312633\n",
      "ff_net_inc_dil - ff_ebit_bef_unusual: 0.8651613588200568\n",
      "ff_net_inc_dil - ff_ebitda_bef_unusual: 0.8238842261218692\n",
      "ff_net_inc_dil - ff_net_inc_dil_bef_unusual: 0.955268070636178\n",
      "ff_net_inc_dil_aft_xord - ff_oper_inc_aft_unusual: 0.9090755261471941\n",
      "ff_net_inc_dil_aft_xord - ff_ut_gross_inc: 0.8413473263185758\n",
      "ff_net_inc_dil_aft_xord - ff_ebit_bef_unusual: 0.8341302069693213\n",
      "ff_net_inc_dil_aft_xord - ff_ebitda_bef_unusual: 0.7956578401569034\n",
      "ff_net_inc_dil_aft_xord - ff_net_inc_dil_bef_unusual: 0.9270523558395499\n",
      "ff_oper_inc_aft_unusual - ff_oper_inc_gr: 0.9827286779325761\n",
      "ff_oper_inc_aft_unusual - ff_sales_gr: 0.9825517601900373\n",
      "ff_oper_inc_aft_unusual - ff_sales_ps_gr: 0.9827914225456872\n",
      "ff_oper_inc_aft_unusual - ff_ut_gross_inc: 0.9954659143276242\n",
      "ff_oper_inc_aft_unusual - ff_ut_non_oper_inc_oth: 0.9569578568541446\n",
      "ff_oper_inc_aft_unusual - ff_ut_operation_exp: 0.9752194361943102\n",
      "ff_oper_inc_aft_unusual - ff_wkcap: 0.9539441984376477\n",
      "ff_oper_inc_aft_unusual - ff_ebit_bef_unusual: 0.9962965835739251\n",
      "ff_oper_inc_aft_unusual - ff_ebitda_bef_unusual: 0.9405133810866609\n",
      "ff_oper_inc_aft_unusual - ff_net_inc_dil_bef_unusual: 0.9124332222062411\n",
      "ff_oper_inc_gr - ff_sales_gr: 0.999999999999547\n",
      "ff_oper_inc_gr - ff_sales_ps_gr: 0.9999977221779962\n",
      "ff_oper_inc_gr - ff_ut_gross_inc: 0.9923427719636849\n",
      "ff_oper_inc_gr - ff_ut_non_oper_inc_oth: 0.980544967093937\n",
      "ff_oper_inc_gr - ff_ut_operation_exp: 0.9983305146329715\n",
      "ff_oper_inc_gr - ff_wkcap: 0.9691692513058974\n",
      "ff_oper_inc_gr - ff_ebit_bef_unusual: 0.9796075123227826\n",
      "ff_oper_inc_gr - ff_ebitda_bef_unusual: 0.9857526110496446\n",
      "ff_reinvest_rate - ff_roce: 0.8279468709087424\n",
      "ff_sales_gr - ff_sales_ps_gr: 0.9999975988955603\n",
      "ff_sales_gr - ff_ut_gross_inc: 0.9922505829838044\n",
      "ff_sales_gr - ff_ut_non_oper_inc_oth: 0.9792669036074497\n",
      "ff_sales_gr - ff_ut_operation_exp: 0.9982796220737378\n",
      "ff_sales_gr - ff_wkcap: 0.9684090948078963\n",
      "ff_sales_gr - ff_ebit_bef_unusual: 0.979317018343708\n",
      "ff_sales_gr - ff_ebitda_bef_unusual: 0.985494998294916\n",
      "ff_sales_ps_gr - ff_ut_gross_inc: 0.9923697302778476\n",
      "ff_sales_ps_gr - ff_ut_non_oper_inc_oth: 0.9794545111627074\n",
      "ff_sales_ps_gr - ff_ut_operation_exp: 0.9982868151888667\n",
      "ff_sales_ps_gr - ff_wkcap: 0.9687654597390939\n",
      "ff_sales_ps_gr - ff_ebit_bef_unusual: 0.9795667713522366\n",
      "ff_sales_ps_gr - ff_ebitda_bef_unusual: 0.9856985288028021\n",
      "ff_sales_ps_gr - ff_eps_dil_gr: 0.9999999986494716\n",
      "ff_ut_gross_inc - ff_ut_non_oper_inc_oth: 0.9635245334332414\n",
      "ff_ut_gross_inc - ff_ut_operation_exp: 0.9865404093380655\n",
      "ff_ut_gross_inc - ff_wkcap: 0.9638131077202757\n",
      "ff_ut_gross_inc - ff_ebit_bef_unusual: 0.9957329674739743\n",
      "ff_ut_gross_inc - ff_ebitda_bef_unusual: 0.9578560641027658\n",
      "ff_ut_gross_inc - ff_net_inc_dil_bef_unusual: 0.9511421895402155\n",
      "ff_ut_non_oper_inc_oth - ff_ut_operation_exp: 0.9801785953856565\n",
      "ff_ut_non_oper_inc_oth - ff_wkcap: 0.9785831072252282\n",
      "ff_ut_non_oper_inc_oth - ff_invest_receiv_lt: 0.9047298720058949\n",
      "ff_ut_non_oper_inc_oth - ff_ebit_bef_unusual: 0.9485286304835776\n",
      "ff_ut_non_oper_inc_oth - ff_ebitda_bef_unusual: 0.9798064801429557\n",
      "ff_ut_operation_exp - ff_wkcap: 0.966566305383845\n",
      "ff_ut_operation_exp - ff_ebit_bef_unusual: 0.9707211823209466\n",
      "ff_ut_operation_exp - ff_ebitda_bef_unusual: 0.990001507760822\n",
      "ff_wkcap - ff_ebit_bef_unusual: 0.9548474714874454\n",
      "ff_wkcap - ff_ebitda_bef_unusual: 0.9503843542376169\n",
      "ff_xord - ff_xord_disc: 0.760691956593312\n",
      "ff_ebit_bef_unusual - ff_ebitda_bef_unusual: 0.9312315387572596\n",
      "ff_ebit_bef_unusual - ff_net_inc_dil_bef_unusual: 0.9388642090196313\n",
      "ff_ebitda_bef_unusual - ff_net_inc_dil_bef_unusual: 0.9067655631270247\n",
      "ff_eps_dil_aft_xord - ff_fcf_yld: 0.8265939667370844\n",
      "ff_bk_oper_inc_oth - ff_bk_oper_inc_tot: 0.956584060192204\n",
      "ff_bk_oper_inc_oth - ff_bk_non_oper_inc: 0.7728570836932788\n",
      "ff_bk_oper_inc_tot - ff_bk_non_oper_inc: 0.7803768156921134\n",
      "GDP - CPI: 0.827281402926256\n",
      "Index(['fsym_id', 'date', 'ff_assets_com_eq', 'ff_assets_gr',\n",
      "       'ff_assets_oth_tot', 'ff_assets_per_emp', 'ff_bps_gr',\n",
      "       'ff_capex_assets', 'ff_cash_div_cf', 'ff_cash_roce', 'ff_cf_ps_gr',\n",
      "       'ff_cf_sales', 'ff_com_eq_tcap', 'ff_debt_entrpr_val',\n",
      "       'ff_dfd_tax_assets_lt', 'ff_dil_adj', 'ff_div_yld', 'ff_earn_yld',\n",
      "       'ff_ebit_oper_roa', 'ff_entrpr_val_sales', 'ff_for_assets_pct',\n",
      "       'ff_for_sales_pct', 'ff_gross_cf_debt', 'ff_inc_adj', 'ff_inc_sund',\n",
      "       'ff_invest_st_tot', 'ff_ltd_tcap', 'ff_min_int_tcap',\n",
      "       'ff_net_inc_per_emp', 'ff_non_oper_exp', 'ff_oper_cf_fix_chrg',\n",
      "       'ff_oper_inc_tcap', 'ff_oper_ps_net_cf', 'ff_reinvest_rate', 'ff_roic',\n",
      "       'ff_tcap_assets', 'ff_xord', 'ff_std_debt', 'ff_tang_assets_debt',\n",
      "       'ff_bk_oper_inc_oth', 'ff_commiss_inc_net', 'ff_cf_roic',\n",
      "       'ff_liabs_lease', 'GDP', 'Unemployment_Rate', 'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def correlation_matrix(df):\n",
    "    # df =df.toPandas()\n",
    "    print(\"Converted to Pandas\")\n",
    "    corr_df = df.drop(['date','fsym_id'], axis=1)\n",
    "    corr_mat = corr_df.corr().abs()\n",
    "    mask = np.triu(np.ones_like(corr_mat))\n",
    "    plt.figure(figsize=(50, 40))\n",
    "    sns.heatmap(corr_mat, annot=True, cmap='coolwarm', fmt=\".2f\", mask=mask)\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('corr_matrix_tab.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Variable pairs with absolute correlation above 0.7:\")\n",
    "    corr_dict = {}\n",
    "    for i in range(len(corr_mat.columns)):\n",
    "        for j in range(i+1, len(corr_mat.columns)):\n",
    "            if abs(corr_mat.iloc[i, j]) >= 0.7:\n",
    "                print(f\"{corr_mat.columns[i]} - {corr_mat.columns[j]}: {corr_mat.iloc[i, j]}\")\n",
    "                if corr_mat.columns[i] not in corr_dict.keys():\n",
    "                    corr_dict[corr_mat.columns[i]] = [corr_mat.columns[j]]\n",
    "                else:\n",
    "                    corr_dict[corr_mat.columns[i]].append(corr_mat.columns[j])\n",
    "                    \n",
    "    for k,v in corr_dict.items():\n",
    "        if len(corr_dict[k]) >= 1:\n",
    "            for col in corr_dict[k]:\n",
    "                if col in df.columns:\n",
    "                    df=df.drop(col,axis=1)\n",
    "    \n",
    "                \n",
    "\n",
    "    print(df.columns)\n",
    "    return df\n",
    "\n",
    "    \n",
    "                \n",
    "df=correlation_matrix(df) #pandas now\n",
    "# df=correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f39b54dc-ccbc-43c6-bd3b-2d1d97595ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/jupyterhub/lib/python3.10/site-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n",
      "/opt/jupyterhub/lib/python3.10/site-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feats = df.columns[2:-1]\n",
    "df[feats] = df.groupby('fsym_id')[feats].transform(lambda x : x.fillna(method='ffill'))\n",
    "df[feats] = df.groupby('fsym_id')[feats].transform(lambda x: x.fillna(x.median()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da310543-4a40-4e70-8089-f4841414e9b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "# df = df.drop('ff_debt_com_eq', 'ff_debt_eq', '' 'ff_div_yld_secs', 'ff_earn_yld', 'ff_roa_ptx', 'ff_net_inc_basic_aft_xord', 'ff_net_inc_dil', 'ff_oper_inc_aft_unusual', \n",
    "#                         'ff_net_inc_dil_aft_xord', 'ff_net_inc_dil_bef_unusual', 'ff_ebit_bef_unusual', 'ff_eps_dil_gr', 'GDP', 'ff_bk_oper_inc_tot')\n",
    "feats = df.columns[2:-1]\n",
    "# write_features_file(feats)\n",
    "print(len(feats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66977c59-6064-49a5-bedc-4dd7de5013f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fsym_id</th>\n",
       "      <th>date</th>\n",
       "      <th>ff_assets_com_eq</th>\n",
       "      <th>ff_assets_gr</th>\n",
       "      <th>ff_assets_oth_tot</th>\n",
       "      <th>ff_assets_per_emp</th>\n",
       "      <th>ff_bps_gr</th>\n",
       "      <th>ff_capex_assets</th>\n",
       "      <th>ff_cash_div_cf</th>\n",
       "      <th>ff_cash_roce</th>\n",
       "      <th>...</th>\n",
       "      <th>ff_xord</th>\n",
       "      <th>ff_std_debt</th>\n",
       "      <th>ff_tang_assets_debt</th>\n",
       "      <th>ff_bk_oper_inc_oth</th>\n",
       "      <th>ff_commiss_inc_net</th>\n",
       "      <th>ff_cf_roic</th>\n",
       "      <th>ff_liabs_lease</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Unemployment_Rate</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1.248053</td>\n",
       "      <td>7.348883</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>56.030954</td>\n",
       "      <td>78.273974</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-32.48987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>129.518779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.792882</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>1.016917</td>\n",
       "      <td>27.333701</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>56.273777</td>\n",
       "      <td>38.851847</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-32.48987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>129.518779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-32.489870</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014049</td>\n",
       "      <td>6.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>1.016917</td>\n",
       "      <td>1213.345113</td>\n",
       "      <td>2.152</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>-492.849757</td>\n",
       "      <td>11.984993</td>\n",
       "      <td>3321.316445</td>\n",
       "      <td>-32.48987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>77.245294</td>\n",
       "      <td>-0.294125</td>\n",
       "      <td>0.294125</td>\n",
       "      <td>3.610653</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006058</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>1.016917</td>\n",
       "      <td>7.348883</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>-48.051855</td>\n",
       "      <td>0.483800</td>\n",
       "      <td>186.785493</td>\n",
       "      <td>-32.48987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.526570</td>\n",
       "      <td>-1.071870</td>\n",
       "      <td>1.071870</td>\n",
       "      <td>19.546902</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>1.016917</td>\n",
       "      <td>76.903869</td>\n",
       "      <td>0.000</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>85.114102</td>\n",
       "      <td>2.179701</td>\n",
       "      <td>238.208140</td>\n",
       "      <td>-32.48987</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.938208</td>\n",
       "      <td>130.866314</td>\n",
       "      <td>-0.710952</td>\n",
       "      <td>0.710952</td>\n",
       "      <td>21.835781</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010414</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    fsym_id        date  ff_assets_com_eq  ff_assets_gr  ff_assets_oth_tot  \\\n",
       "0  B00FG1-R  2012-12-31          1.248053      7.348883              0.000   \n",
       "1  B00FG1-R  2013-12-31          1.016917     27.333701              0.000   \n",
       "2  B00FG1-R  2014-12-31          1.016917   1213.345113              2.152   \n",
       "3  B00FG1-R  2015-12-31          1.016917      7.348883              0.000   \n",
       "4  B00FG1-R  2016-12-31          1.016917     76.903869              0.000   \n",
       "\n",
       "   ff_assets_per_emp   ff_bps_gr  ff_capex_assets  ff_cash_div_cf  \\\n",
       "0          10.576109   56.030954        78.273974        0.000000   \n",
       "1          10.576109   56.273777        38.851847        0.000000   \n",
       "2          10.576109 -492.849757        11.984993     3321.316445   \n",
       "3          10.576109  -48.051855         0.483800      186.785493   \n",
       "4          10.576109   85.114102         2.179701      238.208140   \n",
       "\n",
       "   ff_cash_roce  ...  ff_xord  ff_std_debt  ff_tang_assets_debt  \\\n",
       "0     -32.48987  ...      0.0     0.000000           129.518779   \n",
       "1     -32.48987  ...      0.0     0.000000           129.518779   \n",
       "2     -32.48987  ...      0.0     0.000000            77.245294   \n",
       "3     -32.48987  ...      0.0     0.000000            70.526570   \n",
       "4     -32.48987  ...      0.0     6.938208           130.866314   \n",
       "\n",
       "   ff_bk_oper_inc_oth  ff_commiss_inc_net  ff_cf_roic  ff_liabs_lease  \\\n",
       "0            0.000000            0.000000   20.792882             0.0   \n",
       "1            0.000000            0.000000  -32.489870             0.0   \n",
       "2           -0.294125            0.294125    3.610653             0.0   \n",
       "3           -1.071870            1.071870   19.546902             0.0   \n",
       "4           -0.710952            0.710952   21.835781             0.0   \n",
       "\n",
       "        GDP  Unemployment_Rate  label  \n",
       "0  0.006181                7.9      0  \n",
       "1  0.014049                6.7      0  \n",
       "2  0.006058                5.6      0  \n",
       "3  0.001821                5.0      0  \n",
       "4  0.010414                4.7      0  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6db4abf6-85c4-4a20-b6e1-ee45082bc5f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fsym_id</th>\n",
       "      <th>date</th>\n",
       "      <th>ff_assets_com_eq</th>\n",
       "      <th>ff_assets_gr</th>\n",
       "      <th>ff_assets_oth_tot</th>\n",
       "      <th>ff_assets_per_emp</th>\n",
       "      <th>ff_bps_gr</th>\n",
       "      <th>ff_capex_assets</th>\n",
       "      <th>ff_cash_div_cf</th>\n",
       "      <th>ff_cash_roce</th>\n",
       "      <th>...</th>\n",
       "      <th>ff_xord</th>\n",
       "      <th>ff_std_debt</th>\n",
       "      <th>ff_tang_assets_debt</th>\n",
       "      <th>ff_bk_oper_inc_oth</th>\n",
       "      <th>ff_commiss_inc_net</th>\n",
       "      <th>ff_cf_roic</th>\n",
       "      <th>ff_liabs_lease</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Unemployment_Rate</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1.248053</td>\n",
       "      <td>7.348883</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>56.030954</td>\n",
       "      <td>78.273974</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>-32.48987</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>129.518779</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>20.792882</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>7.9</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2013-12-31</td>\n",
       "      <td>1.016917</td>\n",
       "      <td>27.333701</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>56.273777</td>\n",
       "      <td>38.851847</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>-32.48987</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>129.518779</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>-32.489870</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.014049</td>\n",
       "      <td>6.7</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>1.016917</td>\n",
       "      <td>1213.345113</td>\n",
       "      <td>2.152000e+00</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>-492.849757</td>\n",
       "      <td>11.984993</td>\n",
       "      <td>3.321316e+03</td>\n",
       "      <td>-32.48987</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>77.245294</td>\n",
       "      <td>-2.941250e-01</td>\n",
       "      <td>2.941250e-01</td>\n",
       "      <td>3.610653</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.006058</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>1.016917</td>\n",
       "      <td>7.348883</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>-48.051855</td>\n",
       "      <td>0.483800</td>\n",
       "      <td>1.867855e+02</td>\n",
       "      <td>-32.48987</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>70.526570</td>\n",
       "      <td>-1.071870e+00</td>\n",
       "      <td>1.071870e+00</td>\n",
       "      <td>19.546902</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2016-12-31</td>\n",
       "      <td>1.016917</td>\n",
       "      <td>76.903869</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>10.576109</td>\n",
       "      <td>85.114102</td>\n",
       "      <td>2.179701</td>\n",
       "      <td>2.382081e+02</td>\n",
       "      <td>-32.48987</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>6.938208e+00</td>\n",
       "      <td>130.866314</td>\n",
       "      <td>-7.109520e-01</td>\n",
       "      <td>7.109520e-01</td>\n",
       "      <td>21.835781</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>0.010414</td>\n",
       "      <td>4.7</td>\n",
       "      <td>1.000000e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 46 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    fsym_id        date  ff_assets_com_eq  ff_assets_gr  ff_assets_oth_tot  \\\n",
       "0  B00FG1-R  2012-12-31          1.248053      7.348883       1.000000e-10   \n",
       "1  B00FG1-R  2013-12-31          1.016917     27.333701       1.000000e-10   \n",
       "2  B00FG1-R  2014-12-31          1.016917   1213.345113       2.152000e+00   \n",
       "3  B00FG1-R  2015-12-31          1.016917      7.348883       1.000000e-10   \n",
       "4  B00FG1-R  2016-12-31          1.016917     76.903869       1.000000e-10   \n",
       "\n",
       "   ff_assets_per_emp   ff_bps_gr  ff_capex_assets  ff_cash_div_cf  \\\n",
       "0          10.576109   56.030954        78.273974    1.000000e-10   \n",
       "1          10.576109   56.273777        38.851847    1.000000e-10   \n",
       "2          10.576109 -492.849757        11.984993    3.321316e+03   \n",
       "3          10.576109  -48.051855         0.483800    1.867855e+02   \n",
       "4          10.576109   85.114102         2.179701    2.382081e+02   \n",
       "\n",
       "   ff_cash_roce  ...       ff_xord   ff_std_debt  ff_tang_assets_debt  \\\n",
       "0     -32.48987  ...  1.000000e-10  1.000000e-10           129.518779   \n",
       "1     -32.48987  ...  1.000000e-10  1.000000e-10           129.518779   \n",
       "2     -32.48987  ...  1.000000e-10  1.000000e-10            77.245294   \n",
       "3     -32.48987  ...  1.000000e-10  1.000000e-10            70.526570   \n",
       "4     -32.48987  ...  1.000000e-10  6.938208e+00           130.866314   \n",
       "\n",
       "   ff_bk_oper_inc_oth  ff_commiss_inc_net  ff_cf_roic  ff_liabs_lease  \\\n",
       "0        1.000000e-10        1.000000e-10   20.792882    1.000000e-10   \n",
       "1        1.000000e-10        1.000000e-10  -32.489870    1.000000e-10   \n",
       "2       -2.941250e-01        2.941250e-01    3.610653    1.000000e-10   \n",
       "3       -1.071870e+00        1.071870e+00   19.546902    1.000000e-10   \n",
       "4       -7.109520e-01        7.109520e-01   21.835781    1.000000e-10   \n",
       "\n",
       "        GDP  Unemployment_Rate         label  \n",
       "0  0.006181                7.9  1.000000e-10  \n",
       "1  0.014049                6.7  1.000000e-10  \n",
       "2  0.006058                5.6  1.000000e-10  \n",
       "3  0.001821                5.0  1.000000e-10  \n",
       "4  0.010414                4.7  1.000000e-10  \n",
       "\n",
       "[5 rows x 46 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3da6ede0-14c1-4220-8292-b7c5b9ef6983",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "667\n"
     ]
    }
   ],
   "source": [
    "print(len(df[df['label']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7a3f8175-b763-4444-be76-6d1d999013c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_date = '2020-01-01'\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "train_df = df[df['date'] < split_date]\n",
    "test_df = df[df['date'] >= split_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2a9a3e51-894e-4e9a-8836-aa837749fcdb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "8990\n",
      "7049\n",
      "147\n",
      "520\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df.columns))\n",
    "print(len(train_df['fsym_id'].unique()))\n",
    "print(len(test_df['fsym_id'].unique()))\n",
    "print(len(test_df[test_df['label']==1]))\n",
    "print(len(train_df[train_df['label']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7ccbe6b5-537a-4d03-bfe0-dea35492f651",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%capture\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "import tsfel\n",
    "\n",
    "def feature_extraction(df):\n",
    "    df = df.set_index('date')\n",
    "     \n",
    "    cfg = tsfel.get_features_by_domain(domain='statistical', json_path='features.json')\n",
    "    \n",
    "    result_dfs = []\n",
    "    for fsym_id, group_df in df.groupby('fsym_id'):\n",
    "        # Exclude 'fsym_id' column from group_df\n",
    "        # print(group_df.head())\n",
    "        # non_zero_cols = group_df.columns[(group_df != 0).any()]\n",
    "        # group_df = group_df[non_zero_cols]\n",
    "\n",
    "        # if not group_df.empty:\n",
    "            # try:\n",
    "            # zero_columns = group_df.columns[(group_df == 0.0).all()].tolist()\n",
    "        columns_to_drop = ['fsym_id', 'label'] \n",
    "        X = tsfel.time_series_features_extractor(cfg, group_df.drop(columns_to_drop, axis=1), verbose=0)\n",
    "        X['fsym_id'] = group_df['fsym_id'].iloc[0]\n",
    "        X['label'] = group_df['label'].sum()\n",
    "        result_dfs.append(X)\n",
    "            # except ValueError:\n",
    "            #     print(f'{fsym_id} received value error')\n",
    "            #     continue\n",
    "    \n",
    "    final_result = pd.concat(result_dfs, ignore_index=True)\n",
    "    final_result.reset_index(drop=True, inplace=True)\n",
    "    return final_result\n",
    "\n",
    "train_df = feature_extraction(train_df)\n",
    "test_df = feature_extraction(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0bd36d3e-0ce5-4c71-ad49-0ab62e0811e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['label'] = train_df['label'].astype(int)\n",
    "test_df['label'] = test_df['label'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e1037567-3afd-471b-8094-1e245cc7987c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GDP_Max</th>\n",
       "      <th>Unemployment_Rate_Max</th>\n",
       "      <th>ff_assets_com_eq_Max</th>\n",
       "      <th>ff_assets_gr_Max</th>\n",
       "      <th>ff_assets_oth_tot_Max</th>\n",
       "      <th>ff_assets_per_emp_Max</th>\n",
       "      <th>ff_bk_oper_inc_oth_Max</th>\n",
       "      <th>ff_bps_gr_Max</th>\n",
       "      <th>ff_capex_assets_Max</th>\n",
       "      <th>ff_cash_div_cf_Max</th>\n",
       "      <th>...</th>\n",
       "      <th>ff_oper_inc_tcap_Max</th>\n",
       "      <th>ff_oper_ps_net_cf_Max</th>\n",
       "      <th>ff_reinvest_rate_Max</th>\n",
       "      <th>ff_roic_Max</th>\n",
       "      <th>ff_std_debt_Max</th>\n",
       "      <th>ff_tang_assets_debt_Max</th>\n",
       "      <th>ff_tcap_assets_Max</th>\n",
       "      <th>ff_xord_Max</th>\n",
       "      <th>fsym_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034648</td>\n",
       "      <td>6.7</td>\n",
       "      <td>5.582877</td>\n",
       "      <td>-3.455083</td>\n",
       "      <td>0.266</td>\n",
       "      <td>10.258813</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>58.529332</td>\n",
       "      <td>1.318405</td>\n",
       "      <td>4.372827e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>22.445824</td>\n",
       "      <td>2.984926</td>\n",
       "      <td>43.338964</td>\n",
       "      <td>17.425888</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>143.309067</td>\n",
       "      <td>96.665139</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.034648</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.221653</td>\n",
       "      <td>98.745592</td>\n",
       "      <td>3.236</td>\n",
       "      <td>0.856856</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>199.626566</td>\n",
       "      <td>2.304466</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>13.900820</td>\n",
       "      <td>4.869565</td>\n",
       "      <td>27.664081</td>\n",
       "      <td>25.354809</td>\n",
       "      <td>1.719117e+01</td>\n",
       "      <td>1941.969486</td>\n",
       "      <td>76.896607</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>B01HWF-R</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.034648</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.335485</td>\n",
       "      <td>50.789926</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.502957</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>75.468144</td>\n",
       "      <td>0.707123</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>-26.505512</td>\n",
       "      <td>-0.174382</td>\n",
       "      <td>-54.769119</td>\n",
       "      <td>-48.565640</td>\n",
       "      <td>3.787416e+01</td>\n",
       "      <td>2277.092242</td>\n",
       "      <td>62.982365</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>B04CB3-R</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.034648</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.487855</td>\n",
       "      <td>17.389720</td>\n",
       "      <td>35.711</td>\n",
       "      <td>6.250848</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>9.054891</td>\n",
       "      <td>0.497046</td>\n",
       "      <td>1.624332e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>15.434843</td>\n",
       "      <td>1.032558</td>\n",
       "      <td>12.693160</td>\n",
       "      <td>10.957011</td>\n",
       "      <td>9.722344e-01</td>\n",
       "      <td>7472.199352</td>\n",
       "      <td>9.291904</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>B04HL7-R</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.034648</td>\n",
       "      <td>6.7</td>\n",
       "      <td>8.994633</td>\n",
       "      <td>6.701215</td>\n",
       "      <td>4.987</td>\n",
       "      <td>0.385501</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>61.367784</td>\n",
       "      <td>3.529793</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>14.936892</td>\n",
       "      <td>3.565782</td>\n",
       "      <td>45.362437</td>\n",
       "      <td>8.727564</td>\n",
       "      <td>3.475691e+00</td>\n",
       "      <td>115.305485</td>\n",
       "      <td>76.879541</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>B04XY5-R</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    GDP_Max  Unemployment_Rate_Max  ff_assets_com_eq_Max  ff_assets_gr_Max  \\\n",
       "0  0.034648                    6.7              5.582877         -3.455083   \n",
       "1  0.034648                    6.7              2.221653         98.745592   \n",
       "2  0.034648                    6.7              2.335485         50.789926   \n",
       "3  0.034648                    6.7             18.487855         17.389720   \n",
       "4  0.034648                    6.7              8.994633          6.701215   \n",
       "\n",
       "   ff_assets_oth_tot_Max  ff_assets_per_emp_Max  ff_bk_oper_inc_oth_Max  \\\n",
       "0                  0.266              10.258813            1.000000e-10   \n",
       "1                  3.236               0.856856            1.000000e-10   \n",
       "2                  0.723               0.502957            1.000000e-10   \n",
       "3                 35.711               6.250848            1.000000e-10   \n",
       "4                  4.987               0.385501            1.000000e-10   \n",
       "\n",
       "   ff_bps_gr_Max  ff_capex_assets_Max  ff_cash_div_cf_Max  ...  \\\n",
       "0      58.529332             1.318405        4.372827e+01  ...   \n",
       "1     199.626566             2.304466        1.000000e-10  ...   \n",
       "2      75.468144             0.707123        1.000000e-10  ...   \n",
       "3       9.054891             0.497046        1.624332e+02  ...   \n",
       "4      61.367784             3.529793        1.000000e-10  ...   \n",
       "\n",
       "   ff_oper_inc_tcap_Max  ff_oper_ps_net_cf_Max  ff_reinvest_rate_Max  \\\n",
       "0             22.445824               2.984926             43.338964   \n",
       "1             13.900820               4.869565             27.664081   \n",
       "2            -26.505512              -0.174382            -54.769119   \n",
       "3             15.434843               1.032558             12.693160   \n",
       "4             14.936892               3.565782             45.362437   \n",
       "\n",
       "   ff_roic_Max  ff_std_debt_Max  ff_tang_assets_debt_Max  ff_tcap_assets_Max  \\\n",
       "0    17.425888     1.000000e-10               143.309067           96.665139   \n",
       "1    25.354809     1.719117e+01              1941.969486           76.896607   \n",
       "2   -48.565640     3.787416e+01              2277.092242           62.982365   \n",
       "3    10.957011     9.722344e-01              7472.199352            9.291904   \n",
       "4     8.727564     3.475691e+00               115.305485           76.879541   \n",
       "\n",
       "    ff_xord_Max   fsym_id  label  \n",
       "0  1.000000e-10  B00FG1-R      0  \n",
       "1  1.000000e-10  B01HWF-R      0  \n",
       "2  1.000000e-10  B04CB3-R      0  \n",
       "3  1.000000e-10  B04HL7-R      0  \n",
       "4  1.000000e-10  B04XY5-R      0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6cc4098-ebf6-4b62-a330-05af65838b21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# null_columns = train_df.columns[train_df.isnull().all()].tolist()\n",
    "# train_df2 = train_df.drop(null_columns, axis=1)\n",
    "# test_df2 = test_df.drop(null_columns, axis=1)\n",
    "common_columns = train_df.columns.intersection(test_df.columns)\n",
    "\n",
    "# Include only the common columns in each DataFrame\n",
    "train_df = train_df[common_columns]\n",
    "test_df = test_df[common_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a296c420-cdb2-4e09-ae15-99a7670fdae4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "626\n"
     ]
    }
   ],
   "source": [
    "null_columns = train_df.columns[train_df.isna().any()].tolist()\n",
    "train_df2 = train_df.drop(null_columns, axis=1)\n",
    "print(len(train_df2.columns))\n",
    "# test_df = test_df.drop(null_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8567db0d-d536-498c-8764-606907df48b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df.replace([np.inf, -np.inf], 0.0, inplace=True)\n",
    "test_df.replace([np.inf, -np.inf], 0.0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "27740f40-0f70-42f3-8613-96a982866460",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GDP_Max</th>\n",
       "      <th>Unemployment_Rate_Max</th>\n",
       "      <th>ff_assets_com_eq_Max</th>\n",
       "      <th>ff_assets_gr_Max</th>\n",
       "      <th>ff_assets_oth_tot_Max</th>\n",
       "      <th>ff_assets_per_emp_Max</th>\n",
       "      <th>ff_bk_oper_inc_oth_Max</th>\n",
       "      <th>ff_bps_gr_Max</th>\n",
       "      <th>ff_capex_assets_Max</th>\n",
       "      <th>ff_cash_div_cf_Max</th>\n",
       "      <th>...</th>\n",
       "      <th>ff_oper_inc_tcap_Max</th>\n",
       "      <th>ff_oper_ps_net_cf_Max</th>\n",
       "      <th>ff_reinvest_rate_Max</th>\n",
       "      <th>ff_roic_Max</th>\n",
       "      <th>ff_std_debt_Max</th>\n",
       "      <th>ff_tang_assets_debt_Max</th>\n",
       "      <th>ff_tcap_assets_Max</th>\n",
       "      <th>ff_xord_Max</th>\n",
       "      <th>fsym_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034648</td>\n",
       "      <td>6.7</td>\n",
       "      <td>5.582877</td>\n",
       "      <td>-3.455083</td>\n",
       "      <td>0.266</td>\n",
       "      <td>10.258813</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>58.529332</td>\n",
       "      <td>1.318405</td>\n",
       "      <td>4.372827e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>22.445824</td>\n",
       "      <td>2.984926</td>\n",
       "      <td>43.338964</td>\n",
       "      <td>17.425888</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>143.309067</td>\n",
       "      <td>96.665139</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.034648</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.221653</td>\n",
       "      <td>98.745592</td>\n",
       "      <td>3.236</td>\n",
       "      <td>0.856856</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>199.626566</td>\n",
       "      <td>2.304466</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>13.900820</td>\n",
       "      <td>4.869565</td>\n",
       "      <td>27.664081</td>\n",
       "      <td>25.354809</td>\n",
       "      <td>1.719117e+01</td>\n",
       "      <td>1941.969486</td>\n",
       "      <td>76.896607</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>B01HWF-R</td>\n",
       "      <td>3.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.034648</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.335485</td>\n",
       "      <td>50.789926</td>\n",
       "      <td>0.723</td>\n",
       "      <td>0.502957</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>75.468144</td>\n",
       "      <td>0.707123</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>-26.505512</td>\n",
       "      <td>-0.174382</td>\n",
       "      <td>-54.769119</td>\n",
       "      <td>-48.565640</td>\n",
       "      <td>3.787416e+01</td>\n",
       "      <td>2277.092242</td>\n",
       "      <td>62.982365</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>B04CB3-R</td>\n",
       "      <td>3.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.034648</td>\n",
       "      <td>6.7</td>\n",
       "      <td>18.487855</td>\n",
       "      <td>17.389720</td>\n",
       "      <td>35.711</td>\n",
       "      <td>6.250848</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>9.054891</td>\n",
       "      <td>0.497046</td>\n",
       "      <td>1.624332e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>15.434843</td>\n",
       "      <td>1.032558</td>\n",
       "      <td>12.693160</td>\n",
       "      <td>10.957011</td>\n",
       "      <td>9.722344e-01</td>\n",
       "      <td>7472.199352</td>\n",
       "      <td>9.291904</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>B04HL7-R</td>\n",
       "      <td>3.000000e-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.034648</td>\n",
       "      <td>6.7</td>\n",
       "      <td>8.994633</td>\n",
       "      <td>6.701215</td>\n",
       "      <td>4.987</td>\n",
       "      <td>0.385501</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>61.367784</td>\n",
       "      <td>3.529793</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>...</td>\n",
       "      <td>14.936892</td>\n",
       "      <td>3.565782</td>\n",
       "      <td>45.362437</td>\n",
       "      <td>8.727564</td>\n",
       "      <td>3.475691e+00</td>\n",
       "      <td>115.305485</td>\n",
       "      <td>76.879541</td>\n",
       "      <td>1.000000e-10</td>\n",
       "      <td>B04XY5-R</td>\n",
       "      <td>2.000000e-10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    GDP_Max  Unemployment_Rate_Max  ff_assets_com_eq_Max  ff_assets_gr_Max  \\\n",
       "0  0.034648                    6.7              5.582877         -3.455083   \n",
       "1  0.034648                    6.7              2.221653         98.745592   \n",
       "2  0.034648                    6.7              2.335485         50.789926   \n",
       "3  0.034648                    6.7             18.487855         17.389720   \n",
       "4  0.034648                    6.7              8.994633          6.701215   \n",
       "\n",
       "   ff_assets_oth_tot_Max  ff_assets_per_emp_Max  ff_bk_oper_inc_oth_Max  \\\n",
       "0                  0.266              10.258813            1.000000e-10   \n",
       "1                  3.236               0.856856            1.000000e-10   \n",
       "2                  0.723               0.502957            1.000000e-10   \n",
       "3                 35.711               6.250848            1.000000e-10   \n",
       "4                  4.987               0.385501            1.000000e-10   \n",
       "\n",
       "   ff_bps_gr_Max  ff_capex_assets_Max  ff_cash_div_cf_Max  ...  \\\n",
       "0      58.529332             1.318405        4.372827e+01  ...   \n",
       "1     199.626566             2.304466        1.000000e-10  ...   \n",
       "2      75.468144             0.707123        1.000000e-10  ...   \n",
       "3       9.054891             0.497046        1.624332e+02  ...   \n",
       "4      61.367784             3.529793        1.000000e-10  ...   \n",
       "\n",
       "   ff_oper_inc_tcap_Max  ff_oper_ps_net_cf_Max  ff_reinvest_rate_Max  \\\n",
       "0             22.445824               2.984926             43.338964   \n",
       "1             13.900820               4.869565             27.664081   \n",
       "2            -26.505512              -0.174382            -54.769119   \n",
       "3             15.434843               1.032558             12.693160   \n",
       "4             14.936892               3.565782             45.362437   \n",
       "\n",
       "   ff_roic_Max  ff_std_debt_Max  ff_tang_assets_debt_Max  ff_tcap_assets_Max  \\\n",
       "0    17.425888     1.000000e-10               143.309067           96.665139   \n",
       "1    25.354809     1.719117e+01              1941.969486           76.896607   \n",
       "2   -48.565640     3.787416e+01              2277.092242           62.982365   \n",
       "3    10.957011     9.722344e-01              7472.199352            9.291904   \n",
       "4     8.727564     3.475691e+00               115.305485           76.879541   \n",
       "\n",
       "    ff_xord_Max   fsym_id         label  \n",
       "0  1.000000e-10  B00FG1-R  2.000000e-10  \n",
       "1  1.000000e-10  B01HWF-R  3.000000e-10  \n",
       "2  1.000000e-10  B04CB3-R  3.000000e-10  \n",
       "3  1.000000e-10  B04HL7-R  3.000000e-10  \n",
       "4  1.000000e-10  B04XY5-R  2.000000e-10  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "18bd9768-4d04-4fd3-8984-2f6a6187b6d1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_df[test_df['label']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19433828-fa32-4d67-815b-162686b458a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "961\n",
      "961\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df.columns))\n",
    "print(len(test_df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c922f9f-bdd4-42aa-bc3a-7f6d8ae4048c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "8990\n",
      "7049\n",
      "147\n",
      "520\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df.columns))\n",
    "print(len(train_df))\n",
    "print(len(test_df))\n",
    "print(len(test_df[test_df['label']==1]))\n",
    "print(len(train_df[train_df['label']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c8ba14ec-26eb-4d15-8f69-48c1f446e353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feats = train_df.columns[:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "73d9643f-ef4e-4eb0-8c37-88fd204bb19a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to Pandas\n",
      "{0: 0.5248085513720485, 1: 10.57717041800643}\n",
      " 19%|â–ˆâ–‰        | 19/100 [04:02<17:15, 12.79s/trial, best loss: -0.4749228769989395]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       866\n",
      "           1       0.21      0.13      0.16        31\n",
      "\n",
      "    accuracy                           0.95       897\n",
      "   macro avg       0.59      0.56      0.57       897\n",
      "weighted avg       0.94      0.95      0.95       897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from sklearn import tree\n",
    "import shap\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from collections import Counter\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "from functools import reduce\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "def feature_importances(model, features):\n",
    "    feature_importances = model.feature_importances_\n",
    "\n",
    "    # print(\"Feature Importances:\")\n",
    "    # for feature, importance in zip(features, feature_importances):\n",
    "    #     print(f\"{feature}: {importance}\")\n",
    "\n",
    "    sorted_idx = np.argsort(feature_importances)\n",
    "    sorted_features = [features[i] for i in sorted_idx]\n",
    "    \n",
    "    half_len = (len(sorted_idx) // 4 ) * 3 # Calculate the index for the middle point\n",
    "\n",
    "    # Select the lowest 50% of features\n",
    "    selected_features = [features[i] for i in sorted_idx[:half_len]]\n",
    "\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.bar(range(len(feature_importances)), feature_importances[sorted_idx], align=\"center\")\n",
    "    plt.xticks(range(len(feature_importances)), sorted_features, rotation=45, ha=\"right\")\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"Feature Importance\")\n",
    "    plt.title(\"Feature Importances\")\n",
    "    plt.show()\n",
    "    return selected_features\n",
    "\n",
    "def model_testing(train_df, test_df, classifier):\n",
    "    seed = 42\n",
    "    print(\"Converted to Pandas\")\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "    X_train = train_df.drop(exclude_columns, axis=1)\n",
    "    y_train = train_df['label']\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    print(class_weight_dict)\n",
    "    \n",
    "    if classifier == 'LogisticRegression':\n",
    "        param_space = {\n",
    "            'C': hp.uniform('C', 0.01, 1.0) }\n",
    "        classifier_instance = LogisticRegression(class_weight = class_weight_dict, solver='sag', seed=42)\n",
    "        scaler = StandardScaler()\n",
    "        feats = X_train.columns\n",
    "        X_train[feats] = scaler.fit_transform(X_train[feats])\n",
    "        \n",
    "    elif classifier == 'RandomForest':\n",
    "        param_space = { \n",
    "            'n_estimators': hp.quniform('n_estimators', 100, 500, 1),\n",
    "            'max_depth': hp.quniform('max_depth', 5, 20, 1)\n",
    "        }\n",
    "        classifier_instance = RandomForestClassifier(class_weight = class_weight_dict, random_state=42)\n",
    "    elif classifier == 'GBT':\n",
    "        param_space = { 'n_estimators':hp.uniform('n_estimators',100,500),\n",
    "           'max_depth':hp.quniform('max_depth',5,20,1),\n",
    "           'min_samples_leaf':hp.quniform('min_samples_leaf',1,5,1),\n",
    "           'min_samples_split':hp.quniform('min_samples_split',2,6,1)}\n",
    "        classifier_instance = GradientBoostingClassifier(seed=42)\n",
    "    elif classifier == 'XGB':\n",
    "        param_space = { 'n_estimators':hp.quniform('n_estimators',100,500,1),\n",
    "           'max_depth':hp.quniform('max_depth',5,20,1)}\n",
    "        counter = Counter(y_train)\n",
    "        # estimate scale_pos_weight value\n",
    "        estimate = counter[0] / counter[1]\n",
    "        print('Estimate: %.3f' % estimate)\n",
    "        \n",
    "        classifier_instance = xgb.XGBClassifier(scale_pos_weight=estimate, seed=42)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported classifier\")\n",
    "    \n",
    "    def set_params(classifier, params):\n",
    "        if classifier == 'RandomForest':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            # params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            # params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        elif classifier == 'GBT':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        elif classifier == 'XGB':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            # params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            # params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        \n",
    "        else:\n",
    "            return params\n",
    "    \n",
    "    losses = []\n",
    "        \n",
    "    def objective(params):\n",
    "        params = set_params(classifier, params)\n",
    "        classifier_instance.set_params(**params)\n",
    "        \n",
    "        scores = cross_val_score(classifier_instance, X_train, y_train, cv=3, scoring='matthews_corrcoef')\n",
    "        score = -scores.mean()\n",
    "        losses.append(score)\n",
    "        return score\n",
    "\n",
    "    def report_average(*args):\n",
    "        report_list = list()\n",
    "        for report in args:\n",
    "            splited = [' '.join(x.split()) for x in report.split('\\n\\n')]\n",
    "            header = [x for x in splited[0].split(' ')]\n",
    "            data = np.array(splited[1].split(' ')).reshape(-1, len(header) + 1)\n",
    "            data = np.delete(data, 0, 1).astype(float)\n",
    "            rest = splited[2].split(' ')\n",
    "            accuarcy =np.array([0, 0, rest[1], rest[2]]).astype(float).reshape(-1, len(header))\n",
    "            macro_avg = np.array([rest[5:9]]).astype(float).reshape(-1, len(header))\n",
    "            weighted_avg = np.array([rest[11:]]).astype(float).reshape(-1, len(header))\n",
    "            #avg_total = np.array([x for x in avg]).astype(float).reshape(-1, len(header))\n",
    "            df = pd.DataFrame(np.concatenate((data, accuarcy,macro_avg,weighted_avg)), columns=header)\n",
    "            report_list.append(df)\n",
    "        res = reduce(lambda x, y: x.add(y, fill_value=0), report_list) / len(report_list)\n",
    "        res.to_csv(f'when_{classifier}_results.csv')\n",
    "        return res.rename(index={res.index[-3]: 'accuracy',res.index[-2]: 'macro_avg',res.index[-1]: 'weighted_avg'})\n",
    "    \n",
    "    \n",
    "    best_params = fmin(fn=objective, space=param_space, algo=tpe.suggest, max_evals=100, early_stop_fn=no_progress_loss(10))\n",
    "    \n",
    "    plt.plot(np.arange(len(losses)), losses)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss during Hyperopt Optimization')\n",
    "    plt.show()\n",
    "    \n",
    "    best_params = set_params(classifier, best_params)\n",
    "    classifier_instance.set_params(**best_params)\n",
    "    \n",
    "    classifier_instance.fit(X_train, y_train)\n",
    "    X_test = test_df.drop(exclude_columns, axis=1)\n",
    "    y_test = test_df['label']\n",
    "    preds = classifier_instance.predict(X_test)\n",
    "    \n",
    "    report = classification_report(y_test, preds)\n",
    "    print(report)\n",
    "    with open(f'report_{classifier}_if_pandas', \"w\") as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    return classifier_instance, X_train.columns.tolist(), X_train\n",
    "    \n",
    "    \n",
    "#     i = 0\n",
    "#     final_recall = None\n",
    "#     all_reports = []\n",
    "#     for train_index, test_index in tscv.split(X_train):\n",
    "#         x_train, x_test = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "#         Y_train, Y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        \n",
    "#         classifier_instance.fit(x_train, Y_train)\n",
    "        \n",
    "#         preds = classifier_instance.predict(x_test)\n",
    "#         report = classification_report(Y_test, preds)\n",
    "#         print(report)\n",
    "#         all_reports.append(report)\n",
    "#         # cm = confusion_matrix(Y_test, preds, labels=classifier_instance.classes_)\n",
    "#         # plt.figure(figsize=(8, 6))\n",
    "#         # sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
    "#         # plt.xlabel(\"Predicted\")\n",
    "#         # plt.ylabel(\"Actual\")\n",
    "#         # plt.title(\"Confusion Matrix\")\n",
    "#         # plt.show()\n",
    "#         # final_recall = recall_score(Y_test, preds, pos_label=1)\n",
    "#         # return recall_minority_class\n",
    "#     # return final_recall\n",
    "#     final_report = report_average(*all_reports)\n",
    "#     print(final_report)\n",
    "#     # print(\"MCC: \", matthews_corrcoef(true, preds))\n",
    "#     final_report.to_csv(f'report_{classifier}_if_pandas')\n",
    "#         # f.write(f\"\\n\\nMatthews Correlation Coefficient: {matthews_corrcoef(true, preds)}\")\n",
    "        \n",
    "#     return classifier_instance, X_train.columns.tolist(), X_train\n",
    "\n",
    "\n",
    "model, feats, X_train = model_testing(train_df, test_df, 'RandomForest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76602b7c-3a5e-4c5a-a903-9e20808a9e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top_feats = feature_importances(model,feats, 'RandomForest')\n",
    "\n",
    "df2 = df.select(*top_feats, 'fsym_id', 'label')\n",
    "# df2 = df.drop(*feats_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc9139-0926-4acc-8b62-5db6cabcd2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model2, feats2, X_train2 = model_training_spark(df2, 'RandomForest')\n",
    "_ = feature_importances(model2,top_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ef5a58-f645-44a2-ba76-988859d95f93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "\n",
    "def shapley(model, train, test, model_name):\n",
    "    train = train.toPandas()\n",
    "    test=test.toPandas()\n",
    "    exclude_columns = ['fsym_id',  'label', 'features_vector']\n",
    "    X_train = train.drop(exclude_columns, axis=1)\n",
    "    X_test = test.drop(exclude_columns, axis=1)\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(X_train)\n",
    "    shap.initjs()\n",
    "    print(shap_values.shape)\n",
    "    # shap.plots.waterfall(shap_values[0])\n",
    "    shap.summary_plot(shap_values, show=False)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name}_shap_det.png')\n",
    "    \n",
    "    \n",
    "# shapley(model, train_df, test_df, model_name='rf')\n",
    "    \n",
    "# shapley(model, train_df, test_df, model_name='rf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1593aa9a-1445-4a89-abaa-99012fd63ca8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "\n",
    "def plot_model_performance(mdl, loss, metric):\n",
    "    x = pd.DataFrame(mdl.history).reset_index()\n",
    "    x = pd.melt(x, id_vars='index')\n",
    "    x['validation'] = (x['variable'].str[:4] == 'val_').replace({True:'validation',False:'training'})\n",
    "    x['loss'] = (x['variable'].str[-4:] == 'loss').replace({True:loss,False:metric})\n",
    "    g = sns.FacetGrid(x, col='loss', hue='validation',sharey=False)\n",
    "    g.map(sns.lineplot, 'index','value')\n",
    "    g.add_legend()\n",
    "    return g\n",
    "\n",
    "def nn_training(df):\n",
    "    train_df, test_df = t_t_split(df)\n",
    "\n",
    "    train_df = train_df.toPandas()\n",
    "    train_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    # Drop rows containing NaN values\n",
    "    train_df.dropna(axis=0, how='any', inplace=True)\n",
    "    \n",
    "    test_df = test_df.toPandas()\n",
    "    train_X = train_df.drop(['fsym_id', 'label'], axis=1).values\n",
    "    train_y = np.array(train_df['label'])\n",
    "    test_X = test_df.drop(['fsym_id', 'label'], axis=1).values\n",
    "    test_y = np.array(test_df['label'])\n",
    "    print(np.sum(test_y==1))\n",
    "    print(train_X, train_y)\n",
    "    \n",
    "    class_labels = np.unique(train_y)\n",
    "    class_weights = compute_class_weight('balanced', classes=class_labels, y=train_y.flatten())\n",
    "    class_weight_dict = dict(zip(class_labels, class_weights))\n",
    "    print(class_weight_dict)\n",
    "    \n",
    "\n",
    "    # Define the neural network model\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Flatten(input_shape=(train_X.shape[1],)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),  # Additional Dense layer\n",
    "        tf.keras.layers.Dropout(0.5),  # Dropout layer for regularization\n",
    "        tf.keras.layers.Dense(16, activation='relu'),  # Another Dense layer\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    # model = keras.Sequential()\n",
    "    # model.add(layers.Dense(16,activation=\"relu\",input_shape=(train_X.shape[1],)))\n",
    "    # model.add(layers.Dense(8,activation=\"tanh\"))\n",
    "    # model.add(layers.Dense(1))\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    loss_fn = keras.losses.BinaryCrossentropy()\n",
    "    optimizer = keras.optimizers.Adam(\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "    # Train the model\n",
    "    # early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    fit_model = model.fit(train_X, train_y, epochs=50, batch_size=32, validation_split=0.1, class_weight = class_weight_dict)\n",
    "    plot_model_performance(fit_model, 'bin_cross_entropy','accuracy')\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_loss, test_acc = model.evaluate(test_X, test_y)\n",
    "    print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "    # Make predictions on new data\n",
    "    predictions = model.predict(test_X)\n",
    "    for i in range(len(predictions)):\n",
    "        predictions[i] = 1 if predictions[i] >= 0.5 else 0\n",
    "    print(classification_report(predictions, test_y.flatten()))\n",
    "    \n",
    "    # pred_df = pd.DataFrame()\n",
    "    # pred_df['prediction'] = predictions\n",
    "    # pred_df['label'] = test_y\n",
    "    # confusion_matrix_pandas(pred_df)\n",
    "    cm = confusion_matrix(test_y, predictions)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "nn_training(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b557ef83-82be-4f6d-b70e-6f59fd115154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def anomaly_det(df):\n",
    "    \n",
    "    seed = 42\n",
    "    train_df, test_df = t_t_split(df)\n",
    "    train_df = train_df.toPandas()\n",
    "    test_df = test_df.toPandas()\n",
    "    print(\"Converted to Pandas\")\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "    \n",
    "    \n",
    "    features =train_df.columns[:-2]\n",
    "    print(len(features))\n",
    "    \n",
    "    num_pos = len(train_df[train_df['label']==1])\n",
    "    print(num_pos/len(train_df))\n",
    "    isol_for = IsolationForest(contamination=num_pos/len(train_df), random_state=42)\n",
    "    \n",
    "    isol_for.fit(train_df[features])\n",
    "\n",
    "    train_df['anomaly_scores'] = isol_for.decision_function(train_df[features])\n",
    "    train_df['anomaly'] = isol_for.predict(train_df[features])\n",
    "    train_df['preds'] = np.where(train_df['anomaly'] == 1, 0, 1)\n",
    "\n",
    "    test_df['anomaly_scores'] = isol_for.decision_function(test_df[features])\n",
    "    test_df['anomaly'] = isol_for.predict(test_df[features])\n",
    "    test_df['preds'] = np.where(test_df['anomaly'] == 1, 0, 1)\n",
    "    \n",
    "    print(f\"Classification Report: \")\n",
    "    print(classification_report(test_df['label'], test_df['preds']))\n",
    "    return test_df\n",
    "    \n",
    "    \n",
    "test_df_isol = anomaly_det(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9938e78b-a720-4512-b9ea-6a90ab073601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from CreateDataset import get_fund_data\n",
    "import math\n",
    "\n",
    "def plotting_stocks_pandas(df):\n",
    "    imploded_stocks = df[(df['label'] == 1) & (df['preds'] == 0)]\n",
    "    spark_df = spark.createDataFrame(imploded_stocks['fsym_id'].to_frame())\n",
    "    imp_prices = get_fund_data(spark_df)\n",
    "    \n",
    "    adj_pd = imp_prices.toPandas()\n",
    "    adj_pd['date'] = pd.to_datetime(adj_pd['date'])\n",
    "    list_to_plot = sorted(adj_pd['fsym_id'].unique().tolist())\n",
    "    \n",
    "    columns = 8\n",
    "    num_rows = math.ceil(len(list_to_plot) / columns)\n",
    "    fig, axs = plt.subplots(nrows=num_rows, ncols=columns, figsize=(35, 5*num_rows))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    i = 0\n",
    "    for t in list_to_plot:\n",
    "        temp_df = adj_pd[adj_pd['fsym_id']==t]\n",
    "        axs[i].plot(temp_df['date'], temp_df['adj_price'], label=t)\n",
    "\n",
    "        axs[i].legend()\n",
    "        #axs[i].text(0.5, -0.1, f'Volatility: {vol:.2f}', ha='center', transform=axs[i].transAxes)\n",
    "        i+=1\n",
    "        \n",
    "    for i in range(len(list_to_plot), num_rows * columns):\n",
    "        fig.delaxes(axs.flatten()[i])\n",
    "    \n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('implosions_not_detected_by_model.png')\n",
    "    \n",
    "# print(len(test_df_isol[test_df_isol['preds']==1]))\n",
    "# print(len(test_df_isol[test_df_isol['label']==1]))\n",
    "# plotting_stocks_pandas(test_df_isol)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de2078-0d47-4339-805d-67e3099f0a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Boruta import BorutaPy\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "def boruta_fs(train_df, model_name): #HOW DOES BORUTA ACC WORK?\n",
    "    train_df = train_df.toPandas()\n",
    "    X_train = train_df.drop(['fsym_id', 'date', 'label'], axis=1)\n",
    "    y_train = train_df['label']\n",
    "    \n",
    "    if model_name == 'rf':\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        model = GradientBoostingClassifier\n",
    "    feat_selector = BorutaPy(model, n_estimators='auto', verbose=1, random_state=1)\n",
    "    feat_selector.fit(X_train, y_train)\n",
    "    features = X_train.columns.tolist()\n",
    "    print(\"Number of features: \", len(features) )\n",
    "    feature_ranks = list(zip(features, feat_selector.ranking_, feat_selector.support_))\n",
    "    selected_features = []\n",
    "    for feat in feature_ranks:\n",
    "        print(f\"Feature: {feat[0]}, Rank: {feat[1]}, Keep: {feat[2]}\")\n",
    "        if feat[1] <= 5:\n",
    "            selected_features.append(feat[0])\n",
    "    print(\"Selected features: \", selected_features)\n",
    "    return selected_features\n",
    "\n",
    "rf_feats = boruta_fs(df, 'rf')\n",
    "# gbt_feats = boruta_fs(df, 'gbt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f66c89-d4c3-485b-93f5-c9fa821c0a82",
   "metadata": {},
   "source": [
    "### Investigating metrics that changed the most before and after implosions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f38521f-bf73-407e-a6f1-33f7a17eb635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1fc72f-006a-44b5-83fd-21ba1e382ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "# import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "\n",
    "def pct_change_df(df, big_string, table):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, t.Implosion_Start_Date, b.date, {big_string}\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN sym_ticker_region s ON s.fsym_id = t.fsym_id\n",
    "                LEFT JOIN {table} a ON s.fsym_id = a.fsym_id AND  YEAR(a.date) = YEAR(t.Implosion_Start_Date)\n",
    "                LEFT JOIN {table} b ON s.fsym_id = b.fsym_id AND  YEAR(b.date) = YEAR(t.Implosion_Start_Date)-1\n",
    "                ORDER BY t.fsym_id\n",
    "            \"\"\"\n",
    "    df1 = spark.sql(query1)\n",
    "    #print(df1.show())\n",
    "    df1 = df1.toPandas()\n",
    "    df1 = df1.drop(['fsym_id','Implosion_Start_Date','date'], axis=1)\n",
    "    \n",
    "    def remove_outliers(column):\n",
    "        Q1 = column.quantile(0.25)\n",
    "        Q3 = column.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return column[(column >= lower_bound) & (column <= upper_bound)]\n",
    "\n",
    "\n",
    "\n",
    "    df1 = df1.abs()\n",
    "    null_percentage = df1.isnull().sum() / len(df1)\n",
    "    columns_to_keep = null_percentage[null_percentage <= 0.3].index\n",
    "    df_nulls_removed = df1[columns_to_keep]\n",
    "    print(\"Columns kept: \", len(columns_to_keep)/len(df1.columns))\n",
    "    \n",
    "    df_no_outliers = df_nulls_removed.apply(remove_outliers)\n",
    "\n",
    "    \n",
    "    column_means_no_outliers = df_no_outliers.mean()\n",
    "    #column_means_no_outliers = column_means_no_outliers.dropna()\n",
    "    column_means_no_outliers = column_means_no_outliers.sort_values()\n",
    "    feats = column_means_no_outliers.tail(5)\n",
    "\n",
    "    print(\"Largest averages of differences between previous year and implosion year: \",feats)\n",
    "    return feats.index.tolist()\n",
    "    \n",
    "def avg_change_df(df, big_string, table):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, {big_string}\n",
    "                FROM temp_table t  \n",
    "                LEFT JOIN sym_ticker_region s ON s.fsym_id = t.fsym_id\n",
    "                LEFT JOIN {table} a ON s.fsym_id = a.fsym_id AND  YEAR(a.date) > YEAR(t.Implosion_Start_Date)\n",
    "                LEFT JOIN {table} b ON s.fsym_id = b.fsym_id AND  YEAR(b.date) < YEAR(t.Implosion_Start_Date)\n",
    "                GROUP BY t.fsym_id\n",
    "                ORDER BY t.fsym_id\n",
    "            \"\"\"\n",
    "    df1 = spark.sql(query1)\n",
    "    df1 = df1.toPandas()\n",
    "    df1 = df1.drop(['fsym_id'], axis=1)\n",
    "    \n",
    "    def remove_outliers(column):\n",
    "        Q1 = column.quantile(0.25)\n",
    "        Q3 = column.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return column[(column >= lower_bound) & (column <= upper_bound)]\n",
    "\n",
    "\n",
    "    df1 = df1.abs()\n",
    "    null_percentage = df1.isnull().sum() / len(df1)\n",
    "    columns_to_keep = null_percentage[null_percentage <= 0.3].index\n",
    "    df_nulls_removed = df1[columns_to_keep]\n",
    "    print(\"Columns kept: \", len(columns_to_keep)/len(df1.columns))\n",
    "    \n",
    "    df_no_outliers = df_nulls_removed.apply(remove_outliers)\n",
    "    \n",
    "    column_means_no_outliers = df_no_outliers.mean()\n",
    "    #column_means_no_outliers = column_means_no_outliers.dropna()\n",
    "    column_means_no_outliers = column_means_no_outliers.sort_values()\n",
    "    feats = column_means_no_outliers.tail(5)\n",
    "    print(\"Largest averages of differences in average before and after implosion date: \", feats)\n",
    "#     for feature in feats.index:\n",
    "#         before_implosion = df_no_outliers[feature][df_no_outliers.index.isin(df1[df1[feature].notnull() & (df1['date'] < df1['Implosion_Start_Date'])].index)]\n",
    "#         after_implosion = df_no_outliers[feature][df_no_outliers.index.isin(df1[df1[feature].notnull() & (df1['date'] > df1['Implosion_Start_Date'])].index)]\n",
    "        \n",
    "#         _, p_value = ttest_ind(before_implosion, after_implosion)\n",
    "        \n",
    "#         print(f\"T-test p-value for {feature}: {p_value}\")\n",
    "    return feats.index.tolist()\n",
    "\n",
    "def t_test():\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_metric_changes(filename, table):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df = df[df['Implosion_Start_Date'].notnull()]\n",
    "    df['Implosion_Start_Date'] = pd.to_datetime(df['Implosion_Start_Date']).dt.date\n",
    "    df['Implosion_End_Date'] = pd.to_datetime(df['Implosion_End_Date']).dt.date\n",
    "    cols = get_not_null_cols(df, table)\n",
    "    result_string = ', '.join('(a.' + item + '-b.' + item +')/b.'+item + ' AS ' + item for item in cols)\n",
    "    feats1 = pct_change_df(df, result_string, table) #change 1 year before\n",
    "    print(\"Features with greatest percentage change with year before implosion: \", feats1)\n",
    "    \n",
    "    result_string2 = ', '.join('(MEAN(a.' + item + ')-MEAN(b.' + item +'))/MEAN(b.'+item + ') AS ' + item for item in cols)\n",
    "    feats2 = avg_change_df(df, result_string2, table)\n",
    "    print(\"Features with greatest percentage change in mean before and after implosion\", feats2)\n",
    "    \n",
    "    write_features_file( list(set(feats1+feats2)) )\n",
    "\n",
    "\n",
    "get_metric_changes('imploded_stocks_price.csv', 'FF_ADVANCED_DER_AF')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7274d-d2b0-4016-a2b6-4f7633f51fe7",
   "metadata": {},
   "source": [
    "### Correlations with Market Value Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc63eb-7d98-4de8-b665-808509638bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from CreateDataset import get_feature_col_names, get_fund_data\n",
    "\n",
    "\n",
    "def corr_query(implosion_df, col_string, table): \n",
    "    df = get_fund_data(implosion_df)\n",
    "    df=df.withColumn('year', F.year('date'))\n",
    "    window_spec = Window.partitionBy('fsym_id', 'year').orderBy(col('date').desc())\n",
    "\n",
    "    df = df.withColumn('row_num', F.row_number().over(window_spec))\n",
    "\n",
    "    df = df.filter(col('row_num') == 1).orderBy('date') #should we compare correlations with market val?\n",
    "    #should we do quarterly?\n",
    "    \n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, t.adj_price, t.Market_Value, t.date, {col_string}\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id AND YEAR(t.date)=YEAR(a.date)\n",
    "                ORDER BY t.fsym_id, t.date\n",
    "            \"\"\"\n",
    " \n",
    "    q_df = spark.sql(query1)\n",
    "    #q_df.show()\n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy('date')\n",
    "    \n",
    "    q_df = q_df.withColumn(\"return_market_val\", (F.col('Market_Value') - F.lag('Market_Value').over(window_spec)) / F.lag('Market_Value').over(window_spec))\n",
    "    q_df = q_df.withColumn(\"return\", (F.col('adj_price') - F.lag('adj_price').over(window_spec)) / F.lag('adj_price').over(window_spec))\n",
    "    \n",
    "    return_columns = [c[2:] for c in col_string.split(\", \")]\n",
    "    mean_corrs = []\n",
    "    corr_vals = []\n",
    "    #I THINK U NEED TO GROUP BY DATE AND THEN CALCULATE CORRELATIONS\n",
    "\n",
    "    for column in return_columns:\n",
    "        return_col_name = f\"return_{column}\"\n",
    "        corr_col_name = f\"corr_with_{column}\"\n",
    "        q_df = q_df.withColumn(return_col_name, (F.col(column) - F.lag(column).over(window_spec)) / F.lag(column).over(window_spec))\n",
    "        q_df = q_df.withColumn(column, F.corr(return_col_name, 'return_market_val').over(window_spec)) #calculating correlations with market value return\n",
    "        q_df = q_df.drop(*[return_col_name])\n",
    "    q_df = q_df.drop(*['return_market_val', 'return'])\n",
    "    q_df = q_df.select(q_df.columns[4:])\n",
    "    mean_corrs = q_df.agg(*[F.mean(F.abs(F.col(column))).alias(column) for column in q_df.columns])\n",
    "    # mean_corrs.show()\n",
    "    \n",
    "    return mean_corrs.toPandas()\n",
    "\n",
    "def corr_analysis(table):\n",
    "    imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "    imp_df_price = imp_df_price.loc[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "    cols = get_not_null_cols(imp_df_price, 'FF_ADVANCED_DER_AF')\n",
    "    result_string = ', '.join('a.' + item for item in cols)\n",
    "    mean_corrs_df = corr_query(spark.createDataFrame(imp_df_price), result_string, 'FF_ADVANCED_DER_AF')\n",
    "    mean_corrs = mean_corrs_df.to_dict(orient='records')\n",
    "    sorted_corrs = dict(sorted(mean_corrs[0].items(), key=lambda item: item[1], reverse=True))\n",
    "    top_records = list(sorted_corrs.items())[:5]\n",
    "    top_10 = []\n",
    "    for r in top_records:\n",
    "        top_10.append(r[0])\n",
    "    print(top_10)\n",
    "    current_feature_list = get_feature_col_names()\n",
    "    new_feature_list = list(set(current_feature_list + top_10))\n",
    "    \n",
    "    write_features_file(new_feature_list)\n",
    "    \n",
    "    \n",
    "corr_analysis('FF_Advanced_Der_AF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3722ccc-0c58-4599-a464-6e153f4e1f13",
   "metadata": {},
   "source": [
    "### Adding the Extra Features From Literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586949c-01ca-4b25-88c1-1488355015e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "imp_df_price['Implosion_Start_Date'] = pd.to_datetime(imp_df_price['Implosion_Start_Date'])\n",
    "imp_df_price['Implosion_End_Date'] = pd.to_datetime(imp_df_price['Implosion_End_Date'])\n",
    "available_feats = get_not_null_cols(imp_df_price)\n",
    "extra_feats = ['ff_capex_assets', 'ff_gross_cf_debt', 'ff_mkt_val_gr']\n",
    "\n",
    "current_feats = get_feature_col_names()\n",
    "final_feats = list(set(current_feats + extra_feats))\n",
    "write_features_file(final_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca6a8a0-1c51-42e5-8a36-18044f9e9bc4",
   "metadata": {},
   "source": [
    "### Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203b078-0417-4933-afe8-4442a98809ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(all_feats=False, imploded_only=False):\n",
    "    df = get_tabular_dataset(all_feats=all_feats, imploded_only=imploded_only)\n",
    "    df = forward_fill(df)\n",
    "    print(\"Number of rows: \", df.count())\n",
    "    print(\"Number of positives: \", df.filter(F.col('label')==1).count())\n",
    "    df=df.fillna(0.0)\n",
    "    print(\"Number of rows after dropping nulls: \", df.count())\n",
    "    print(\"Number of positives after dropping nulls: \", df.filter(F.col('label')==1).count())\n",
    "    return df\n",
    "\n",
    "\n",
    "def forward_fill(df):\n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy('date')\n",
    "    feature_cols = df.columns[2:-1]\n",
    "    for c in feature_cols:\n",
    "        df = df.withColumn(\n",
    "            c, F.last(c, ignorenulls=True).over(window_spec)\n",
    "        )\n",
    "    return df.orderBy('fsym_id','date')\n",
    "\n",
    "df = get_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0e0a7-413f-4d65-9fec-31092302bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Boruta import BorutaPy\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "def boruta_fs(train_df, model_name): #HOW DOES BORUTA ACC WORK?\n",
    "    train_df = train_df.toPandas()\n",
    "    X_train = train_df.drop(['fsym_id', 'date', 'label'], axis=1)\n",
    "    y_train = train_df['label']\n",
    "    \n",
    "    if model_name == 'rf':\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        model = GradientBoostingClassifier\n",
    "    feat_selector = BorutaPy(model, n_estimators='auto', verbose=2, random_state=1)\n",
    "    feat_selector.fit(X_train, y_train)\n",
    "    features = X_train.columns.tolist()\n",
    "    print(\"Number of features: \", len(features) )\n",
    "    feature_ranks = list(zip(features, feat_selector.ranking_, feat_selector.support_))\n",
    "    selected_features = []\n",
    "    for feat in feature_ranks:\n",
    "        print(f\"Feature: {feat[0]}, Rank: {feat[1]}, Keep: {feat[2]}\")\n",
    "        if feat[1] <= 5:\n",
    "            selected_features.append(feat[0])\n",
    "    print(\"Selected features: \", selected_features)\n",
    "    return selected_features\n",
    "\n",
    "rf_feats = boruta_fs(df, 'rf')\n",
    "gbt_feats = boruta_fs(df, 'gbt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f1dd31-27ce-42b8-ac91-41d2ebf9876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_features = get_feature_col_names()\n",
    "# for f in boruta_features:\n",
    "#     if f in current_features:\n",
    "#         print(f)\n",
    "# final_features = list(set(boruta_features + current_features))\n",
    "# write_features_file(final_features) #in the feature selection pipeline, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c21d5-a37f-4211-a2dc-a32a14a41cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def correlation_matrix(df):\n",
    "    df =df.toPandas()\n",
    "    print(\"Converted to Pandas\")\n",
    "    corr_df = df.drop(['date','fsym_id'], axis=1)\n",
    "    corr_mat = corr_df.corr()\n",
    "    mask = np.triu(np.ones_like(corr_mat))\n",
    "    plt.figure(figsize=(50, 40))\n",
    "    sns.heatmap(corr_mat, annot=True, cmap='coolwarm', fmt=\".2f\", mask=mask)\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('corr_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Variable pairs with absolute correlation above 0.7:\")\n",
    "    for i in range(len(corr_mat.columns)):\n",
    "        for j in range(i+1, len(corr_mat.columns)):\n",
    "            if abs(corr_mat.iloc[i, j]) >= 0.7:\n",
    "                print(f\"{corr_mat.columns[i]} - {corr_mat.columns[j]}: {corr_mat.iloc[i, j]}\")\n",
    "                \n",
    "# correlation_matrix(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ea6a0-81ff-4b2b-9dd0-f6fff61d7fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('ff_div_yld_secs', 'ff_earn_yld', 'ff_roa_ptx', 'ff_net_inc_basic_aft_xord', 'ff_net_inc_dil', 'ff_oper_inc_aft_unusual', \n",
    "                        'ff_net_inc_dil_aft_xord', 'ff_net_inc_dil_bef_unusual', 'ff_ebit_bef_unusual', 'ff_eps_dil_gr', 'GDP', 'ff_bk_oper_inc_tot')\n",
    "feats = df.columns[2:-1]\n",
    "# write_features_file(feats)\n",
    "feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb6e21-44f3-4b26-b63b-a45b4ba8056a",
   "metadata": {},
   "source": [
    "### Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385f503-98bc-4b81-938c-27e8225f53b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_dates(imp_df_price):\n",
    "    price_data = get_fund_data(spark.createDataFrame(imp_df_price))\n",
    "    #cols = get_not_null_cols(imp_df_price, 'FF_ADVANCED_DER_AF')\n",
    "    #result_string = ', '.join('a.' + item for item in cols)\n",
    "    \n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy(col('p_date'))\n",
    "\n",
    "    price_data = price_data.withColumn('row_num', F.row_number().over(window_spec))\n",
    "    price_data.show()\n",
    "\n",
    "    price_data = price_data.filter(col('row_num') == 1).orderBy(col('p_date').desc())\n",
    "    price_data.show()\n",
    "    \n",
    "    start_dates = price_data.groupBy('year').count().orderBy('year')\n",
    "    years = [row['year'] for row in start_dates.collect()]\n",
    "    counts = [row['count'] for row in start_dates.collect()]\n",
    "    plt.bar(years, counts)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Start Dates Count per Year')\n",
    "    plt.show()\n",
    "    #start_dates.show(25)\n",
    "    \n",
    "def null_vals(imp_df_price, table):\n",
    "    price_data = get_fund_data(spark.createDataFrame(imp_df_price))\n",
    "    cols = get_not_null_cols(imp_df_price, table)\n",
    "    col_string = ', '.join('a.' + item for item in cols)\n",
    "    price_data.createOrReplaceTempView('temp_table')\n",
    "    null_counts = []\n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, t.split_adj_price, t.Market_Value, t.p_date, {col_string}\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id AND YEAR(t.p_date)=YEAR(a.date)\n",
    "                ORDER BY t.fsym_id, t.p_date\n",
    "            \"\"\"\n",
    "    full_df = spark.sql(query1)\n",
    "    for column in cols:\n",
    "        null_count = full_df.select(column).filter(col(column).isNull()).count()\n",
    "        null_counts.append((column, null_count))\n",
    "    null_counts_df = pd.DataFrame(null_counts, columns=['Column', 'Null Count'])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(null_counts_df['Column'], null_counts_df['Null Count'])\n",
    "    plt.xlabel('Column')\n",
    "    plt.ylabel('Null Count')\n",
    "    plt.title('Null Counts for Each Column')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # null_counts = price_data.groupBy('year').agg(F.sum(col('p_price').isNull().cast('int')).alias('null_count'))\n",
    "    # null_counts.show()\n",
    "    \n",
    "imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "imp_df_price_imploded = imp_df_price.loc[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "start_dates(imp_df_price)\n",
    "start_dates(imp_df_price_imploded)\n",
    "\n",
    "#null_vals(imp_df_price, 'FF_ADVANCED_DER_AF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345755a7-1bf1-442d-9418-576cb9688733",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "imp_df_test = imp_df_price[imp_df_price['fsym_id']=='H7CTYF-R']\n",
    "df = get_fund_data(spark.createDataFrame(imp_df_test))\n",
    "df.show(1000)\n",
    "imp_df_imp = imp_df_price[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "print(len(imp_df_imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730ca87-195a-49a3-87e3-60f22f57b015",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df_imp = imp_df_price[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "print(len(imp_df_imp))\n",
    "print(len(imp_df_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19776f2-22bb-4ab6-a748-cb9928010b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols():\n",
    "    df_metrics = ps.DataFrame(spark.sql(\"SELECT * FROM FF_BASIC_AF LIMIT 10\")) #get all the metrics\n",
    "    cols = []\n",
    "    for c in df_metrics.columns:\n",
    "        if df_metrics[c].dtype=='float64':#get all the metrics we can calculate correlations with\n",
    "            cols.append(c)\n",
    "    return cols\n",
    "\n",
    "#%change average of each feature plotted for pharmacy industry\n",
    "def industry_analysis():\n",
    "    stock_df = get_all_stocks_df()\n",
    "    #stock_df = pd.read_csv('imploded_stocks.csv')\n",
    "    #stock_df = spark.createDataFrame(stock_df)\n",
    "    cols = ['ff_gross_inc', 'ff_sales', 'FF_OPER_EXP_TOT', 'FF_CASH_ST']\n",
    "    col_string = ', '.join('a.' + item for item in cols)\n",
    "    stock_df.createOrReplaceTempView(\"temp_table\")\n",
    "    q = f\"\"\"SELECT e.factset_industry_desc, t.ticker_region, a.date, {col_string} FROM temp_table t\n",
    "    LEFT JOIN FF_BASIC_AF a ON a.fsym_id = t.fsym_id\n",
    "    LEFT JOIN sym_coverage sc ON sc.fsym_id = t.fsym_id\n",
    "    LEFT JOIN ff_sec_entity_hist c on c.fsym_id=sc.fsym_security_id\n",
    "    LEFT JOIN sym_entity_sector d on d.factset_entity_id=c.factset_entity_id\n",
    "    LEFT JOIN factset_industry_map e on e.factset_industry_code=d.industry_code\n",
    "    WHERE a.date >= \"2009-01-01\" AND e.factset_industry_desc=\"Regional Banks\"\n",
    "    ORDER BY t.ticker_region,a.date\"\"\"\n",
    "    ind_df = spark.sql(q)\n",
    "    #print(ind_df.show(10))\n",
    "    ind_df =ind_df.toPandas()\n",
    "    ind_df['date'] = pd.to_datetime(ind_df['date'])\n",
    "    new_cols = []\n",
    "    for column in cols:\n",
    "        ind_df[f'{column}_percentage_change'] = ind_df.groupby('ticker_region')[column].pct_change() * 100\n",
    "        ind_df[f'{column}_percentage_change'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        ind_df.drop(column, axis=1, inplace=True)\n",
    "        new_cols.append(f'{column}_percentage_change')\n",
    "    ind_df['year'] = ind_df['date'].dt.year\n",
    "    avg_pct_change = ind_df.groupby(['year'])[new_cols].mean().reset_index()\n",
    "    print(avg_pct_change.head(20))\n",
    "    num_rows = (len(new_cols) + 1) // 2  # Adjust the number of rows as needed\n",
    "    num_cols = 2\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n",
    "    for i,column in enumerate(new_cols):\n",
    "        row = i//num_cols\n",
    "        col = i % num_cols \n",
    "        axes[row,col].plot(avg_pct_change['year'], avg_pct_change[column])\n",
    "        axes[row, col].set_title(f'Avg {column} Percentage Change Over Time')\n",
    "        axes[row, col].set_xlabel('Year')\n",
    "        axes[row, col].set_ylabel(f'Avg {column} Percentage Change')\n",
    "        axes[row, col].grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#industry_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700de0e-2f78-48aa-8c16-e12e167d67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#YOU'VE DONE WORST CHANGES NOW FIND OUT WHICH ONES DECREASE CONSISTENTLY\n",
    "#ALSO FIGURE OUT MEANS BEFORE PERIOD AND AFTER PERIOD USING QUARTERLY AND COMPARE DIFF\n",
    "#FINALLY WITH A HUGE LIST USE BORUTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b54c9-5ddc-4459-afff-5247cc6b7b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_not_null_cols(df, table='FF_ADVANCED_DER_AF'):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query1 = f\"\"\"SELECT t.fsym_id, a.*\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id\n",
    "                ORDER BY t.fsym_id, a.date\n",
    "            \"\"\"\n",
    "    #we get all the available dates per stock, so these null values are only within the timeframe available\n",
    "    q_df = spark.sql(query1)\n",
    "    column_types = q_df.dtypes\n",
    "    null_pcts = []\n",
    "    for c, dtype in zip(q_df.columns, column_types):\n",
    "        if dtype[1] == 'double':\n",
    "            null_count = q_df.filter(F.col(c).isNull()).count()\n",
    "            null_pcts.append(null_count/q_df.count())\n",
    "\n",
    "\n",
    "    columns_to_drop = [col_name for col_name, null_pct, dtype in zip(q_df.columns, null_pcts, column_types) if null_pct > 0.2 or dtype[1]!='double']\n",
    "\n",
    "    q_df = q_df.drop(*columns_to_drop)\n",
    "\n",
    "    cols = q_df.columns\n",
    "    print(cols)\n",
    "\n",
    "    return cols\n",
    "    \n",
    "df = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "df = df.loc[df['Implosion_Start_Date'].notnull()]\n",
    "get_not_null_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d7b62c-becd-45ee-bcb2-8c8c59e8e0b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4997ed0d-ad2e-4ec5-9b4b-7818b3a857f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
