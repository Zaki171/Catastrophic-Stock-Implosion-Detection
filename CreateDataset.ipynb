{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b862fd7-75a8-4818-95c2-5375dde4b345",
   "metadata": {},
   "source": [
    "## Building the dataset that will be input into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9cedb4-51ae-42f3-8064-60ce28dd207c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"Building dataset\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "spark.sql(\"USE 2023_04_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "63645eaf-dfa5-43d5-ab36-e94c6ced8c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10607\n",
      "Number of rows where every cell (except the first column) has lists with 23 values: 157\n",
      "+--------------------------+---------------------------------+-------------------------+---------------------------------+----------------------+----------------------------------+-------------------------------------+-----------------------+---------------------+-------------------------------------+-------------------+------------------------------------+----------------------------------+-------------------------+\n",
      "|avg_length_ff_non_oper_exp|avg_length_ff_net_inc_bef_xord_gr|avg_length_ff_oper_inc_gr|avg_length_ff_ut_non_oper_inc_oth|avg_length_ff_earn_yld|avg_length_ff_net_inc_dil_aft_xord|avg_length_ff_net_inc_basic_beft_xord|avg_length_ff_assets_gr|avg_length_ff_fcf_yld|avg_length_ff_net_inc_dil_bef_unusual|avg_length_ff_wkcap|avg_length_ff_net_inc_basic_aft_xord|avg_length_ff_oper_inc_aft_unusual|avg_length_ff_net_inc_dil|\n",
      "+--------------------------+---------------------------------+-------------------------+---------------------------------+----------------------+----------------------------------+-------------------------------------+-----------------------+---------------------+-------------------------------------+-------------------+------------------------------------+----------------------------------+-------------------------+\n",
      "|        10.625530310172527|                9.615442632224003|        9.885453002734044|                9.494013387385689|      9.59922692561516|                11.084001131328368|                    11.18421796926558|     10.397944753464692|    9.295748090883379|                   11.084001131328368|    9.7577071745074|                  11.021118129537099|                11.147921184123692|       11.084001131328368|\n",
      "+--------------------------+---------------------------------+-------------------------+---------------------------------+----------------------+----------------------------------+-------------------------------------+-----------------------+---------------------+-------------------------------------+-------------------+------------------------------------+----------------------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import lit,col\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "#from boruta import BorutaPy\n",
    "#from fredapi import Fred\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import csv\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "\n",
    "def get_macro_features():\n",
    "    # fred_key = 'bdfdde3b7a21b7d528011d17996b0b8e'\n",
    "    # fred = Fred(api_key=fred_key)\n",
    "    # cpi = fred.get_series(series_id='CPIAUCSL')\n",
    "    # cpi_change = cpi.pct_change()\n",
    "    # unemp = fred.get_series(series_id='UNRATE')\n",
    "    # gdp = fred.get_series(series_id='GDP')\n",
    "    # gdp_change = gdp.pct_change()\n",
    "    # df = pd.DataFrame({'CPI_change': cpi_change,'Unemployment_Rate': unemp,'GDP_change': gdp_change})\n",
    "    # df.to_csv('macro.csv')\n",
    "    df = pd.read_csv('macro.csv')\n",
    "    return df\n",
    "\n",
    "def get_all_stocks():\n",
    "    query = f\"\"\"SELECT s.ticker_region, sc.fref_listing_exchange FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    ticker_list = [row.ticker_region for row in df.collect()]\n",
    "    return ticker_list\n",
    "\n",
    "\n",
    "\n",
    "def get_non_imp_stocks_query():\n",
    "    df2 = spark.createDataFrame(get_implosion_df('imploded_stocks.csv'))\n",
    "    df2.createOrReplaceTempView(\"imp_table\")\n",
    "    query = f\"\"\"SELECT s.ticker_region, s.fsym_id FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\n",
    "                AND NOT EXISTS (\n",
    "                SELECT 1\n",
    "                FROM imp_table\n",
    "                WHERE s.ticker_region = CONCAT(imp_table.Ticker, '-US') )    \n",
    "                \"\"\"\n",
    "    df = spark.sql(query)\n",
    "    print(\"got non imploded stocks\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_implosion_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df = df[df['Implosion_Start_Date'].notnull()]\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Start_Date'])\n",
    "    return df\n",
    "\n",
    "def get_non_implosion_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df = df[df['Implosion_Start_Date'].isnull()]\n",
    "    return df\n",
    "\n",
    "def get_features_for_imploded_stocks(df, big_string, table):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    # query = \"\"\"SELECT t.Ticker, t.Implosion_Date, t.Implosion_Next_Year, a.date, a.ff_gross_inc, b.date, b.ff_gross_inc, c.date, c.ff_gross_inc\n",
    "    #             FROM temp_table t \n",
    "    #             LEFT JOIN sym_ticker_region s ON s.ticker_region = CONCAT(t.Ticker, '-US')\n",
    "    #             LEFT JOIN FF_BASIC_AF a ON s.fsym_id = a.fsym_id AND YEAR(a.date) = YEAR(t.Implosion_Date)-1\n",
    "    #             LEFT JOIN FF_BASIC_AF b ON s.fsym_id = b.fsym_id AND YEAR(b.date) = YEAR(t.Implosion_Date)-2\n",
    "    #             LEFT JOIN FF_BASIC_AF c ON s.fsym_id = c.fsym_id AND YEAR(c.date) = YEAR(t.Implosion_Date)-3\n",
    "    #             ORDER BY t.Ticker, a.date\n",
    "    # \"\"\"\n",
    "    query = f\"\"\"SELECT t.ticker_region, a.date, {big_string}, t.Implosion_Next_Year FROM temp_table t\n",
    "                    LEFT JOIN sym_ticker_region s ON s.ticker_region = t.ticker_region\n",
    "                    LEFT JOIN {table} a ON a.fsym_id = s.fsym_id AND t.Year = YEAR(a.date)\n",
    "                    LEFT JOIN FF_BASIC_AF b ON b.fsym_id = s.fsym_id AND YEAR(b.date) = t.Year\n",
    "                    ORDER BY t.ticker_region, a.date\n",
    "    \"\"\"\n",
    "    df2 = spark.sql(query)\n",
    "    print(\"imploded query done\")\n",
    "    return df2\n",
    "    \n",
    "    \n",
    "def get_features_for_non_imploded(metric_string, metric_string2,table):\n",
    "    df = spark.createDataFrame(get_non_implosion_df('imploded_stocks3.csv'))\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query = f\"\"\"WITH RankedData AS (\n",
    "    SELECT\n",
    "        t.ticker_region, s.fsym_id,\n",
    "        a.date,\n",
    "        {metric_string},\n",
    "        ROW_NUMBER() OVER (PARTITION BY t.ticker_region ORDER BY a.date DESC) AS row_num\n",
    "        FROM temp_table t\n",
    "        LEFT JOIN sym_ticker_region s ON s.ticker_region = t.ticker_region\n",
    "        LEFT JOIN {table} a ON a.fsym_id = s.fsym_id \n",
    "        WHERE a.date < (\n",
    "            SELECT MAX(date)\n",
    "            FROM {table} a_sub\n",
    "            WHERE a_sub.fsym_id = s.fsym_id ))\n",
    "    SELECT\n",
    "        r.ticker_region, r.date, {metric_string2}\n",
    "        FROM RankedData r\n",
    "        WHERE row_num <= 4\n",
    "        ORDER BY ticker_region, date\"\"\"\n",
    "    new_df = spark.sql(query)\n",
    "    print(\"non imploded query done\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def create_non_imploded_ds(table):\n",
    "    #df_metrics = ps.DataFrame(spark.sql(f\"SELECT * FROM {table} LIMIT 10\")) #get all the metrics\n",
    "    # cols = []\n",
    "    # for c in df_metrics.columns:\n",
    "    #     if df_metrics[c].dtype=='float64':#get all the metrics we can calculate correlations with\n",
    "    #         cols.append(c)\n",
    "    cols = ['ff_debt_entrpr_val', 'ff_tot_debt_tcap_std', 'ff_fix_assets_com_eq', 'ff_debt_eq', 'ff_inven_curr_assets', 'ff_liabs_lease', 'ff_ltd_tcap', 'ff_sales_wkcap',\n",
    "           'ff_bps_gr', 'ff_oper_inc_tcap', 'ff_assets_gr', 'ff_fcf_yld', 'ff_mkt_val_gr', 'ff_earn_yld', 'ff_pbk_tang', 'ff_zscore', 'ff_entrpr_val_sales', 'ff_psales_dil', 'ff_roea', 'ff_dps_gr',\n",
    "           'ff_loan_loss_pct', 'ff_loan_loss_actual_rsrv'] #advanced_der_qf\n",
    "    \n",
    "    metric_string = ', '.join('a.' + item for item in cols)\n",
    "    metric_string2 = ', '.join('r.' + item for item in cols)\n",
    "    df = get_features_for_non_imploded(metric_string, metric_string2, table)\n",
    "    df = df.withColumn(\"Implosion_Next_Year\", lit(0))\n",
    "    return df\n",
    "\n",
    "def create_imploded_df(table):\n",
    "    df = get_implosion_df('imploded_stocks3.csv')\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    df['Implosion_Year'] = df['Implosion_Date'].dt.year-1\n",
    "    df['Implosion_Next_Year'] = 1\n",
    "    \n",
    "    additional_rows_1 = df.copy()\n",
    "    additional_rows_1['Implosion_Year'] = df['Implosion_Year'] - 1\n",
    "    additional_rows_1['Implosion_Next_Year'] = 0\n",
    "    additional_rows_2 = df.copy()\n",
    "    additional_rows_2['Implosion_Year'] = df['Implosion_Year'] - 2\n",
    "    additional_rows_2['Implosion_Next_Year'] = 0\n",
    "    additional_rows_3 = df.copy()\n",
    "    additional_rows_3['Implosion_Year'] = df['Implosion_Year'] - 3\n",
    "    additional_rows_3['Implosion_Next_Year'] = 0\n",
    "    df = pd.concat([df, additional_rows_1, additional_rows_2, additional_rows_3])\n",
    "    df = df.sort_values(by=['ticker_region', 'Implosion_Year'])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df =df.rename({'Implosion_Year' : 'Year'},axis=1)\n",
    "    \n",
    "    # df_metrics = ps.DataFrame(spark.sql(f\"SELECT * FROM {table} LIMIT 10\")) #get all the metrics\n",
    "    # cols = []\n",
    "    # for c in df_metrics.columns:\n",
    "    #     if df_metrics[c].dtype=='float64':#get all the metrics we can calculate correlations with\n",
    "    #         cols.append(c)\n",
    "    \n",
    "    cols = ['ff_debt_entrpr_val', 'ff_tot_debt_tcap_std', 'ff_fix_assets_com_eq', 'ff_debt_eq', 'ff_inven_curr_assets', 'ff_liabs_lease', 'ff_ltd_tcap', 'ff_sales_wkcap',\n",
    "           'ff_bps_gr', 'ff_oper_inc_tcap', 'ff_assets_gr', 'ff_fcf_yld', 'ff_mkt_val_gr', 'ff_earn_yld', 'ff_pbk_tang', 'ff_zscore', 'ff_entrpr_val_sales', 'ff_psales_dil', 'ff_roea', 'ff_dps_gr',\n",
    "           'ff_loan_loss_pct', 'ff_loan_loss_actual_rsrv']\n",
    "    \n",
    "    metric_string = ', '.join('a.' + item for item in cols)\n",
    "    df = get_features_for_imploded_stocks(df, metric_string, table)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "    \n",
    "def create_dataset(table):\n",
    "    # df = get_implosion_df('imploded_stocks.csv')\n",
    "    # df = df.drop(df.columns[0], axis=1)\n",
    "    # df['Implosion_Year'] = df['Implosion_Date'].dt.year\n",
    "    # df['Implosion_Next_Year'] = 1\n",
    "    # get_features_for_imploded_stocks(df)\n",
    "    #print(df.head())\n",
    "    #df=spark.createDataFrame(df)\n",
    "    #df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    imp_df = create_imploded_df(table).toPandas()\n",
    "    non_imp_df =create_non_imploded_ds(table).toPandas()\n",
    "    result_df = pd.concat([non_imp_df,imp_df], ignore_index=True)\n",
    "    #print(result_df.head())\n",
    "    result_df['date'] = pd.to_datetime(result_df['date'], format='%Y-%m-%d')\n",
    "    result_df=result_df.sort_values(by=['ticker_region','date'])\n",
    "    macro_df = get_macro_features().reset_index()\n",
    "    macro_df['Date'] = pd.to_datetime(macro_df['Date'], format='%d/%m/%Y')\n",
    "    #print(macro_df.head())\n",
    "    result_df['month_year'] = result_df['date'].dt.to_period(\"M\")\n",
    "    macro_df['Month_year'] = macro_df['Date'].dt.to_period(\"M\")\n",
    "    result_df = pd.merge(result_df, macro_df, left_on='month_year', right_on='Month_year', how='left')\n",
    "    result_df.drop(['Date', 'index', 'month_year','Month_year','GDP'],axis=1,inplace=True)\n",
    "    \n",
    "    print(result_df.head())\n",
    "    \n",
    "    null_pcts = result_df.isnull().sum()/len(result_df)\n",
    "    print(null_pcts)\n",
    "    \n",
    "    cols_to_drop = null_pcts[null_pcts > 0.1].index.tolist()\n",
    "    result_df.drop(cols_to_drop,axis=1,inplace=True)\n",
    "    print(\"dropped cols: \", cols_to_drop)\n",
    "    \n",
    "    result_df=pd.DataFrame(result_df)\n",
    "    print(\"before dropping nulls: \",len(result_df))\n",
    "    result_df = result_df.dropna()\n",
    "    print(\"after dropping nulls: \", len(result_df))\n",
    "    print(\"number of implosions: \", len(result_df[result_df['Implosion_Next_Year']==1]))\n",
    "    print(\"number of non-implosions: \", len(result_df[result_df['Implosion_Next_Year']==0]))\n",
    "    result_df.to_csv('Advanced_AF_DER_Dataset.csv', index=False)\n",
    "    print(\"dataset written\")\n",
    "    \n",
    "def get_feature_col_names():\n",
    "    csv_file_path = 'features.csv'\n",
    "    data_list = []\n",
    "    with open(csv_file_path, mode='r') as file:\n",
    "    # Create a CSV reader object\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            data_list.append(row)\n",
    "    col_list = data_list[0]\n",
    "    return col_list\n",
    "    \n",
    "    \n",
    "def get_features_all_stocks():\n",
    "    table = \"FF_ADVANCED_DER_AF\"\n",
    "    df = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    spark_df.createOrReplaceTempView(\"temp_table\")\n",
    "    col_names = get_feature_col_names()\n",
    "    col_string = ', '.join('a.' + item for item in col_names)\n",
    "    q=f\"\"\"SELECT t.fsym_id, a.date, {col_string}\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id\n",
    "                WHERE a.date >= \"2000-01-01\"\n",
    "                ORDER BY t.fsym_id, a.date\"\"\"\n",
    "    features_df = spark.sql(q)\n",
    "    feature_cols = [col for col in features_df.columns if col not in ['fsym_id', 'date']]\n",
    "    sequences = []\n",
    "    \n",
    "\n",
    "    grouped_df = features_df.groupBy(\"fsym_id\").agg(\n",
    "        *[F.collect_list(col).alias(col) for col in feature_cols])\n",
    "    \n",
    "    spark_df = spark_df.withColumn('label', F.when(F.isnan('Implosion_Start_Date'), 0).otherwise(1))\n",
    "    joined_df = grouped_df.join(spark_df.select(\"fsym_id\", \"label\"), \"fsym_id\", \"inner\")\n",
    "    joined_df=joined_df.orderBy('fsym_id')\n",
    "    return joined_df\n",
    "\n",
    "def train_test(df):\n",
    "    #df=df.drop('fsym_id')\n",
    "    # df.createOrReplaceTempView(\"temp_table\")\n",
    "\n",
    "    # Count cells with length 23\n",
    "    # for c in df.columns:\n",
    "    #     if c!='fsym_id':\n",
    "    #         grouped_lengths = spark.sql(f\"\"\"\n",
    "    #             SELECT size({c}) AS array_length, COUNT(*) AS count_length\n",
    "    #             FROM temp_table\n",
    "    #             GROUP BY size({c})\n",
    "    #             ORDER BY COUNT(*) DESC\n",
    "    #             LIMIT 5\n",
    "    #         \"\"\")\n",
    "    #         grouped_lengths.show()\n",
    "\n",
    "\n",
    "    filtered_df = df.filter(\n",
    "    reduce(lambda acc, column: acc & (F.size(col(column)) == 23), df.columns[1:-1], lit(True))\n",
    "    )\n",
    "    count_rows_with_23_values = filtered_df.count()\n",
    "\n",
    "    # Count the number of rows in the filtered DataFrame\n",
    "    count_rows_with_23_values = filtered_df.count()\n",
    "\n",
    "    print(f\"Number of rows where every cell (except the first column) has lists with 23 values: {count_rows_with_23_values}\")\n",
    "    \n",
    "    average_lengths = df.agg(*[(F.avg(F.size(col(column))).alias(f'avg_length_{column}')) for column in df.columns[1:-1]])\n",
    "\n",
    "    # Show the results\n",
    "    average_lengths.show()\n",
    "    #need to decide whether to only include stocks that started from 2000, or include just from e.g. 2019\n",
    "    #temporary measure - replace with 0\n",
    "    #look into masking\n",
    "    \n",
    "    \n",
    "#create_dataset('FF_ADVANCED_DER_AF')\n",
    "df = get_features_all_stocks()\n",
    "print(df.count())\n",
    "train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88d7337e-6ca2-4212-b9d0-17953b8998d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(p_date)|\n",
      "+-----------+\n",
      "| 2020-07-14|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT MIN(a.p_date) FROM fp_basic_prices a LEFT JOIN sym_ticker_region s ON s.fsym_id = a.fsym_id WHERE s.ticker_region = 'AACQU-US' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f034c3-4ed9-400e-a07b-6d2c6be018d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
