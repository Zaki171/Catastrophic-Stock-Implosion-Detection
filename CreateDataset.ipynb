{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9cedb4-51ae-42f3-8064-60ce28dd207c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"Building dataset\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "spark.sql(\"USE 2023_04_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63645eaf-dfa5-43d5-ab36-e94c6ced8c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 CPI  Unemployment Rate       GDP\n",
      "2023-06-01  0.001804                3.6       NaN\n",
      "2023-07-01  0.001669                3.5  0.020712\n",
      "2023-08-01  0.006312                3.8       NaN\n",
      "2023-09-01  0.003957                3.8       NaN\n",
      "2023-10-01  0.000449                3.9       NaN\n"
     ]
    }
   ],
   "source": [
    "# import pyspark.pandas as ps\n",
    "# from pyspark.sql.functions import lit\n",
    "import pandas as pd\n",
    "from fredapi import Fred\n",
    "\n",
    "def get_macro_features():\n",
    "    fred_key = 'bdfdde3b7a21b7d528011d17996b0b8e'\n",
    "    fred = Fred(api_key=fred_key)\n",
    "    cpi = fred.get_series(series_id='CPIAUCSL')\n",
    "    cpi_change = cpi.pct_change()\n",
    "    unemp = fred.get_series(series_id='UNRATE')\n",
    "    gdp = fred.get_series(series_id='GDP')\n",
    "    gdp_change = gdp.pct_change()\n",
    "    df = pd.DataFrame({'CPI_change': cpi_change,'Unemployment_Rate': unemp,'GDP_change': gdp_change})\n",
    "    df.to_csv('macro.csv')\n",
    "\n",
    "def get_all_stocks():\n",
    "    query = f\"\"\"SELECT s.ticker_region, sc.fref_listing_exchange FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    ticker_list = [row.ticker_region for row in df.collect()]\n",
    "    return ticker_list\n",
    "\n",
    "def get_stocks_query():\n",
    "    query = f\"\"\"SELECT s.ticker_region, s.fsym_id FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    print(\"stocks obtained\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_implosion_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Date'])\n",
    "    return df\n",
    "\n",
    "def get_features_for_imploded_stocks(df, big_string):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    # query = \"\"\"SELECT t.Ticker, t.Implosion_Date, t.Implosion_Next_Year, a.date, a.ff_gross_inc, b.date, b.ff_gross_inc, c.date, c.ff_gross_inc\n",
    "    #             FROM temp_table t \n",
    "    #             LEFT JOIN sym_ticker_region s ON s.ticker_region = CONCAT(t.Ticker, '-US')\n",
    "    #             LEFT JOIN FF_BASIC_AF a ON s.fsym_id = a.fsym_id AND YEAR(a.date) = YEAR(t.Implosion_Date)-1\n",
    "    #             LEFT JOIN FF_BASIC_AF b ON s.fsym_id = b.fsym_id AND YEAR(b.date) = YEAR(t.Implosion_Date)-2\n",
    "    #             LEFT JOIN FF_BASIC_AF c ON s.fsym_id = c.fsym_id AND YEAR(c.date) = YEAR(t.Implosion_Date)-3\n",
    "    #             ORDER BY t.Ticker, a.date\n",
    "    # \"\"\"\n",
    "    query = f\"\"\"SELECT t.Ticker, a.date, {big_string}, t.Implosion_Next_Year FROM temp_table t\n",
    "                    LEFT JOIN sym_ticker_region s ON s.ticker_region = CONCAT(t.Ticker, '-US')\n",
    "                    LEFT JOIN FF_ADVANCED_AF a ON a.fsym_id = s.fsym_id AND YEAR(a.date) = t.Year\n",
    "                    ORDER BY t.Ticker, a.date\n",
    "    \"\"\"\n",
    "    df2 = spark.sql(query)\n",
    "    return df2\n",
    "    \n",
    "    \n",
    "def get_features_for_non_imploded(metric_string, metric_string2):\n",
    "    df = get_stocks_query()\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query = f\"\"\"WITH RankedData AS (\n",
    "    SELECT\n",
    "        t.ticker_region,\n",
    "        a.date,\n",
    "        {metric_string},\n",
    "        ROW_NUMBER() OVER (PARTITION BY t.ticker_region ORDER BY a.date DESC) AS row_num\n",
    "    FROM\n",
    "        temp_table t\n",
    "        LEFT JOIN FF_ADVANCED_AF a ON a.fsym_id = t.fsym_id\n",
    ")\n",
    "SELECT\n",
    "    ticker_region AS Ticker,\n",
    "    date,\n",
    "    {metric_string2}\n",
    "FROM\n",
    "    RankedData r\n",
    "WHERE\n",
    "    row_num <= 4\n",
    "ORDER BY\n",
    "    ticker_region,\n",
    "    date\"\"\"\n",
    "    new_df = spark.sql(query)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def create_non_imploded_ds():\n",
    "    df_metrics = ps.DataFrame(spark.sql(\"SELECT * FROM FF_ADVANCED_AF LIMIT 10\")) #get all the metrics\n",
    "    cols = []\n",
    "    for c in df_metrics.columns:\n",
    "        if df_metrics[c].dtype=='float64':#get all the metrics we can calculate correlations with\n",
    "            cols.append(c)\n",
    "    metric_string = ', '.join('a.' + item for item in cols)\n",
    "    metric_string2 = ', '.join('r.' + item for item in cols)\n",
    "    df = get_features_for_non_imploded(metric_string, metric_string2)\n",
    "    df = df.withColumn(\"Implosion_Next_Year\", lit(0))\n",
    "    return df\n",
    "\n",
    "def create_imploded_df():\n",
    "    df = get_implosion_df('imploded_stocks.csv')\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    df['Implosion_Year'] = df['Implosion_Date'].dt.year\n",
    "    df['Implosion_Next_Year'] = 1\n",
    "    additional_rows_1 = df.copy()\n",
    "    additional_rows_1['Implosion_Year'] = df['Implosion_Year'] - 1\n",
    "    additional_rows_1['Implosion_Next_Year'] = 0\n",
    "    additional_rows_2 = df.copy()\n",
    "    additional_rows_2['Implosion_Year'] = df['Implosion_Year'] - 2\n",
    "    additional_rows_2['Implosion_Next_Year'] = 0\n",
    "    additional_rows_3 = df.copy()\n",
    "    additional_rows_3['Implosion_Year'] = df['Implosion_Year'] - 3\n",
    "    additional_rows_3['Implosion_Next_Year'] = 0\n",
    "    df = pd.concat([df, additional_rows_1, additional_rows_2, additional_rows_3])\n",
    "    df = df.sort_values(by=['Ticker', 'Implosion_Year'])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df =df.rename({'Implosion_Year' : 'Year'},axis=1)\n",
    "    \n",
    "    df_metrics = ps.DataFrame(spark.sql(\"SELECT * FROM FF_ADVANCED_AF LIMIT 10\")) #get all the metrics\n",
    "    cols = []\n",
    "    for c in df_metrics.columns:\n",
    "        if df_metrics[c].dtype=='float64':#get all the metrics we can calculate correlations with\n",
    "            cols.append(c)\n",
    "    metric_string = ', '.join('a.' + item for item in cols)\n",
    "    \n",
    "    df = get_features_for_imploded_stocks(df, metric_string)\n",
    "    \n",
    "    return df\n",
    "    \n",
    "    \n",
    "\n",
    "def create_dataset():\n",
    "    # df = get_implosion_df('imploded_stocks.csv')\n",
    "    # df = df.drop(df.columns[0], axis=1)\n",
    "    # df['Implosion_Year'] = df['Implosion_Date'].dt.year\n",
    "    # df['Implosion_Next_Year'] = 1\n",
    "    # get_features_for_imploded_stocks(df)\n",
    "    #print(df.head())\n",
    "    #df=spark.createDataFrame(df)\n",
    "    #df.createOrReplaceTempView(\"temp_table\")\n",
    "    non_imp_df =create_non_imploded_ds().toPandas()\n",
    "    imp_df = create_imploded_df().toPandas()\n",
    "    result_df = pd.concat([non_imp_df,imp_df], ignore_index=True)\n",
    "    result_df=result_df.sort_values(by=['Ticker','date'])\n",
    "    print(result_df.isnull().sum())\n",
    "    # result_df=pd.DataFrame(result_df)\n",
    "    #result_df.to_csv('Advanced_AF_Dataset.csv', index=False)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "#create_dataset()\n",
    "get_macro_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abb8ef9-7771-4c77-942f-f2068b83f19e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
