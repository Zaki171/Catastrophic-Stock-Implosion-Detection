{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc9cedb4-51ae-42f3-8064-60ce28dd207c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"Building dataset\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "spark.sql(\"USE 2023_04_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63645eaf-dfa5-43d5-ab36-e94c6ced8c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stocks obtained\n",
      "   ticker_region        date  ff_accr_exp_cf  ff_actg_chg_ps  ff_amort_cf  Implosion_Next_Year\n",
      "0           A-US  2019-10-31            23.0             0.0   127.000000                    1\n",
      "1           A-US  2020-10-31            29.0             0.0   189.000000                    1\n",
      "2           A-US  2021-10-31           112.0             0.0   199.000000                    1\n",
      "3           A-US  2022-10-31           -22.0             0.0   197.000000                    1\n",
      "4          AA-US  2019-12-31          -175.0             0.0    19.000000                    1\n",
      "5          AA-US  2020-12-31          -153.0             0.0     9.000000                    1\n",
      "6          AA-US  2021-12-31           -38.0             0.0    11.000000                    1\n",
      "7          AA-US  2022-12-31          -173.0             0.0     7.000000                    1\n",
      "8        AAAP-US  2013-12-31             NaN             0.0     3.987880                    1\n",
      "9        AAAP-US  2014-12-31             NaN             0.0     4.167538                    1\n",
      "10       AAAP-US  2015-12-31             NaN             0.0     3.652011                    1\n",
      "11       AAAP-US  2016-12-31             NaN             0.0     5.440482                    1\n",
      "12        AAB-US  1995-12-31             NaN             NaN          NaN                    1\n",
      "13        AAB-US  1996-12-31             NaN             NaN          NaN                    1\n",
      "14        AAB-US  1997-12-31             NaN             NaN          NaN                    1\n",
      "15        AAB-US  1998-12-31             NaN             NaN          NaN                    1\n",
      "16       AABC-US  2001-12-31             NaN             NaN          NaN                    1\n",
      "17       AABC-US  2002-12-31             NaN             NaN          NaN                    1\n",
      "18       AABC-US  2003-12-31             NaN             NaN          NaN                    1\n",
      "19       AABC-US  2004-12-31             NaN             NaN          NaN                    1\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "def get_all_stocks():\n",
    "    query = f\"\"\"SELECT s.ticker_region, sc.fref_listing_exchange FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    ticker_list = [row.ticker_region for row in df.collect()]\n",
    "    return ticker_list\n",
    "\n",
    "def get_stocks_query():\n",
    "    query = f\"\"\"SELECT s.ticker_region, s.fsym_id FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    print(\"stocks obtained\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_implosion_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Date'])\n",
    "    return df\n",
    "\n",
    "def get_features_for_imploded_stocks(df):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query = \"\"\"SELECT t.Ticker, t.Implosion_Date, t.Implosion_Next_Year, a.date, a.ff_gross_inc, b.date, b.ff_gross_inc, c.date, c.ff_gross_inc\n",
    "                FROM temp_table t \n",
    "                LEFT JOIN sym_ticker_region s ON s.ticker_region = CONCAT(t.Ticker, '-US')\n",
    "                LEFT JOIN FF_BASIC_AF a ON s.fsym_id = a.fsym_id AND YEAR(a.date) = YEAR(t.Implosion_Date)-1\n",
    "                LEFT JOIN FF_BASIC_AF b ON s.fsym_id = b.fsym_id AND YEAR(b.date) = YEAR(t.Implosion_Date)-2\n",
    "                LEFT JOIN FF_BASIC_AF c ON s.fsym_id = c.fsym_id AND YEAR(c.date) = YEAR(t.Implosion_Date)-3\n",
    "                ORDER BY t.Ticker, a.date\n",
    "    \"\"\"\n",
    "    df = spark.sql(query)\n",
    "    print(df.show(10))\n",
    "    \n",
    "    \n",
    "def get_features_for_non_imploded(metric_string, metric_string2):\n",
    "    df = get_stocks_query()\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query = f\"\"\"WITH RankedData AS (\n",
    "    SELECT\n",
    "        t.ticker_region,\n",
    "        a.date,\n",
    "        {metric_string},\n",
    "        ROW_NUMBER() OVER (PARTITION BY t.ticker_region ORDER BY a.date DESC) AS row_num\n",
    "    FROM\n",
    "        temp_table t\n",
    "        LEFT JOIN FF_ADVANCED_AF a ON a.fsym_id = t.fsym_id\n",
    ")\n",
    "SELECT\n",
    "    ticker_region,\n",
    "    date,\n",
    "    {metric_string2}\n",
    "FROM\n",
    "    RankedData r\n",
    "WHERE\n",
    "    row_num <= 4\n",
    "ORDER BY\n",
    "    ticker_region,\n",
    "    date\"\"\"\n",
    "    new_df = spark.sql(query)\n",
    "    return new_df\n",
    "\n",
    "def create_non_imploded_ds():\n",
    "    df_metrics = ps.DataFrame(spark.sql(\"SELECT * FROM FF_ADVANCED_AF LIMIT 10\")) #get all the metrics\n",
    "    cols = []\n",
    "    for c in df_metrics.columns:\n",
    "        if df_metrics[c].dtype=='float64':#get all the metrics we can calculate correlations with\n",
    "            cols.append(c)\n",
    "    metric_string = ', '.join('a.' + item for item in cols[:3])\n",
    "    metric_string2 = ', '.join('r.' + item for item in cols[:3])\n",
    "    df = ps.DataFrame(get_features_for_non_imploded(metric_string, metric_string2))\n",
    "    df['Implosion_Next_Year'] = 0\n",
    "    print(df.head(20))\n",
    "    \n",
    "\n",
    "def create_dataset():\n",
    "    # df = get_implosion_df('imploded_stocks.csv')\n",
    "    # df = df.drop(df.columns[0], axis=1)\n",
    "    # df['Implosion_Year'] = df['Implosion_Date'].dt.year\n",
    "    # df['Implosion_Next_Year'] = 1\n",
    "    # get_features_for_imploded_stocks(df)\n",
    "    #print(df.head())\n",
    "    #df=spark.createDataFrame(df)\n",
    "    #df.createOrReplaceTempView(\"temp_table\")\n",
    "    create_non_imploded_ds()\n",
    "\n",
    "    \n",
    "create_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abb8ef9-7771-4c77-942f-f2068b83f19e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
