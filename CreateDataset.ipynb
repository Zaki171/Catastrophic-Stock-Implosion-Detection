{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b862fd7-75a8-4818-95c2-5375dde4b345",
   "metadata": {},
   "source": [
    "## Building the dataset that will be input into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc9cedb4-51ae-42f3-8064-60ce28dd207c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"Building dataset\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "spark.sql(\"USE 2023_04_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63645eaf-dfa5-43d5-ab36-e94c6ced8c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imploded query done\n",
      "non imploded query done\n",
      "  ticker_region       date  ff_debt_entrpr_val  ff_tot_debt_tcap_std  \\\n",
      "0          A-US 2018-10-31            0.087276             28.259504   \n",
      "1          A-US 2019-10-31            0.095845             33.640811   \n",
      "2          A-US 2020-10-31            0.077000             34.237517   \n",
      "3          A-US 2021-10-31            0.058589             35.072289   \n",
      "4         AA-US 2018-12-31            0.235814             25.059102   \n",
      "\n",
      "   ff_fix_assets_com_eq  ff_debt_eq  ff_inven_curr_assets  ff_liabs_lease  \\\n",
      "0             17.998686   39.391285             16.580042             0.0   \n",
      "1             17.902275   50.695029             21.291941             5.0   \n",
      "2             20.931664   52.062385             21.083455           127.0   \n",
      "3             20.838746   54.017443             21.847855           130.0   \n",
      "4            154.518464   33.438486             39.767779             0.0   \n",
      "\n",
      "   ff_ltd_tcap  ff_sales_wkcap  ...  ff_zscore  ff_entrpr_val_sales  \\\n",
      "0    28.259504        1.835637  ...    4.36936             4.194700   \n",
      "1    25.031447        4.655546  ...    3.99615             4.864130   \n",
      "2    32.537112        2.740760  ...    5.08127             6.171210   \n",
      "3    34.445783        3.021999  ...    6.66287             7.862860   \n",
      "4    25.045195       11.125103  ...    1.80781             0.565334   \n",
      "\n",
      "   ff_psales_dil  ff_roea  ff_dps_gr  ff_loan_loss_pct  \\\n",
      "0       4.285050      NaN  12.878788               NaN   \n",
      "1       4.665600      NaN  10.067114               NaN   \n",
      "2       5.965930      NaN   9.756098               NaN   \n",
      "3       7.651440      NaN   7.777778               NaN   \n",
      "4       0.371652      NaN        NaN               NaN   \n",
      "\n",
      "   ff_loan_loss_actual_rsrv  Implosion_Next_Year       CPI  Unemployment Rate  \n",
      "0                       NaN                    0  0.002340                3.8  \n",
      "1                       NaN                    0  0.002858                3.6  \n",
      "2                       NaN                    0  0.000988                6.9  \n",
      "3                       NaN                    0  0.009101                4.5  \n",
      "4                       NaN                    0  0.000685                3.9  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "ticker_region               0.000000\n",
      "date                        0.013434\n",
      "ff_debt_entrpr_val          0.182725\n",
      "ff_tot_debt_tcap_std        0.043467\n",
      "ff_fix_assets_com_eq        0.116700\n",
      "ff_debt_eq                  0.086854\n",
      "ff_inven_curr_assets        0.269898\n",
      "ff_liabs_lease              0.038572\n",
      "ff_ltd_tcap                 0.045621\n",
      "ff_sales_wkcap              0.384869\n",
      "ff_bps_gr                   0.151974\n",
      "ff_oper_inc_tcap            0.046207\n",
      "ff_assets_gr                0.105368\n",
      "ff_fcf_yld                  0.215817\n",
      "ff_mkt_val_gr               0.270111\n",
      "ff_earn_yld                 0.184640\n",
      "ff_pbk_tang                 0.327729\n",
      "ff_zscore                   0.400351\n",
      "ff_entrpr_val_sales         0.236380\n",
      "ff_psales_dil               0.240078\n",
      "ff_roea                     0.891280\n",
      "ff_dps_gr                   0.654873\n",
      "ff_loan_loss_pct            0.906283\n",
      "ff_loan_loss_actual_rsrv    0.906975\n",
      "Implosion_Next_Year         0.000000\n",
      "CPI                         0.013434\n",
      "Unemployment Rate           0.013434\n",
      "dtype: float64\n",
      "dropped cols:  ['ff_debt_entrpr_val', 'ff_fix_assets_com_eq', 'ff_inven_curr_assets', 'ff_sales_wkcap', 'ff_bps_gr', 'ff_assets_gr', 'ff_fcf_yld', 'ff_mkt_val_gr', 'ff_earn_yld', 'ff_pbk_tang', 'ff_zscore', 'ff_entrpr_val_sales', 'ff_psales_dil', 'ff_roea', 'ff_dps_gr', 'ff_loan_loss_pct', 'ff_loan_loss_actual_rsrv']\n",
      "before dropping nulls:  37592\n",
      "after dropping nulls:  33642\n",
      "number of implosions:  840\n",
      "number of non-implosions:  32802\n",
      "dataset written\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import lit,col\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "#from boruta import BorutaPy\n",
    "#from fredapi import Fred\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def get_macro_features():\n",
    "    # fred_key = 'bdfdde3b7a21b7d528011d17996b0b8e'\n",
    "    # fred = Fred(api_key=fred_key)\n",
    "    # cpi = fred.get_series(series_id='CPIAUCSL')\n",
    "    # cpi_change = cpi.pct_change()\n",
    "    # unemp = fred.get_series(series_id='UNRATE')\n",
    "    # gdp = fred.get_series(series_id='GDP')\n",
    "    # gdp_change = gdp.pct_change()\n",
    "    # df = pd.DataFrame({'CPI_change': cpi_change,'Unemployment_Rate': unemp,'GDP_change': gdp_change})\n",
    "    # df.to_csv('macro.csv')\n",
    "    df = pd.read_csv('macro.csv')\n",
    "    return df\n",
    "\n",
    "def get_all_stocks():\n",
    "    query = f\"\"\"SELECT s.ticker_region, sc.fref_listing_exchange FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    ticker_list = [row.ticker_region for row in df.collect()]\n",
    "    return ticker_list\n",
    "\n",
    "\n",
    "\n",
    "def get_non_imp_stocks_query():\n",
    "    df2 = spark.createDataFrame(get_implosion_df('imploded_stocks.csv'))\n",
    "    df2.createOrReplaceTempView(\"imp_table\")\n",
    "    query = f\"\"\"SELECT s.ticker_region, s.fsym_id FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\n",
    "                AND NOT EXISTS (\n",
    "                SELECT 1\n",
    "                FROM imp_table\n",
    "                WHERE s.ticker_region = CONCAT(imp_table.Ticker, '-US') )    \n",
    "                \"\"\"\n",
    "    df = spark.sql(query)\n",
    "    print(\"got non imploded stocks\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_implosion_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df = df[df['Implosion_Start_Date'].notnull()]\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Start_Date'])\n",
    "    return df\n",
    "\n",
    "def get_non_implosion_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df = df[df['Implosion_Start_Date'].isnull()]\n",
    "    return df\n",
    "\n",
    "def get_features_for_imploded_stocks(df, big_string, table):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    # query = \"\"\"SELECT t.Ticker, t.Implosion_Date, t.Implosion_Next_Year, a.date, a.ff_gross_inc, b.date, b.ff_gross_inc, c.date, c.ff_gross_inc\n",
    "    #             FROM temp_table t \n",
    "    #             LEFT JOIN sym_ticker_region s ON s.ticker_region = CONCAT(t.Ticker, '-US')\n",
    "    #             LEFT JOIN FF_BASIC_AF a ON s.fsym_id = a.fsym_id AND YEAR(a.date) = YEAR(t.Implosion_Date)-1\n",
    "    #             LEFT JOIN FF_BASIC_AF b ON s.fsym_id = b.fsym_id AND YEAR(b.date) = YEAR(t.Implosion_Date)-2\n",
    "    #             LEFT JOIN FF_BASIC_AF c ON s.fsym_id = c.fsym_id AND YEAR(c.date) = YEAR(t.Implosion_Date)-3\n",
    "    #             ORDER BY t.Ticker, a.date\n",
    "    # \"\"\"\n",
    "    query = f\"\"\"SELECT t.ticker_region, a.date, {big_string}, t.Implosion_Next_Year FROM temp_table t\n",
    "                    LEFT JOIN sym_ticker_region s ON s.ticker_region = t.ticker_region\n",
    "                    LEFT JOIN {table} a ON a.fsym_id = s.fsym_id AND t.Year = YEAR(a.date)\n",
    "                    LEFT JOIN FF_BASIC_AF b ON b.fsym_id = s.fsym_id AND YEAR(b.date) = t.Year\n",
    "                    ORDER BY t.ticker_region, a.date\n",
    "    \"\"\"\n",
    "    df2 = spark.sql(query)\n",
    "    print(\"imploded query done\")\n",
    "    return df2\n",
    "    \n",
    "    \n",
    "def get_features_for_non_imploded(metric_string, metric_string2,table):\n",
    "    df = spark.createDataFrame(get_non_implosion_df('imploded_stocks3.csv'))\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query = f\"\"\"WITH RankedData AS (\n",
    "    SELECT\n",
    "        t.ticker_region, s.fsym_id,\n",
    "        a.date,\n",
    "        {metric_string},\n",
    "        ROW_NUMBER() OVER (PARTITION BY t.ticker_region ORDER BY a.date DESC) AS row_num\n",
    "        FROM temp_table t\n",
    "        LEFT JOIN sym_ticker_region s ON s.ticker_region = t.ticker_region\n",
    "        LEFT JOIN {table} a ON a.fsym_id = s.fsym_id \n",
    "        WHERE a.date < (\n",
    "            SELECT MAX(date)\n",
    "            FROM {table} a_sub\n",
    "            WHERE a_sub.fsym_id = s.fsym_id ))\n",
    "    SELECT\n",
    "        r.ticker_region, r.date, {metric_string2}\n",
    "        FROM RankedData r\n",
    "        WHERE row_num <= 4\n",
    "        ORDER BY ticker_region, date\"\"\"\n",
    "    new_df = spark.sql(query)\n",
    "    print(\"non imploded query done\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def create_non_imploded_ds(table):\n",
    "    #df_metrics = ps.DataFrame(spark.sql(f\"SELECT * FROM {table} LIMIT 10\")) #get all the metrics\n",
    "    # cols = []\n",
    "    # for c in df_metrics.columns:\n",
    "    #     if df_metrics[c].dtype=='float64':#get all the metrics we can calculate correlations with\n",
    "    #         cols.append(c)\n",
    "    cols = ['ff_debt_entrpr_val', 'ff_tot_debt_tcap_std', 'ff_fix_assets_com_eq', 'ff_debt_eq', 'ff_inven_curr_assets', 'ff_liabs_lease', 'ff_ltd_tcap', 'ff_sales_wkcap',\n",
    "           'ff_bps_gr', 'ff_oper_inc_tcap', 'ff_assets_gr', 'ff_fcf_yld', 'ff_mkt_val_gr', 'ff_earn_yld', 'ff_pbk_tang', 'ff_zscore', 'ff_entrpr_val_sales', 'ff_psales_dil', 'ff_roea', 'ff_dps_gr',\n",
    "           'ff_loan_loss_pct', 'ff_loan_loss_actual_rsrv'] #advanced_der_qf\n",
    "    \n",
    "    metric_string = ', '.join('a.' + item for item in cols)\n",
    "    metric_string2 = ', '.join('r.' + item for item in cols)\n",
    "    df = get_features_for_non_imploded(metric_string, metric_string2, table)\n",
    "    df = df.withColumn(\"Implosion_Next_Year\", lit(0))\n",
    "    return df\n",
    "\n",
    "def create_imploded_df(table):\n",
    "    df = get_implosion_df('imploded_stocks3.csv')\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    df['Implosion_Year'] = df['Implosion_Date'].dt.year-1\n",
    "    df['Implosion_Next_Year'] = 1\n",
    "    \n",
    "    additional_rows_1 = df.copy()\n",
    "    additional_rows_1['Implosion_Year'] = df['Implosion_Year'] - 1\n",
    "    additional_rows_1['Implosion_Next_Year'] = 0\n",
    "    additional_rows_2 = df.copy()\n",
    "    additional_rows_2['Implosion_Year'] = df['Implosion_Year'] - 2\n",
    "    additional_rows_2['Implosion_Next_Year'] = 0\n",
    "    additional_rows_3 = df.copy()\n",
    "    additional_rows_3['Implosion_Year'] = df['Implosion_Year'] - 3\n",
    "    additional_rows_3['Implosion_Next_Year'] = 0\n",
    "    df = pd.concat([df, additional_rows_1, additional_rows_2, additional_rows_3])\n",
    "    df = df.sort_values(by=['ticker_region', 'Implosion_Year'])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df =df.rename({'Implosion_Year' : 'Year'},axis=1)\n",
    "    \n",
    "    # df_metrics = ps.DataFrame(spark.sql(f\"SELECT * FROM {table} LIMIT 10\")) #get all the metrics\n",
    "    # cols = []\n",
    "    # for c in df_metrics.columns:\n",
    "    #     if df_metrics[c].dtype=='float64':#get all the metrics we can calculate correlations with\n",
    "    #         cols.append(c)\n",
    "    \n",
    "    cols = ['ff_debt_entrpr_val', 'ff_tot_debt_tcap_std', 'ff_fix_assets_com_eq', 'ff_debt_eq', 'ff_inven_curr_assets', 'ff_liabs_lease', 'ff_ltd_tcap', 'ff_sales_wkcap',\n",
    "           'ff_bps_gr', 'ff_oper_inc_tcap', 'ff_assets_gr', 'ff_fcf_yld', 'ff_mkt_val_gr', 'ff_earn_yld', 'ff_pbk_tang', 'ff_zscore', 'ff_entrpr_val_sales', 'ff_psales_dil', 'ff_roea', 'ff_dps_gr',\n",
    "           'ff_loan_loss_pct', 'ff_loan_loss_actual_rsrv']\n",
    "    \n",
    "    metric_string = ', '.join('a.' + item for item in cols)\n",
    "    df = get_features_for_imploded_stocks(df, metric_string, table)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "    \n",
    "def create_dataset(table):\n",
    "    # df = get_implosion_df('imploded_stocks.csv')\n",
    "    # df = df.drop(df.columns[0], axis=1)\n",
    "    # df['Implosion_Year'] = df['Implosion_Date'].dt.year\n",
    "    # df['Implosion_Next_Year'] = 1\n",
    "    # get_features_for_imploded_stocks(df)\n",
    "    #print(df.head())\n",
    "    #df=spark.createDataFrame(df)\n",
    "    #df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    imp_df = create_imploded_df(table).toPandas()\n",
    "    non_imp_df =create_non_imploded_ds(table).toPandas()\n",
    "    result_df = pd.concat([non_imp_df,imp_df], ignore_index=True)\n",
    "    #print(result_df.head())\n",
    "    result_df['date'] = pd.to_datetime(result_df['date'], format='%Y-%m-%d')\n",
    "    result_df=result_df.sort_values(by=['ticker_region','date'])\n",
    "    macro_df = get_macro_features().reset_index()\n",
    "    macro_df['Date'] = pd.to_datetime(macro_df['Date'], format='%d/%m/%Y')\n",
    "    #print(macro_df.head())\n",
    "    result_df['month_year'] = result_df['date'].dt.to_period(\"M\")\n",
    "    macro_df['Month_year'] = macro_df['Date'].dt.to_period(\"M\")\n",
    "    result_df = pd.merge(result_df, macro_df, left_on='month_year', right_on='Month_year', how='left')\n",
    "    result_df.drop(['Date', 'index', 'month_year','Month_year','GDP'],axis=1,inplace=True)\n",
    "    \n",
    "    print(result_df.head())\n",
    "    \n",
    "    null_pcts = result_df.isnull().sum()/len(result_df)\n",
    "    print(null_pcts)\n",
    "    \n",
    "    cols_to_drop = null_pcts[null_pcts > 0.1].index.tolist()\n",
    "    result_df.drop(cols_to_drop,axis=1,inplace=True)\n",
    "    print(\"dropped cols: \", cols_to_drop)\n",
    "    \n",
    "    result_df=pd.DataFrame(result_df)\n",
    "    print(\"before dropping nulls: \",len(result_df))\n",
    "    result_df = result_df.dropna()\n",
    "    print(\"after dropping nulls: \", len(result_df))\n",
    "    print(\"number of implosions: \", len(result_df[result_df['Implosion_Next_Year']==1]))\n",
    "    print(\"number of non-implosions: \", len(result_df[result_df['Implosion_Next_Year']==0]))\n",
    "    result_df.to_csv('Advanced_AF_DER_Dataset.csv', index=False)\n",
    "    print(\"dataset written\")\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "create_dataset('FF_ADVANCED_DER_AF')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88d7337e-6ca2-4212-b9d0-17953b8998d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|min(p_date)|\n",
      "+-----------+\n",
      "| 2020-07-14|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT MIN(a.p_date) FROM fp_basic_prices a LEFT JOIN sym_ticker_region s ON s.fsym_id = a.fsym_id WHERE s.ticker_region = 'AACQU-US' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f034c3-4ed9-400e-a07b-6d2c6be018d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
