{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e9989a2-2a32-4b50-9f32-699e89186404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='2022_10_22', catalog='spark_catalog', description='FactSet data version for the day', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse'),\n",
       " Database(name='2023_04_01', catalog='spark_catalog', description='FactSet data version for the day', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse'),\n",
       " Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"List available databases and tables\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e09a3162-25b7-416d-b765-96669b96b50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------+\n",
      "| namespace|           tableName|isTemporary|\n",
      "+----------+--------------------+-----------+\n",
      "|2023_04_01|  affiliate_type_map|      false|\n",
      "|2023_04_01|     asset_class_map|      false|\n",
      "|2023_04_01|      audit_type_map|      false|\n",
      "|2023_04_01|ca_div_freq_qual_map|      false|\n",
      "|2023_04_01|     ca_div_type_map|      false|\n",
      "|2023_04_01|   ca_event_type_map|      false|\n",
      "|2023_04_01| ce_audio_source_map|      false|\n",
      "|2023_04_01|   ce_event_type_map|      false|\n",
      "|2023_04_01|ce_fiscal_period_map|      false|\n",
      "|2023_04_01|  ce_market_time_map|      false|\n",
      "+----------+--------------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN 2023_04_01\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9337ae4-d830-40cd-b8dd-a025e9afa4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "imploded_stocks = pd.read_csv('filtered_tickers.csv')\n",
    "imploded_stocks = imploded_stocks['Ticker'].tolist()\n",
    "sp500_stocks = pd.read_csv('spx500.csv', usecols=['Symbol'])\n",
    "sp500_stocks = sp500_stocks['Symbol'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4014111-e7b0-46f3-8a94-e02db58cd2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "combined_stocks = imploded_stocks + sp500_stocks\n",
    "#random.shuffle(combined_stocks)\n",
    "combined_stocks = sorted(combined_stocks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ece0df-9c71-478e-9617-3cd142bd32a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "from datetime import datetime\n",
    "\n",
    "def pull_stock_monthly(stock_list, start_date='2009-01-01', end_date=datetime.now().strftime('%Y-%m-%d')):\n",
    "    all_series = []\n",
    "    for t in stock_list:\n",
    "        df = yf.download(t, start=start_date, end=end_date, progress=False)\n",
    "        if not(df.empty):\n",
    "            monthly_df = df['Adj Close'].resample('M').last().to_frame()\n",
    "            monthly_df['Ticker'] = t\n",
    "            all_series.append(monthly_df)\n",
    "    combined_df = pd.concat(all_series, axis=0)\n",
    "    combined_df.to_csv('imploded_only.csv')\n",
    "        \n",
    "\n",
    "#pull_stock_monthly(imploded_stocks)\n",
    "#with (filtered) imploded stocks and sp500 stocks the total number of rows of data is 134,523 (monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25d58f1c-af0b-4d11-b9ff-e2a359352fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_data(filename):\n",
    "    pandas_df = pd.read_csv(filename)\n",
    "    spark_df = spark.createDataFrame(pandas_df)\n",
    "    spark_df = spark_df.orderBy(\"Ticker\", \"Date\")\n",
    "    #spark_df.show(10)\n",
    "    \n",
    "    return spark_df\n",
    "\n",
    "def clean_after_implosions(df):\n",
    "    cleaned_rows = []\n",
    "    unique_tickers_df = df.select(\"Ticker\").distinct()\n",
    "    unique_tickers = [row.Ticker for row in unique_tickers_df.collect()]\n",
    "    for t in unique_tickers:\n",
    "        filtered_df = df.filter(df[\"Ticker\"] == t)\n",
    "        latest_imploded_date = filtered_df.filter(filtered_df['Imploded']==1).agg({\"Date\": \"max\"}).collect()[0][0]\n",
    "        cleaned_rows_for_ticker = filtered_df.filter(filtered_df['Date'] <= latest_imploded_date).collect()\n",
    "        cleaned_rows.extend(cleaned_rows_for_ticker)\n",
    "    cleaned_df = spark.createDataFrame(cleaned_rows, schema=df.schema)\n",
    "    pandas_df = cleaned_df.toPandas()\n",
    "    pandas_df.to_csv('imploded_cleaned.csv', index=False)\n",
    "    print(pandas_df.head(100))\n",
    "        \n",
    "\n",
    "#task of which stocks vs when stock implodes\n",
    "#clean_after_implosions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a298f65-c1d2-4bf4-8872-342736ab3bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = get_data('imploded_only.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "581439d4-e3a0-4e84-b316-1f8207931d52",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'imploded_cleaned.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m     plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumber of samples\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 13\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimploded_cleaned.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m imploded_vs_normal(df)\n",
      "Cell \u001b[0;32mIn [6], line 3\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_data\u001b[39m(filename):\n\u001b[0;32m----> 3\u001b[0m     pandas_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     spark_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(pandas_df)\n\u001b[1;32m      5\u001b[0m     spark_df \u001b[38;5;241m=\u001b[39m spark_df\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTicker\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'imploded_cleaned.csv'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imploded_vs_normal(df):\n",
    "    imploded = df.filter(col('Imploded')==1).count()\n",
    "    normal = df.filter(col('Imploded')==0).count()\n",
    "    print(imploded)\n",
    "    plt.figure(figsize=(5,2))\n",
    "    plt.bar(['Non-imploded', 'Imploded'], [normal, imploded])\n",
    "    plt.ylabel('Number of samples')\n",
    "    plt.show()\n",
    "\n",
    "# df = get_data('imploded_cleaned.csv')\n",
    "# imploded_vs_normal(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2955cdc8-146c-4fec-9ba4-80addb867a06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "99ee2ff4-62f8-44c0-9843-347c529fa1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import quarter, expr\n",
    "from pyspark.sql.functions import col, lit, when, coalesce\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Suppress FutureWarning\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def prev_quarter(date_col):\n",
    "    return F.when((F.month(date_col) == 1) | (F.month(date_col) == 2) | (F.month(date_col) == 12), 1) \\\n",
    "            .when((F.month(date_col) >= 3) & (F.month(date_col) <= 5), 2) \\\n",
    "            .when((F.month(date_col) >= 6) & (F.month(date_col) <= 8), 3) \\\n",
    "            .otherwise(4)\n",
    "\n",
    "\n",
    "def reporting_year(date_col):\n",
    "    return F.when((F.month(date_col) == 1) | (F.month(date_col) == 2), F.year(date_col) - 1) \\\n",
    "            .otherwise(F.year(date_col))\n",
    "\n",
    "def total_assets(ticker, start_date, end_date):\n",
    "    spark.sql(\"USE 2023_04_01\")\n",
    "    # Assuming that 'ticker' is a valid Python variable\n",
    "\n",
    "    query = f\"\"\"SELECT d.ticker_region, a.date, a.ff_assets, a.ff_sales, a.FF_GROSS_INC, a.FF_COM_SHS_OUT, FF_CASH_ST, FF_DEBT, FF_PRICE_CLOSE_FP\n",
    "                FROM FF_BASIC_QF a \n",
    "                LEFT JOIN sym_ticker_region d ON d.fsym_id = a.fsym_id \n",
    "                WHERE d.ticker_region = \"{ticker}-US\" \n",
    "                AND a.date >= \"{start_date}\" AND a.date <= \"{end_date}\"\n",
    "                ORDER BY a.date\"\"\"\n",
    "\n",
    "    df = spark.sql(query)\n",
    "    df = df.withColumnRenamed('ff_assets', 'Total Assets')\n",
    "    df = df.withColumnRenamed('ff_sales', 'Sales')\n",
    "    df = df.withColumnRenamed('FF_COM_SHS_OUT', 'Market Val')\n",
    "    df = df.withColumnRenamed('FF_CASH_ST', 'Cash/Short Term Investments')\n",
    "    df = df.withColumnRenamed('FF_DEBT', 'Total Debt')\n",
    "    df = df.withColumnRenamed('FF_GROSS_INC', 'Gross Income')\n",
    "    df = df.withColumnRenamed('FF_PRICE_CLOSE_FP', 'Share Price')\n",
    "    df = df.withColumn('Market Val', col('Market Val') * col('Share Price'))\n",
    "    df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    return df\n",
    "\n",
    "def plot_with_price(spark_df):\n",
    "    unique_tickers_df = spark_df.select(\"Ticker\").distinct()\n",
    "    unique_tickers = sorted([row.Ticker for row in unique_tickers_df.collect()])\n",
    "    for t in unique_tickers:\n",
    "        filtered_df = spark_df.filter(spark_df[\"Ticker\"] == t)\n",
    "        start_date = filtered_df.agg({\"Date\": \"min\"}).collect()[0][0]\n",
    "        end_date = filtered_df.agg({\"Date\": \"max\"}).collect()[0][0]\n",
    "        t_df = total_assets(t, start_date, end_date)\n",
    "        num_cols = 3\n",
    "        columns = t_df.columns\n",
    "        columns.remove('date')\n",
    "        columns.remove('ticker_region')\n",
    "        num_rows = (len(columns) - 1) // num_cols + 1\n",
    "        scaler = StandardScaler()\n",
    "        fig, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(15, 5*num_rows))\n",
    "        axes = axes.flatten()\n",
    "        for i, column in enumerate(columns):\n",
    "            if column != \"Share Price\" and column != 'ticker_region' and column != 'date':\n",
    "                # Plot the selected column against the \"FF_PRICE_CLOSE_FP\"\n",
    "                normalized_column = scaler.fit_transform(t_df.select(column).collect())\n",
    "                normalized_price = scaler.fit_transform(t_df.select(\"Share Price\").collect())\n",
    "                \n",
    "                axes[i].plot(t_df.select(\"date\").collect(), normalized_column, label=column)\n",
    "                axes[i].plot(t_df.select(\"date\").collect(), normalized_price, label=\"Price\")\n",
    "\n",
    "                # Set subplot title\n",
    "                axes[i].set_title(f\"{column} and Price over Time\")\n",
    "\n",
    "                # Set axis labels\n",
    "                axes[i].set_xlabel(\"Date\")\n",
    "                axes[i].set_ylabel(\"Value\")\n",
    "\n",
    "                # Display legend\n",
    "                axes[i].legend()\n",
    "\n",
    "        for j in range(len(columns), num_cols * num_rows):\n",
    "            fig.delaxes(axes[j])\n",
    "\n",
    "        # Adjust layout to prevent overlapping\n",
    "        plt.tight_layout()\n",
    "\n",
    "        # Show the plots\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "def corr_analysis(spark_df):\n",
    "    n = 0\n",
    "    corr_matrices = []\n",
    "    unique_tickers_df = spark_df.select(\"Ticker\").distinct()\n",
    "    unique_tickers = sorted([row.Ticker for row in unique_tickers_df.collect()])\n",
    "    for t in unique_tickers:\n",
    "        filtered_df = spark_df.filter(spark_df[\"Ticker\"] == t)\n",
    "        start_date = filtered_df.agg({\"Date\": \"min\"}).collect()[0][0]\n",
    "        end_date = filtered_df.agg({\"Date\": \"max\"}).collect()[0][0]\n",
    "        t_df = total_assets(t, start_date, end_date)\n",
    "        cm = t_df.toPandas().corr()\n",
    "        cm = pd.DataFrame(cm)\n",
    "        sns.heatmap(cm, annot=True, cmap='RdYlGn',linewidths=0.2)\n",
    "        fig=plt.gcf()\n",
    "        fig.set_size_inches(5,4)\n",
    "        plt.show()\n",
    "        if not(cm.isnull().values.any()):\n",
    "            corr_matrices.append(cm)\n",
    "            n+=1\n",
    "    avg_corr_mat = sum(corr_matrices) / len(corr_matrices)\n",
    "    sns.heatmap(avg_corr_mat, annot=True, cmap='RdYlGn',linewidths=0.2)\n",
    "    fig=plt.gcf()\n",
    "    fig.set_size_inches(5,4)\n",
    "    plt.show()\n",
    "    \n",
    "def corr_analysis_spx500():\n",
    "    n = 0\n",
    "    corr_matrices = []\n",
    "    start_date = '2009-01-01'\n",
    "    end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "    unique_tickers = sp500_stocks[:200]\n",
    "    for t in unique_tickers:\n",
    "        temp_df = yf.download(t, start=start_date, end=end_date, progress=False)\n",
    "        if temp_df.empty:\n",
    "            print(\"No data available for the specified date range.\")\n",
    "            continue\n",
    "        weekly_data = temp_df['Adj Close'].resample('M').last().to_frame().reset_index()\n",
    "        filtered_df = spark.createDataFrame(weekly_data)\n",
    "        start_date = filtered_df.agg({\"Date\": \"min\"}).collect()[0][0]\n",
    "        end_date = filtered_df.agg({\"Date\": \"max\"}).collect()[0][0]\n",
    "        t_df = total_assets(t, start_date, end_date)\n",
    "        cm = t_df.toPandas().corr()\n",
    "        cm = pd.DataFrame(cm)\n",
    "        if not(cm.isnull().values.any()):\n",
    "            corr_matrices.append(cm)\n",
    "            n+=1\n",
    "    avg_corr_mat = sum(corr_matrices) / len(corr_matrices)\n",
    "    sns.heatmap(avg_corr_mat, annot=True, cmap='RdYlGn',linewidths=0.2)\n",
    "    fig=plt.gcf()\n",
    "    fig.set_size_inches(5,4)\n",
    "    plt.show()\n",
    "    print(n, len(unique_tickers))\n",
    "        \n",
    "def add_fundamental(spark_df):\n",
    "    \n",
    "    unique_tickers_df = spark_df.select(\"Ticker\").distinct()\n",
    "    unique_tickers = sorted([row.Ticker for row in unique_tickers_df.collect()])\n",
    "    \n",
    "    for t in unique_tickers:\n",
    "        filtered_df = spark_df.filter(spark_df[\"Ticker\"] == t).select('Date','Ticker')\n",
    "        start_date = filtered_df.agg({\"Date\": \"min\"}).collect()[0][0]\n",
    "        end_date = filtered_df.agg({\"Date\": \"max\"}).collect()[0][0]\n",
    "        assets_df = total_assets(t, start_date, end_date)\n",
    "        joined_df = filtered_df.alias(\"a\").join(assets_df.alias(\"b\"),\n",
    "                             (F.col(\"a.Ticker\") == F.col(\"b.ticker_region\")) &\n",
    "                             (prev_quarter(\"a.Date\") == prev_quarter(\"b.date\")) &\n",
    "                             (reporting_year(\"a.Date\") == reporting_year(\"b.date\")),\n",
    "                             how='left').orderBy('a.Ticker', 'a.Date')\n",
    "        joined_df = joined_df.toPandas()\n",
    "        joined_df.drop(['ticker_region','date','Share Price'],axis=1,inplace=True)\n",
    "        spark_df = spark_df.toPandas()\n",
    "        merged_df = pd.merge(spark_df[['Ticker','Date']], joined_df, how='left', left_on=['Ticker', 'Date'], right_on=['Ticker', 'Date'])\n",
    "        spark_df.loc[spark_df['Ticker']==t, 'Total Assets'] = merged_df.loc[spark_df['Ticker']==t, 'Total Assets']\n",
    "        spark_df.loc[spark_df['Ticker']==t, 'Sales'] = merged_df.loc[spark_df['Ticker']==t, 'Sales']\n",
    "        spark_df.loc[spark_df['Ticker']==t, 'Market Val'] = merged_df.loc[spark_df['Ticker']==t, 'Market Val']\n",
    "        spark_df.loc[spark_df['Ticker']==t, 'Cash/Short Term Investments'] = merged_df.loc[spark_df['Ticker']==t, 'Cash/Short Term Investments']\n",
    "        spark_df.loc[spark_df['Ticker']==t, 'Total Debt'] = merged_df.loc[spark_df['Ticker']==t, 'Total Debt']\n",
    "        spark_df.loc[spark_df['Ticker']==t, 'Gross Income'] = merged_df.loc[spark_df['Ticker']==t, 'Gross Income']\n",
    "        spark_df = spark.createDataFrame(spark_df)\n",
    "        print(\"Done\")\n",
    "        \n",
    "        # Update 'Total Assets' column with 'ff_assets' for the current ticker\n",
    "        \n",
    "        \n",
    "    return spark_df\n",
    "#         update_df = joined_df.select(\"a.Ticker\", \"a.Date\", \"b.ff_assets\")\n",
    "#         spark_df = spark_df.alias(\"a\").join(update_df.alias(\"b\"),\n",
    "#                              (F.col(\"a.Ticker\") == F.col(\"b.Ticker\")) &\n",
    "#                              (F.col(\"a.Date\") == F.col(\"b.Date\")),\n",
    "#                              how='left').select(\"a.*\", \"b.ff_assets\").orderBy('a.Ticker','a.Date')\n",
    "#         spark_df.show(100)\n",
    "    \n",
    "#pull_fundamental('ABEO')\n",
    "# stock_df = get_data('imploded_sp500.csv')\n",
    "# new_df = add_fundamental(df)\n",
    "# new_df = new_df.toPandas()\n",
    "#new_df.to_csv('test.csv', index=False)\n",
    "#get sp500 stocks\n",
    "#get imploded stocks\n",
    "#randomise order\n",
    "#get monthly dates\n",
    "#returns\n",
    "\n",
    "# df = get_data('imploded_only.csv')\n",
    "# corr_analysis(df)\n",
    "#plot_with_price(df)\n",
    "# new_df = add_fundamental(df)\n",
    "# new_df = new_df.toPandas()\n",
    "# new_df.to_csv('test2.csv', index=False)\n",
    "# plot_with_price(df)\n",
    "# corr_analysis_spx500()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "088f7a1f-41e0-46f2-88ec-1d10ae905758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "420\n",
      "377\n"
     ]
    }
   ],
   "source": [
    "print(df.filter(col(\"Imploded\") == 1).count())\n",
    "print(df.select(\"Ticker\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ccbb1-2394-4663-a06a-ba862114a38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stat(spark_df, rowname):\n",
    "    unique_tickers_df = spark_df.select(\"Ticker\").distinct()\n",
    "    unique_tickers = [row.Ticker for row in unique_tickers_df.collect()]\n",
    "    for t in unique_tickers:\n",
    "        filtered_df = spark_df.filter(spark_df[\"Ticker\"] == t)\n",
    "        prices = filtered_df.select(\"Price\").collect()\n",
    "        total_assets = filtered_df.select(rowname).collect()\n",
    "        prices = [row.Price for row in prices]\n",
    "        total_assets = [row[rowname] for row in total_assets]\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "        # Plot Price\n",
    "        axs[0].plot(prices, label='Price', marker='o', color='blue')\n",
    "        axs[0].set_title(f'Price for Ticker {t}')\n",
    "        axs[0].set_xlabel('Data Points')\n",
    "        axs[0].set_ylabel('Price')\n",
    "        axs[0].legend()\n",
    "        axs[0].grid(True)\n",
    "\n",
    "        # Plot Total Assets\n",
    "        axs[1].plot(total_assets, label=rowname, marker='o', color='green')\n",
    "        axs[1].set_title(f'{rowname} for Ticker {t}')\n",
    "        axs[1].set_xlabel('Data Points')\n",
    "        axs[1].set_ylabel(rowname)\n",
    "        axs[1].legend()\n",
    "        axs[1].grid(True)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "#new_df = spark.createDataFrame(new_df)\n",
    "#plot_stat(new_df, 'Total Assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3e0086-9ccb-42b6-b317-fe8a7252e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new_df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7adca791-0dd3-4daf-9d3a-6b5a3c5d2800",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be28461e-df89-4c13-9aff-005bb1361ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
