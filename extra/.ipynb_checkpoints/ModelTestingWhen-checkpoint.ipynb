{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd144b3e-7d0d-4cf5-9c93-9bc5d02ad9bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop-3.2.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.7-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "2024-01-22 12:52:45,922 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-01-22 12:52:48,903 WARN spark.ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"when_approach\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "spark.sql(\"USE 2023_11_02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69308bac-14f7-4812-84d7-bcb53404d755",
   "metadata": {},
   "source": [
    "## WHEN Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c6105a-8637-426f-b539-f5a2f7a0a5f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,col\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "#from boruta import BorutaPy\n",
    "#from fredapi import Fred\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import csv\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "# from pyspark.ml.regression import LinearRegression\n",
    "# from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "# from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from CreateDataset import get_tabular_dataset, get_feature_col_names, get_not_null_cols\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e750d07a-ef46-4855-b9fe-aa2c6bfe4b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_train_test(df, split_date):\n",
    "    split_date = pd.to_datetime(split_date)\n",
    "    train_df = df.filter(col('date')<split_date)\n",
    "    test_df = df.filter(col('date')>=split_date)\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def forward_fill(df):\n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy('date')\n",
    "    feature_cols = df.columns[2:-1]\n",
    "    for c in feature_cols:\n",
    "        df = df.withColumn(\n",
    "            c, F.last(c, ignorenulls=True).over(window_spec)\n",
    "        )\n",
    "    return df.orderBy('fsym_id','date')\n",
    "\n",
    "\n",
    "def write_features_file(data_list, csv_file_path='features.csv'):\n",
    "    data_list = [data_list]\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row in data_list:\n",
    "            writer.writerow(row)\n",
    "    print(\"Features written: \", data_list[0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f517213d-ec24-4599-aff8-7dff42b26765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_df(all_feats=False, imploded_only=False):\n",
    "    df = get_tabular_dataset(all_feats=all_feats, imploded_only=imploded_only)\n",
    "    df = forward_fill(df)\n",
    "    print(\"Number of rows: \", df.count())\n",
    "    print(\"Number of positives: \", df.filter(F.col('label')==1).count())\n",
    "    df=df.fillna(0.0)\n",
    "    print(\"Number of rows after dropping nulls: \", df.count())\n",
    "    print(\"Number of positives after dropping nulls: \", df.filter(F.col('label')==1).count())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c44fee8-ddcf-4bb2-909d-5510bf9b7241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ff_assets_gr', 'ff_net_inc_per_emp', 'ff_debt_entrpr_val', 'ff_fcf_yld', 'ff_sga_oth', 'ff_gross_cf_debt', 'ff_dil_adj', 'ff_shs_float', 'ff_xord', 'ff_inc_sund', 'ff_net_inc_basic_beft_xord', 'ff_non_oper_exp', 'ff_cf_ps_gr', 'ff_emp_gr', 'ff_net_inc_bef_xord_gr', 'ff_com_eq_gr', 'ff_mkt_val_gr', 'ff_zscore', 'ff_dfd_tax_assets_lt', 'ff_ut_non_oper_inc_oth', 'ff_mkt_val_public', 'ff_xord_disc', 'ff_bps_gr', 'ff_ut_operation_exp', 'ff_sales_fix_assets', 'CPI', 'ff_bk_non_oper_inc', 'ff_capex_assets']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 12:52:56,790 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+-------------------+------------------+----------+----------+-----------------+----------+---------------+-------+-----------+--------------------------+-------------------+------------------+-----------------+----------------------+-----------------+-------------+---------+--------------------+----------------------+-----------------+------------+-----------------+-------------------+-------------------+------------+------------------+-----------------+\n",
      "| fsym_id|      date|     ff_assets_gr| ff_net_inc_per_emp|ff_debt_entrpr_val|ff_fcf_yld|ff_sga_oth| ff_gross_cf_debt|ff_dil_adj|   ff_shs_float|ff_xord|ff_inc_sund|ff_net_inc_basic_beft_xord|    ff_non_oper_exp|       ff_cf_ps_gr|        ff_emp_gr|ff_net_inc_bef_xord_gr|     ff_com_eq_gr|ff_mkt_val_gr|ff_zscore|ff_dfd_tax_assets_lt|ff_ut_non_oper_inc_oth|ff_mkt_val_public|ff_xord_disc|        ff_bps_gr|ff_ut_operation_exp|ff_sales_fix_assets|         CPI|ff_bk_non_oper_inc|  ff_capex_assets|\n",
      "+--------+----------+-----------------+-------------------+------------------+----------+----------+-----------------+----------+---------------+-------+-----------+--------------------------+-------------------+------------------+-----------------+----------------------+-----------------+-------------+---------+--------------------+----------------------+-----------------+------------+-----------------+-------------------+-------------------+------------+------------------+-----------------+\n",
      "|B00FG1-R|2012-12-31|             null|               null|              null|      null|      0.59|             null|       0.0|           null|    0.0|        0.0|                     -0.59|                0.0|              null|             null|                  null|             null|         null|     null|                 0.0|                  null|             null|         0.0|             null|                0.0|                0.0| -1.21082E-4|             -0.59| 78.2739737657597|\n",
      "|B00FG1-R|2013-12-31| 27.3337012353016|               null|              null|      null|     7.858|             null|       0.0|           null|    0.0|        0.0|                     -8.89|                0.0|              null|             null|     -1406.77966101695| 56.2754966887417|         null|     null|                 0.0|                  null|             null|         0.0| 56.2737770595563|                0.0|                0.0| 0.002644169|             -8.89| 38.8518469129217|\n",
      "|B00FG1-R|2014-12-31| 1213.34511268169|               null|          0.432116|  -95.4447|     6.666| 3.62235294117647|       0.0| 14.62325559744|    0.0|        0.0|                    29.964|               -1.1|              null|             null|                  null|-507.814354002102|         null|  1.15752|                 0.0|                   0.0| 312.206507005344|         0.0|-492.849756559955|             22.364|  0.359627598152425|-0.003084609|            14.392| 11.9849930575122|\n",
      "|B00FG1-R|2015-12-31| 7.34888349058638|               null|            0.4599|  -11.9743|    13.059| 14.5585231015534|       0.0|  14.7848629761|    0.0|        0.0|                    73.848| -0.829999999999998|  883.087919032337|             null|      146.455746896276|-54.3522898256756|      4.30427|     null|                 0.0|                   0.0| 315.804673169496|         0.0|-48.0518554671434|             25.255|   0.87369424206093|-0.001075553|            75.952|0.483800029321214|\n",
      "|B00FG1-R|2016-12-31| 76.9038689814662|               null|          0.425666|  -22.8867|     13.44| 17.8101193860148|       0.0| 21.88557444594|    0.0|        0.0|                     80.96|  -3.52200000000001|  12.2864859020326|             null|      9.63059256851911| 81.9351016561196|      3.40202|  1.64783|                 0.0|                   0.0| 398.317454916108|         0.0| 85.1141023124192|             44.162|  0.296733531435523| 0.002524522|            85.144| 2.17970138879606|\n",
      "|B00FG1-R|2017-12-31|-1.41419272051649|   2.34312820512821|          0.346485|  -4.98055|    15.751| 26.5156443321981|       0.0|34.416235934208|    0.0|        0.0|                    91.382| -0.532999999999987|  29.7047334404834|             null|       12.873023715415| 55.8523642001073|      15.5789|  1.97356|                 0.0|                   0.0| 721.020142821658|         0.0| 56.0309541286687|             66.483|  0.349934699583753| 0.002106889|           115.385| 10.7710663683818|\n",
      "|B00FG1-R|2018-12-31|  29.666192122568|  0.920121951219512|          0.395634|  -18.7801|    18.475| 20.1105262845228|       0.0|22.052102494977|    0.0|        0.0|                     75.45|             -2.896| -12.2888973454115|  110.25641025641|     -17.4345057013416|             null|      3.83772|  1.52302|                 0.0|                   0.0| 443.247260149038|         0.0|             null|              88.39|   0.29922470145094|  6.84894E-4|           103.733| 18.3714590742121|\n",
      "|B00FG1-R|2019-12-31| 1.74088438055823|   1.09008695652174|          0.410164| -0.559477|    20.673| 19.6100505670088|       0.0|25.060584527495|    0.0|        0.0|                   100.288|  -3.05200000000001| -14.3631120254975| 12.1951219512195|      32.9198144466534| 343.051690699047|      38.0276|  1.82694|                 0.0|                   0.0| 507.476836681774|         0.0| 223.385786122636|            118.614|  0.348310925643052| 0.003153571|               0.0| 3.26268599653444|\n",
      "|B00FG1-R|2020-12-31| -4.0544623752058|   1.62013186813187|          0.573539|   15.0117|    17.366| 28.0527714002317|       0.0|23.996368176612|    0.0|        0.0|                   147.432|   6.00800000000001|  17.5352245204647|-1.08695652173913|      47.0086151882578| 59.1284984250545|      -54.644|  1.57815|                 0.0|                   0.0|    219.566768816|         0.0| 58.5293317724408|             99.852|  0.365529962894531| 0.004685349|               0.0| 1.31840540216292|\n",
      "|B00FG1-R|2021-12-31|-3.45508338046515|   1.72232584269663|          0.479072|   14.6534|    18.735|  31.857507260531|       0.0|24.168267326764|    0.0|      2.795|                   153.287| -0.193000000000012|0.0248188939876068| -2.1978021978022|      3.97132237234794| 48.7265050802251|      23.9127|  1.86135|                 0.0|                 2.795| 273.101420792433|         0.0|    48.2278920064|            103.438|  0.359795700687542|  0.00780737|               0.0|0.956621402268065|\n",
      "|B01DPB-R|2001-12-31|  16.344051186523|             0.0484|         0.0488336|   3.49776|     835.4| 98.7631578947368|       0.0|  302.954677788|    0.0|      -30.8|                     484.0|              -60.8|   6.7608885020849| 8.08473843493299|      29.3080416777986| 30.6125897760879|     -8.66636|  9.69598|                43.8|                 -30.8| 15087.1429538424|         0.0| 32.0351352760257|              575.2|   2.49479406615682|  -5.6338E-4|             728.0| 5.81970066132962|\n",
      "|B01DPB-R|2002-12-31|   27.46954403063| 0.0556181818181818|         0.0419551|   9.56695|     993.2| 227.055630936228|       0.0|  303.337440984|    0.0|      -43.2|                     611.8|             -124.8|  53.1113380013017|             10.0|      26.4049586776859| 50.2005434079441|     -37.7576|  7.25408|                53.9|                 -43.2|  9357.9600543564|         0.0| 49.4895038356305|              682.6|   2.68801858612678| 0.001652893|             964.0| 4.13685762656327|\n",
      "|B01DPB-R|2003-12-31| 26.6779531429196| 0.0354583333333333|         0.0518372|   0.44542|    1198.9| 71.9919856585469|       0.0|  307.025718584|    0.0|      -33.3|                     425.5|             -539.8| -61.8559613870213| 9.09090909090909|     -30.4511278195489| 16.8619174778189|      96.6442|  8.54648|                 0.9|                 -33.3| 18482.9482587568|       -95.2| 15.9667116972571|              774.0|   2.57899874494492| 0.002702703|            1024.8| 5.42119330919124|\n",
      "|B01DPB-R|2004-12-31| 15.0198310053458|            0.04775|         0.0305096|   3.44323|    1191.0| 152.503033980583|       0.0|  318.301561832|    0.0|      -37.2|                     573.0|             -203.0|  171.576782054103|              0.0|      34.6650998824912| 37.9169277263848|      24.3109|  11.3929|                36.2|                 -37.2| 22949.5426080872|       -49.0| 32.8762100406184|              779.4|   2.36934499465173|         0.0|            1080.8| 4.01986506746627|\n",
      "|B01DPB-R|2005-12-31| 17.6799100449775| 0.0341230769230769|         0.0187992|   2.57323|    1344.7| 162.451253481894|       0.0|  331.069481466|    0.0|       38.6|                     443.6|             -204.0| -15.4480745627036| 8.33333333333333|     -22.5828970331588| 25.8918788915315|      -6.5128|  10.4988|                79.3|                  38.6| 21436.7489249235|       -29.7|  20.934403877681|              683.8|   1.94149168853893|         0.0|             631.8| 4.93836990795299|\n",
      "|B01HWF-R|2001-12-31| 55.1791059504622|-0.0846993865030675|              null|      null|    15.874|-3417.62114537445|       0.0|           null|    0.0|       0.49|                    -14.21|               0.49|  70.2828811540748|             null|      35.2195945945946| 189.748157745566|         null|     null|                 0.0|                  0.49|             null|         0.0| 84.4022303573138|             32.928|   5.30754942284729|  -5.6338E-4|           -14.296| 3.61200610479905|\n",
      "|B01HWF-R|2002-12-31| 35.5689333559437|-0.0235051546391753|         0.0012791|  0.436023|    20.524| 840.659340659341|       0.0|    6.927463815|    0.0|      0.398|                   -11.573|-0.0410000000000084|              null| 19.0184049079755|      66.9708822251195| 9.75377993907382|         null|  5.98403|                 0.0|                 0.398|     90.057029595|         0.0| 13.4152186938286|             73.441|    9.8628841607565| 0.001652893|            -4.519| 2.73781975107887|\n",
      "|B01HWF-R|2003-12-31| 52.8113077740947|-0.0364631901840491|        5.65612E-4|  -5.31887|    37.084|          -5300.0|       0.0|    4.954369125|    0.0|      0.576|                   -12.149|  0.575999999999993| -438.519975607204| 68.0412371134021|     -160.679824561404| 39.8334648977617|       77.092|  6.40421|                 0.0|                 0.576|   98.44331451375|         0.0| 20.6885378695828|            211.923|   14.8109465071592| 0.002702703|           -12.463| 7.03863627061761|\n",
      "|B01HWF-R|2004-12-31| 284.995702533459|-0.0117417840375587|          0.110449|   1.32405|    71.128|0.639515600949068|       0.0|    12.00540327|    0.0|      1.124|                     -5.19|   1.12399999999998|              null| 30.6748466257669|      57.9204172625557| 206.901700841316|      319.417|  5.76224|                 0.0|                 1.124|     828.37282563|         0.0| 154.099633182268|            424.921|   19.7254346785771|         0.0|            -6.126| 2.32124253183935|\n",
      "|B01HWF-R|2005-12-31|-13.3818276529245|-0.0401903225806452|          0.161971|  -9.29705|   116.218|-11.7010723227361|       0.0|    10.62432724|    0.0|      4.728|                   -25.103|   4.72799999999996| -122.407315732113| 45.5399061032864|     -398.160735705718|-46.7466119193981|     -60.1957|  3.47132|                 0.0|                 4.728|    299.074811806|         0.0|-45.4183276083244|            695.784|   9.53627314897202|         0.0|           -29.646| 13.7275898782804|\n",
      "+--------+----------+-----------------+-------------------+------------------+----------+----------+-----------------+----------+---------------+-------+-----------+--------------------------+-------------------+------------------+-----------------+----------------------+-----------------+-------------+---------+--------------------+----------------------+-----------------+------------+-----------------+-------------------+-------------------+------------+------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  110777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positives:  579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after dropping nulls:  110777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:==================================================>   (187 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positives after dropping nulls:  579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e32173bf-00ff-4a98-afb6-27204dcc2ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Boruta import BorutaPy\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def boruta_fs(train_df, test_df, model_name): #HOW DOES BORUTA ACC WORK?\n",
    "    train_df = train_df.toPandas()\n",
    "    test_df = test_df.toPandas()\n",
    "    X_train = train_df.drop(['fsym_id', 'date', 'label'], axis=1)\n",
    "    y_train = train_df['label']\n",
    "    X_test = test_df.drop(['fsym_id', 'date', 'label'], axis=1)\n",
    "    y_test = test_df['label']\n",
    "    \n",
    "    if model_name == 'rf':\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        model = GradientBoostingClassifier\n",
    "    feat_selector = BorutaPy(model, n_estimators='auto', verbose=2, random_state=1)\n",
    "    feat_selector.fit(X_train, y_train)\n",
    "    features = X_train.columns.tolist()\n",
    "    print(\"Number of features: \", len(features) )\n",
    "    feature_ranks = list(zip(features, feat_selector.ranking_, feat_selector.support_))\n",
    "    selected_features = []\n",
    "    for feat in feature_ranks:\n",
    "        print(f\"Feature: {feat[0]}, Rank: {feat[1]}, Keep: {feat[2]}\")\n",
    "        if feat[1] <= 5:\n",
    "            selected_features.append(feat[0])\n",
    "    print(\"Selected features: \", selected_features)\n",
    "    return selected_features\n",
    "\n",
    "\n",
    "# train_df, test_df = get_df(all_feats=True)\n",
    "# boruta_features = boruta_fs(train_df, test_df, 'rf')\n",
    "# current_features = get_feature_col_names()\n",
    "# for f in boruta_features:\n",
    "#     if f in current_features:\n",
    "#         print(f)\n",
    "# final_features = list(set(boruta_features + current_features))\n",
    "# write_features_file(final_features) #in the feature selection pipeline, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7029e19c-0f91-44a2-9235-eb105d12f241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def correlation_matrix(df):\n",
    "    df =df.toPandas()\n",
    "    print(\"Converted to Pandas\")\n",
    "    corr_df = df.drop(['date','fsym_id'], axis=1)\n",
    "    corr_mat = corr_df.corr()\n",
    "    mask = np.triu(np.ones_like(corr_mat))\n",
    "    plt.figure(figsize=(50, 40))\n",
    "    sns.heatmap(corr_mat, annot=True, cmap='coolwarm', fmt=\".2f\", mask=mask)\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('corr_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Variable pairs with absolute correlation above 0.7:\")\n",
    "    for i in range(len(corr_mat.columns)):\n",
    "        for j in range(i+1, len(corr_mat.columns)):\n",
    "            if abs(corr_mat.iloc[i, j]) >= 0.7:\n",
    "                print(f\"{corr_mat.columns[i]} - {corr_mat.columns[j]}: {corr_mat.iloc[i, j]}\")\n",
    "                \n",
    "# correlation_matrix(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "848cd54e-1b88-40ad-a7b1-c2e68b008be4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_div_yld_secs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_earn_yld\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_roa_ptx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_net_inc_basic_aft_xord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_net_inc_dil\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_oper_inc_aft_unusual\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      2\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_net_inc_dil_aft_xord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_net_inc_dil_bef_unusual\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_ebit_bef_unusual\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_eps_dil_gr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGDP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_bk_oper_inc_tot\u001b[39m\u001b[38;5;124m'\u001b[39m )\n\u001b[1;32m      3\u001b[0m test_df \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_div_yld_secs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_earn_yld\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_roa_ptx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_net_inc_basic_aft_xord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_net_inc_dil\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_oper_inc_aft_unusual\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_net_inc_dil_aft_xord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_net_inc_dil_bef_unusual\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_ebit_bef_unusual\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_eps_dil_gr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGDP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_bk_oper_inc_tot\u001b[39m\u001b[38;5;124m'\u001b[39m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# train_df = train_df.drop('ff_div_yld_secs', 'ff_earn_yld', 'ff_roa_ptx', 'ff_net_inc_basic_aft_xord', 'ff_net_inc_dil', 'ff_oper_inc_aft_unusual', \n",
    "#                         'ff_net_inc_dil_aft_xord', 'ff_net_inc_dil_bef_unusual', 'ff_ebit_bef_unusual', 'ff_eps_dil_gr', 'GDP', 'ff_bk_oper_inc_tot' )\n",
    "# test_df = test_df.drop('ff_div_yld_secs', 'ff_earn_yld', 'ff_roa_ptx', 'ff_net_inc_basic_aft_xord', 'ff_net_inc_dil', 'ff_oper_inc_aft_unusual', \n",
    "#                         'ff_net_inc_dil_aft_xord', 'ff_net_inc_dil_bef_unusual', 'ff_ebit_bef_unusual', 'ff_eps_dil_gr', 'GDP', 'ff_bk_oper_inc_tot' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21aec020-f103-4438-8592-c5cf06826d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop('ff_div_yld_secs', 'ff_earn_yld', 'ff_roa_ptx', 'ff_net_inc_basic_aft_xord', 'ff_net_inc_dil', 'ff_oper_inc_aft_unusual', \n",
    "                        'ff_net_inc_dil_aft_xord', 'ff_net_inc_dil_bef_unusual', 'ff_ebit_bef_unusual', 'ff_eps_dil_gr', 'GDP', 'ff_bk_oper_inc_tot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "134eb184-057f-4541-b038-04d7b86bf9a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ff_assets_gr',\n",
       " 'ff_net_inc_per_emp',\n",
       " 'ff_debt_entrpr_val',\n",
       " 'ff_fcf_yld',\n",
       " 'ff_sga_oth',\n",
       " 'ff_gross_cf_debt',\n",
       " 'ff_dil_adj',\n",
       " 'ff_shs_float',\n",
       " 'ff_xord',\n",
       " 'ff_inc_sund',\n",
       " 'ff_net_inc_basic_beft_xord',\n",
       " 'ff_non_oper_exp',\n",
       " 'ff_cf_ps_gr',\n",
       " 'ff_emp_gr',\n",
       " 'ff_net_inc_bef_xord_gr',\n",
       " 'ff_com_eq_gr',\n",
       " 'ff_mkt_val_gr',\n",
       " 'ff_zscore',\n",
       " 'ff_dfd_tax_assets_lt',\n",
       " 'ff_ut_non_oper_inc_oth',\n",
       " 'ff_mkt_val_public',\n",
       " 'ff_xord_disc',\n",
       " 'ff_bps_gr',\n",
       " 'ff_ut_operation_exp',\n",
       " 'ff_sales_fix_assets',\n",
       " 'CPI',\n",
       " 'ff_bk_non_oper_inc',\n",
       " 'ff_capex_assets']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = df.columns[2:-1]\n",
    "# write_features_file(feats)\n",
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b57d4b-57d7-4969-8ec1-5827dcbbd6c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from pyspark.ml.feature import VectorAssembler\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.ml.regression import LinearRegression\n",
    "# from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "# from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from sklearn import tree\n",
    "import shap\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from collections import Counter\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "\n",
    "def confusion_matrix_pandas(acts, preds):\n",
    "    cm = confusion_matrix(acts, preds) #correct order\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "def feature_importances(model, features):\n",
    "    feature_importances = model.feature_importances_\n",
    "\n",
    "    print(\"Feature Importances:\")\n",
    "    for feature, importance in zip(features, feature_importances):\n",
    "        print(f\"{feature}: {importance}\")\n",
    "\n",
    "    sorted_idx = np.argsort(feature_importances)[::-1]\n",
    "    sorted_features = [features[i] for i in sorted_idx]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(feature_importances)), feature_importances[sorted_idx], align=\"center\")\n",
    "    plt.xticks(range(len(feature_importances)), sorted_features, rotation=45, ha=\"right\")\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"Feature Importance\")\n",
    "    plt.title(\"Feature Importances\")\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "def model_testing(df, classifier):\n",
    "    df = df.toPandas()\n",
    "    print(\"Converted to Pandas\")\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.set_index('date', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    X_train = df.drop(exclude_columns, axis=1)\n",
    "    y_train = df['label']\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    if classifier == 'LogisticRegression':\n",
    "        param_space = {\n",
    "            'C': hp.uniform('C', 0.01, 1.0) }\n",
    "        classifier_instance = LogisticRegression(class_weight = class_weight_dict, solver='sag')\n",
    "        scaler = StandardScaler()\n",
    "        feats = X_train.columns\n",
    "        X_train[feats] = scaler.fit_transform(X_train[feats])\n",
    "        \n",
    "    elif classifier == 'RandomForest':\n",
    "        param_space = { \n",
    "            'n_estimators': hp.quniform('n_estimators', 100, 500, 1),\n",
    "            'max_depth': hp.quniform('max_depth', 5, 20, 1)\n",
    "        }\n",
    "        classifier_instance = RandomForestClassifier(class_weight = class_weight_dict)\n",
    "    elif classifier == 'GBT':\n",
    "        param_space = { 'n_estimators':hp.uniform('n_estimators',100,500),\n",
    "           'max_depth':hp.quniform('max_depth',5,20,1)}\n",
    "        classifier_instance = GradientBoostingClassifier()\n",
    "    elif classifier == 'XGB':\n",
    "        param_space = { 'n_estimators':hp.quniform('n_estimators',100,500,1),\n",
    "           'max_depth':hp.quniform('max_depth',5,20,1)}\n",
    "        counter = Counter(y_train)\n",
    "        # estimate scale_pos_weight value\n",
    "        estimate = counter[0] / counter[1]\n",
    "        print('Estimate: %.3f' % estimate)\n",
    "        \n",
    "        classifier_instance = xgb.XGBClassifier(scale_pos_weight=estimate, seed=42)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported classifier\")\n",
    "        \n",
    "        \n",
    "        \n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    def set_params(classifier, params):\n",
    "        if classifier == 'RandomForest':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            # params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            # params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        elif classifier == 'GBT':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            # params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            # params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        elif classifier == 'XGB':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            # params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            # params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        \n",
    "        else:\n",
    "            return params\n",
    "        \n",
    "    def objective(params):\n",
    "        params = set_params(classifier, params)\n",
    "        classifier_instance.set_params(**params)\n",
    "        scores = cross_val_score(classifier_instance, X_train, y_train, cv=tscv, scoring='f1')\n",
    "        score = -scores.mean()\n",
    "        return score\n",
    "    \n",
    "    \n",
    "#     initial_model = classifier_instance\n",
    "#     initial_model = initial_model.fit(X_train, y_train)\n",
    "#     initial_preds = pd.DataFrame()\n",
    "#     print(\"INITIAL: \")\n",
    "#     initial_preds['prediction'] = initial_model.predict(X_test)\n",
    "#     initial_preds['label'] = y_test\n",
    "#     confusion_matrix_pandas(initial_preds)\n",
    "    \n",
    "    best_params = fmin(fn=objective, space=param_space, algo=tpe.suggest, max_evals=10, early_stop_fn=no_progress_loss(10))\n",
    "    \n",
    "    best_params = set_params(classifier, best_params)\n",
    "    classifier_instance.set_params(**best_params)\n",
    "    i = 0\n",
    "    final_recall = None\n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        x_train, x_test = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "        Y_train, Y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        \n",
    "        classifier_instance.fit(x_train, Y_train)\n",
    "        \n",
    "        preds = classifier_instance.predict(x_test)\n",
    "        print(f\"Classification Report: \")\n",
    "        print(classification_report(Y_test, preds))\n",
    "        # cm = confusion_matrix(Y_test, preds, labels=classifier_instance.classes_)\n",
    "        # plt.figure(figsize=(8, 6))\n",
    "        # sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
    "        # plt.xlabel(\"Predicted\")\n",
    "        # plt.ylabel(\"Actual\")\n",
    "        # plt.title(\"Confusion Matrix\")\n",
    "        # plt.show()\n",
    "        final_recall = recall_score(Y_test, preds, pos_label=1)\n",
    "        # return recall_minority_class\n",
    "    # return final_recall\n",
    "        \n",
    "    if classifier != 'LogisticRegression':\n",
    "        feature_importances(classifier_instance, X_train.columns.tolist())\n",
    "        \n",
    "    return classifier_instance, X_train.columns.tolist(), X_train\n",
    "#     shapley(classifier_instance, X_train.columns.tolist(), X_train)\n",
    "        \n",
    "        \n",
    "\n",
    "# model_testing(df, 'RandomForest')\n",
    "# basic_test(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5233a-92be-4a5f-b716-d2d925f8b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def model_testing(df, classifier):\n",
    "    print(\"Converted to Pandas\")\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.set_index('date', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    X_train = df.drop(exclude_columns, axis=1)\n",
    "    y_train = df['label']\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    if classifier == 'LogisticRegression':\n",
    "        param_space = {\n",
    "            'C': hp.uniform('C', 0.01, 1.0) }\n",
    "        classifier_instance = LogisticRegression(class_weight = class_weight_dict, solver='sag')\n",
    "        scaler = StandardScaler()\n",
    "        feats = X_train.columns\n",
    "        X_train[feats] = scaler.fit_transform(X_train[feats])\n",
    "        \n",
    "    elif classifier == 'RandomForest':\n",
    "        param_space = { \n",
    "            'n_estimators': hp.quniform('n_estimators', 100, 500, 1),\n",
    "            'max_depth': hp.quniform('max_depth', 5, 20, 1)\n",
    "        }\n",
    "        classifier_instance = RandomForestClassifier(class_weight = class_weight_dict)\n",
    "    elif classifier == 'GBT':\n",
    "        param_space = { 'n_estimators':hp.uniform('n_estimators',100,500),\n",
    "           'max_depth':hp.quniform('max_depth',5,20,1),\n",
    "           'min_samples_leaf':hp.quniform('min_samples_leaf',1,5,1),\n",
    "           'min_samples_split':hp.quniform('min_samples_split',2,6,1)}\n",
    "        classifier_instance = GradientBoostingClassifier()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported classifier\")\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    def set_params(classifier, params):\n",
    "        if classifier == 'RandomForest':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            # params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            # params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        elif classifier == 'GBT':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        else:\n",
    "            return params\n",
    "        \n",
    "    def objective(params):\n",
    "        params = set_params(classifier, params)\n",
    "        classifier_instance.set_params(**params)\n",
    "        scores = cross_val_score(classifier_instance, X_train, y_train, cv=tscv, scoring='f1')\n",
    "        score = -scores.mean()\n",
    "        return score\n",
    "    \n",
    "    \n",
    "#     initial_model = classifier_instance\n",
    "#     initial_model = initial_model.fit(X_train, y_train)\n",
    "#     initial_preds = pd.DataFrame()\n",
    "#     print(\"INITIAL: \")\n",
    "#     initial_preds['prediction'] = initial_model.predict(X_test)\n",
    "#     initial_preds['label'] = y_test\n",
    "#     confusion_matrix_pandas(initial_preds)\n",
    "    \n",
    "    best_params = fmin(fn=objective, space=param_space, algo=tpe.suggest, max_evals=5)\n",
    "    \n",
    "    best_params = set_params(classifier, best_params)\n",
    "    classifier_instance.set_params(**best_params)\n",
    "    i = 0\n",
    "    final_recall = None\n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        x_train, x_test = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "        Y_train, Y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        \n",
    "        classifier_instance.fit(x_train, Y_train)\n",
    "        \n",
    "        preds = classifier_instance.predict(x_test)\n",
    "        print(f\"Classification Report: \")\n",
    "        print(classification_report(Y_test, preds))\n",
    "        # cm = confusion_matrix(Y_test, preds, labels=classifier_instance.classes_)\n",
    "        # plt.figure(figsize=(8, 6))\n",
    "        # sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
    "        # plt.xlabel(\"Predicted\")\n",
    "        # plt.ylabel(\"Actual\")\n",
    "        # plt.title(\"Confusion Matrix\")\n",
    "        # plt.show()\n",
    "        final_recall = recall_score(Y_test, preds, pos_label=1)\n",
    "        # return recall_minority_class\n",
    "    # return final_recall\n",
    "        \n",
    "    if classifier != 'LogisticRegression':\n",
    "        feature_importances(classifier_instance, X_train.columns.tolist())\n",
    "        \n",
    "    return classifier_instance, X_train.columns.tolist(), X_train\n",
    "#     shapley(classifier_instance, X_train.columns.tolist(), X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3d3ef2b-4836-4841-ba91-52867fa18db8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def shapley(model, features, X_train, X_test):\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    shap.initjs()\n",
    "    print(shap_values.shape)\n",
    "    shap.summary_plot(shap_values, X_test)\n",
    "    # shap.plots.waterfall(shap_values)\n",
    "    # shap.plots.bar(shap_values)\n",
    "    # shap.summary_plot(shap_values[0], X_test)\n",
    "    # shap.summary_plot(shap_values[1], X_test)\n",
    "\n",
    "# shapley(model, feats, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "232afb58-57b9-4b0e-97a7-af4375254b7e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-22 09:25:53,352 ERROR scheduler.AsyncEventQueue: Listener EventLoggingListener threw an exception\n",
      "java.util.ConcurrentModificationException\n",
      "\tat java.util.Hashtable$Enumerator.next(Hashtable.java:1408)\n",
      "\tat scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:424)\n",
      "\tat scala.collection.convert.Wrappers$JPropertiesWrapper$$anon$6.next(Wrappers.scala:420)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:941)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1429)\n",
      "\tat scala.collection.IterableLike.foreach(IterableLike.scala:74)\n",
      "\tat scala.collection.IterableLike.foreach$(IterableLike.scala:73)\n",
      "\tat scala.collection.AbstractIterable.foreach(Iterable.scala:56)\n",
      "\tat scala.collection.mutable.MapLike.toSeq(MapLike.scala:75)\n",
      "\tat scala.collection.mutable.MapLike.toSeq$(MapLike.scala:72)\n",
      "\tat scala.collection.mutable.AbstractMap.toSeq(Map.scala:82)\n",
      "\tat org.apache.spark.scheduler.EventLoggingListener.redactProperties(EventLoggingListener.scala:290)\n",
      "\tat org.apache.spark.scheduler.EventLoggingListener.onJobStart(EventLoggingListener.scala:162)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:37)\n",
      "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:37)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:117)\n",
      "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:101)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:105)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:105)\n",
      "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)\n",
      "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1381)\n",
      "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to Pandas\n",
      "             fsym_id  ff_assets_gr  ff_net_inc_per_emp  ff_debt_entrpr_val  \\\n",
      "date                                                                         \n",
      "2001-01-31  JYS0GH-R     30.130998            0.004602            0.004077   \n",
      "2001-01-31  FYSXNQ-R     -4.190111            0.005855            0.240762   \n",
      "2001-01-31  WXCKXC-R    362.295373            0.077503            0.200075   \n",
      "2001-01-31  GZMRSV-R      0.000000            0.000000            0.000000   \n",
      "2001-01-31  SL27B1-R     15.255612            0.005969            0.027512   \n",
      "...              ...           ...                 ...                 ...   \n",
      "2018-12-31  LLTT6X-R    -10.594743           -0.023289            0.000000   \n",
      "2018-12-31  LKYRMJ-R     -4.536684           -1.061128            0.592564   \n",
      "2018-12-31  LKVW3T-R     -4.026814           -0.105273            0.985095   \n",
      "2018-12-31  LM7NQZ-R     -0.082665            0.055703            0.449658   \n",
      "2018-12-31  XR7GZL-R      4.200943            0.062377            0.000000   \n",
      "\n",
      "            ff_fcf_yld  ff_sga_oth  ff_gross_cf_debt  ff_dil_adj  \\\n",
      "date                                                               \n",
      "2001-01-31    0.555847     20.0610        698.581963         0.0   \n",
      "2001-01-31   -9.802180   2724.0000         26.856148         0.0   \n",
      "2001-01-31    3.058260     36.1640         24.208500         0.0   \n",
      "2001-01-31    0.000000     31.8695          0.000000         0.0   \n",
      "2001-01-31    3.187380    114.1870        998.384424         0.0   \n",
      "...                ...         ...               ...         ...   \n",
      "2018-12-31  -21.996500     19.0150          0.000000         0.0   \n",
      "2018-12-31   -4.512150      0.0000          3.326857         0.0   \n",
      "2018-12-31  -37.579000     96.1000         -3.166195         0.0   \n",
      "2018-12-31    5.786160      0.0000         11.258332         0.0   \n",
      "2018-12-31    0.000000     55.5220         29.305407         0.0   \n",
      "\n",
      "            ff_shs_float  ff_xord  ...  ff_bps_gr  ff_ut_operation_exp  \\\n",
      "date                               ...                                   \n",
      "2001-01-31     22.816861    0.000  ...  22.512487           250.690000   \n",
      "2001-01-31    197.073795    0.000  ...  12.538147          7805.000000   \n",
      "2001-01-31     65.042630    0.000  ...  81.742862           102.982000   \n",
      "2001-01-31      0.000000    0.000  ...   0.000000             2.561000   \n",
      "2001-01-31      0.000000   -0.308  ...  17.075085           214.781000   \n",
      "...                  ...      ...  ...        ...                  ...   \n",
      "2018-12-31     13.418737    0.000  ...  -9.469590           218.317000   \n",
      "2018-12-31     49.636871    0.000  ... -18.276663           -14.390639   \n",
      "2018-12-31    125.116986    0.000  ...  -7.107022           682.700000   \n",
      "2018-12-31      4.030195    0.000  ...   2.109027             6.979000   \n",
      "2018-12-31      0.000000    0.000  ... -21.101104           225.525000   \n",
      "\n",
      "            ff_sales_fix_assets       CPI  ff_bk_non_oper_inc  \\\n",
      "date                                                            \n",
      "2001-01-31             2.572500 -0.000563              22.184   \n",
      "2001-01-31             1.792754 -0.000563             268.000   \n",
      "2001-01-31             3.919573 -0.000563              57.976   \n",
      "2001-01-31             0.000000 -0.000563             -31.801   \n",
      "2001-01-31             3.862850 -0.000563              24.760   \n",
      "...                         ...       ...                 ...   \n",
      "2018-12-31             1.000787  0.000685             -26.627   \n",
      "2018-12-31           864.285808  0.000685             -74.681   \n",
      "2018-12-31             0.095326  0.000685            -442.100   \n",
      "2018-12-31             0.000000  0.000685             -17.297   \n",
      "2018-12-31            33.493046  0.000685              54.563   \n",
      "\n",
      "            ff_capex_assets  label  anomaly_scores  anomaly  preds  \n",
      "date                                                                \n",
      "2001-01-31        15.533070      0        0.237647        1      0  \n",
      "2001-01-31         5.023116      0        0.212226        1      0  \n",
      "2001-01-31         3.351361      0        0.221488        1      0  \n",
      "2001-01-31         0.000000      0        0.242039        1      0  \n",
      "2001-01-31         5.354091      0        0.233328        1      0  \n",
      "...                     ...    ...             ...      ...    ...  \n",
      "2018-12-31         1.347710      0        0.240699        1      0  \n",
      "2018-12-31         0.000000      0        0.146557        1      0  \n",
      "2018-12-31         2.956502      0        0.199247        1      0  \n",
      "2018-12-31         0.038485      0        0.240357        1      0  \n",
      "2018-12-31         1.481137      0        0.232231        1      0  \n",
      "\n",
      "[84966 rows x 33 columns]\n",
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     84622\n",
      "           1       0.01      0.01      0.01       344\n",
      "\n",
      "    accuracy                           0.99     84966\n",
      "   macro avg       0.50      0.50      0.50     84966\n",
      "weighted avg       0.99      0.99      0.99     84966\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def anomaly_det(df):\n",
    "    train_df, test_df = split_train_test(df, '2019-01-01')\n",
    "    features = df.columns[2:-1]\n",
    "    train_df = train_df.toPandas()\n",
    "    test_df = test_df.toPandas()\n",
    "    print(\"Converted to Pandas\")\n",
    "    train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "    train_df.set_index('date', inplace=True)\n",
    "    train_df.sort_index(inplace=True)\n",
    "    test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "    test_df.set_index('date', inplace=True)\n",
    "    test_df.sort_index(inplace=True)\n",
    "    \n",
    "    isol_for = IsolationForest(contamination=0.005, random_state=42)\n",
    "    isol_for.fit(train_df[features])\n",
    "    train_df['anomaly_scores'] = isol_for.decision_function(train_df[features])\n",
    "    train_df['anomaly'] = isol_for.predict(train_df[features])\n",
    "    train_df['preds'] = np.where(train_df['anomaly'] == 1, 0, 1)\n",
    "    print(train_df)\n",
    "    print(f\"Classification Report: \")\n",
    "    print(classification_report(train_df['label'], train_df['preds']))\n",
    "    cm = confusion_matrix(train_df['label'], train_df['preds'])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "anomaly_det(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef026f88-0ea7-4451-ad30-09a06e3f808d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-21 00:44:45.452302: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-21 00:44:45.452341: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-21 00:44:45.453840: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-21 00:44:45.464049: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-21 00:44:46.572114: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "\n",
    "def prepare_seqs(df):\n",
    "    train_df = df.toPandas()\n",
    "    # val_split_date = pd.to_datetime('2017-01-01')\n",
    "    # val_df = train_df[train_df['date'] >= val_split_date]\n",
    "    # print(len(val_df)/len(train_df))\n",
    "    # print(len(val_df[val_df['label']==1]))\n",
    "    # train_df = train_df[train_df['date'] < val_split_date]\n",
    "    # test_df = test_df.toPandas()\n",
    "    feature_cols = train_df.columns[2:-1]\n",
    "    # nn_train_feats = train_df[feature_cols].values\n",
    "    # nn_train_labels = train_df['label'].values\n",
    "    # nn_test_feats = test_df[feature_cols].values\n",
    "    # nn_test_labels = test_df['label'].values\n",
    "    # nn_val_feats = val_df[feature_cols].values\n",
    "    # nn_val_labels = val_df['label'].values\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(16, activation=\"tanh\"),\n",
    "        keras.layers.Dense(8, activation=\"relu\"),\n",
    "        keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "    ])\n",
    "    loss_fn = keras.losses.BinaryCrossentropy()\n",
    "    optimizer = keras.optimizers.Adam(\n",
    "        learning_rate=0.005\n",
    "    )\n",
    "    \n",
    "    def plot_model_performance(mdl, loss, metric):\n",
    "        x = pd.DataFrame(mdl.history).reset_index()\n",
    "        x = pd.melt(x, id_vars='index')\n",
    "        x['validation'] = (x['variable'].str[:4] == 'val_').replace({True:'validation',False:'training'})\n",
    "        x['loss'] = (x['variable'].str[-4:] == 'loss').replace({True:loss,False:metric})\n",
    "        g = sns.FacetGrid(x, col='loss', hue='validation',sharey=False)\n",
    "        g.map(sns.lineplot, 'index','value')\n",
    "        g.add_legend()\n",
    "        return g\n",
    "    \n",
    "    # space = {\n",
    "    # 'units1': hp.choice('units1', np.arange(8, 64, dtype=int)),\n",
    "    # 'units2': hp.choice('units2', np.arange(8, 64, dtype=int)),\n",
    "    # 'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.1))\n",
    "    # }\n",
    "#     def objective(params):\n",
    "#         model = keras.Sequential([\n",
    "#             keras.layers.Dense(params['units1'], activation=\"tanh\"),\n",
    "#             keras.layers.Dense(params['units2'], activation=\"relu\"),\n",
    "#             keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "#         ])\n",
    "#         model.compile(optimizer=keras.optimizers.Adam(learning_rate=params['learning_rate']),\n",
    "#                       loss='binary_crossentropy',\n",
    "#                       metrics=['accuracy'])\n",
    "\n",
    "#         # Train and evaluate the model using cross-validation\n",
    "#         scores = cross_val_score(model, nn_train_feats, nn_train_labels, cv=tscv, scoring='f1')\n",
    "#         mean_f1 = -np.mean(scores)  # Negate to maximize F1 score\n",
    "#         return mean_f1\n",
    "    \n",
    "#     best_params = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=5)\n",
    "#     best_units1 = np.arange(8, 64, dtype=int)[best_params['units1']]\n",
    "#     best_units2 = np.arange(8, 64, dtype=int)[best_params['units2']]\n",
    "#     best_learning_rate = best_params['learning_rate']\n",
    "#     final_model = keras.Sequential([\n",
    "#         keras.layers.Dense(best_units1, activation=\"tanh\"),\n",
    "#         keras.layers.Dense(best_units2, activation=\"relu\"),\n",
    "#         keras.layers.Dense(1, activation=\"sigmoid\")])\n",
    "    model.compile(optimizer=optimizer,\n",
    "                        loss=loss_fn,\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "    # final_model.fit(X_train, y_train, epochs=10, batch_size=32, class_weight=class_weight_dict)\n",
    "    \n",
    "    i=1\n",
    "    for train_idx, val_idx in tscv.split(train_df):\n",
    "        train_fold = train_df.iloc[train_idx, :]\n",
    "        val_fold = train_df.iloc[val_idx, :]\n",
    "        nn_train_feats = train_fold[feature_cols].values\n",
    "        nn_train_labels = train_fold['label'].values\n",
    "        nn_val_feats = val_fold[feature_cols].values\n",
    "        nn_val_labels = val_fold['label'].values\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(nn_train_labels), y=nn_train_labels)\n",
    "        class_weight_dict = dict(enumerate(class_weights))\n",
    "        fit_model = model.fit(nn_train_feats, nn_train_labels, epochs=50, batch_size=32, class_weight=class_weight_dict)\n",
    "\n",
    "        plot_model_performance(fit_model, 'bin_cross_entropy','accuracy')\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        test_loss, test_acc = model.evaluate(nn_val_feats, nn_val_labels)\n",
    "        print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "        # Make predictions on new data\n",
    "        predictions = model.predict(nn_val_feats)\n",
    "        for i in range(len(predictions)):\n",
    "            predictions[i] = 1 if predictions[i] >= 0.5 else 0\n",
    "        print(f\"Classification Report for Fold {i}:\")\n",
    "        print(classification_report(nn_val_labels, predictions))\n",
    "        i+=1\n",
    "        print(np.sum(nn_val_labels==1))\n",
    "        print(np.sum(predictions==1))\n",
    "        cm = confusion_matrix(nn_val_labels, predictions)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "# prepare_seqs(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f51b0e-3083-43d6-beaa-eea3f6c5718f",
   "metadata": {},
   "source": [
    "## Cycling through each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25aab8b6-d366-4465-887f-73f713d427ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+------------------+------------------+----------+----------+----------------+----------+------------+-------+-----------+--------------------------+---------------+-----------+---------+----------------------+------------+-------------+---------+--------------------+----------------------+-----------------+------------+---------+-------------------+-------------------+---+------------------+---------------+-----+----+\n",
      "|fsym_id|date|ff_assets_gr|ff_net_inc_per_emp|ff_debt_entrpr_val|ff_fcf_yld|ff_sga_oth|ff_gross_cf_debt|ff_dil_adj|ff_shs_float|ff_xord|ff_inc_sund|ff_net_inc_basic_beft_xord|ff_non_oper_exp|ff_cf_ps_gr|ff_emp_gr|ff_net_inc_bef_xord_gr|ff_com_eq_gr|ff_mkt_val_gr|ff_zscore|ff_dfd_tax_assets_lt|ff_ut_non_oper_inc_oth|ff_mkt_val_public|ff_xord_disc|ff_bps_gr|ff_ut_operation_exp|ff_sales_fix_assets|CPI|ff_bk_non_oper_inc|ff_capex_assets|label|year|\n",
      "+-------+----+------------+------------------+------------------+----------+----------+----------------+----------+------------+-------+-----------+--------------------------+---------------+-----------+---------+----------------------+------------+-------------+---------+--------------------+----------------------+-----------------+------------+---------+-------------------+-------------------+---+------------------+---------------+-----+----+\n",
      "+-------+----+------------+------------------+------------------+----------+----------+----------------+----------+------------+-------+-----------+--------------------------+---------------+-----------+---------+----------------------+------------+-------------+---------+--------------------+----------------------+-----------------+------------+---------+-------------------+-------------------+---+------------------+---------------+-----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023]\n",
      "YEAR: 2001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 837:===================================================> (193 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to Pandas\n",
      "  0%|          | 0/5 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:01<00:00,  4.39trial/s, best loss: -0.2018120169481122]\n",
      "Classification Report for Fold 0:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.87      0.93       731\n",
      "           1       0.11      0.69      0.18        16\n",
      "\n",
      "    accuracy                           0.87       747\n",
      "   macro avg       0.55      0.78      0.56       747\n",
      "weighted avg       0.97      0.87      0.91       747\n",
      "\n",
      "Classification Report for Fold 0:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.86      0.92       732\n",
      "           1       0.10      0.73      0.17        15\n",
      "\n",
      "    accuracy                           0.86       747\n",
      "   macro avg       0.55      0.80      0.55       747\n",
      "weighted avg       0.98      0.86      0.91       747\n",
      "\n",
      "Classification Report for Fold 0:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.93       737\n",
      "           1       0.09      0.90      0.17        10\n",
      "\n",
      "    accuracy                           0.88       747\n",
      "   macro avg       0.54      0.89      0.55       747\n",
      "weighted avg       0.99      0.88      0.92       747\n",
      "\n",
      "Classification Report for Fold 0:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94       729\n",
      "           1       0.16      0.78      0.26        18\n",
      "\n",
      "    accuracy                           0.89       747\n",
      "   macro avg       0.58      0.84      0.60       747\n",
      "weighted avg       0.97      0.89      0.93       747\n",
      "\n",
      "Classification Report for Fold 0:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.93       731\n",
      "           1       0.13      0.81      0.22        16\n",
      "\n",
      "    accuracy                           0.88       747\n",
      "   macro avg       0.56      0.85      0.58       747\n",
      "weighted avg       0.98      0.88      0.92       747\n",
      "\n",
      "YEAR: 2002\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 860:====================================================>(197 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to Pandas\n",
      "  0%|          | 0/5 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60%|    | 3/5 [00:00<00:00,  8.87trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "\n",
      "\n",
      "1 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "\n",
      "\n",
      "1 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 5/5 [00:00<00:00,  9.89trial/s, best loss=?]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "\n",
      "\n",
      "1 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AllTrialsFailed",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAllTrialsFailed\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(recalls)\n\u001b[1;32m     14\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mcycle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 11\u001b[0m, in \u001b[0;36mcycle\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      9\u001b[0m     temp_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfilter(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m==\u001b[39my)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYEAR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     recall \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_testing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLogisticRegression\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     recalls\u001b[38;5;241m.\u001b[39mappend(recall)\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(recalls)\n",
      "Cell \u001b[0;32mIn[31], line 125\u001b[0m, in \u001b[0;36mmodel_testing\u001b[0;34m(df, classifier)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m score\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m#     initial_model = classifier_instance\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m#     initial_model = initial_model.fit(X_train, y_train)\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#     initial_preds = pd.DataFrame()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m#     initial_preds['label'] = y_test\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m#     confusion_matrix_pandas(initial_preds)\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m set_params(classifier, best_params)\n\u001b[1;32m    128\u001b[0m     classifier_instance\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_params)\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/hyperopt/fmin.py:593\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    590\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    591\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are no evaluation tasks, cannot return argmin of task losses.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    592\u001b[0m         )\n\u001b[0;32m--> 593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmin\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Only if there are some successful trail runs, return the best point in\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# the evaluation space\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m space_eval(space, trials\u001b[38;5;241m.\u001b[39margmin)\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/hyperopt/base.py:620\u001b[0m, in \u001b[0;36mTrials.argmin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margmin\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 620\u001b[0m     best_trial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_trial\u001b[49m\n\u001b[1;32m    621\u001b[0m     vals \u001b[38;5;241m=\u001b[39m best_trial[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmisc\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvals\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;66;03m# unpack the one-element lists to values\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;66;03m# and skip over the 0-element lists\u001b[39;00m\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/hyperopt/base.py:611\u001b[0m, in \u001b[0;36mTrials.best_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    605\u001b[0m candidates \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    606\u001b[0m     t\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m STATUS_OK \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    609\u001b[0m ]\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m candidates:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AllTrialsFailed\n\u001b[1;32m    612\u001b[0m losses \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m candidates]\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(losses) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mAllTrialsFailed\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def cycle(df):\n",
    "    df = df.withColumn('year', F.year('date'))\n",
    "    years = df.select(\"year\").distinct().rdd.map(lambda row: row[0]).collect()\n",
    "    years = sorted(years)\n",
    "    recalls = []\n",
    "    for y in years:\n",
    "        temp_df = df.filter(F.col('year')==y)\n",
    "        print(f\"YEAR: {y}\\n\")\n",
    "        recall = model_testing(temp_df, 'LogisticRegression')\n",
    "        recalls.append(recall)\n",
    "    plt.plot(recalls)\n",
    "    plt.show()\n",
    "    \n",
    "cycle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd66cf-5f79-479b-b435-4ffcf3473d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a722ab-ed0c-4696-882e-04028e46d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #backup extra code:\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import cross_validate\n",
    "\n",
    "# def cross_val(train_df, test_df):\n",
    "#     feature_cols = train_df.columns[2:-1]\n",
    "#     print(\"Features: \",feature_cols)\n",
    "#     assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "#     lr = LogisticRegression(labelCol='label', featuresCol = 'features')\n",
    "#     rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "#     gb = GBTClassifier(labelCol='label', featuresCol = 'features')\n",
    "    \n",
    "#     classifiers = [lr, rf, gb]\n",
    "    \n",
    "#     cv_models = []\n",
    "    \n",
    "#     evaluator = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol=\"rawPrediction\", metricName='areaUnderROC')\n",
    "    \n",
    "#     for classifier in classifiers:\n",
    "#         print(\"Starting...\")\n",
    "#         pipeline = Pipeline(stages=[assembler, classifier])\n",
    "        \n",
    "#         paramGrid = ParamGridBuilder() \\\n",
    "#         .addGrid(classifier.regParam, [0.01, 0.1]) \\\n",
    "#         .addGrid(classifier.elasticNetParam, [0.0, 0.5]) \\\n",
    "#         .addGrid(classifier.maxIter, [10, 20]) \\\n",
    "#         .build()\n",
    "        \n",
    "#         cross_val = CrossValidator(\n",
    "#             estimator=pipeline,\n",
    "#             estimatorParamMaps=paramGrid,\n",
    "#             evaluator=evaluator,\n",
    "#             numFolds=5\n",
    "#         )\n",
    "        \n",
    "#         cv_model = cross_val.fit(train_df)\n",
    "#         cv_models.append(cv_model)\n",
    "#         print(\"Working\")\n",
    "    \n",
    "#     for cv_model in cv_models:\n",
    "#         predictions = cv_model.transform(test_df)\n",
    "#         auc = evaluator.evaluate(predictions)\n",
    "#         print(f\"Model AUC: {auc}\")\n",
    "        \n",
    "# def basic_test(train_df, test_df):\n",
    "#     feature_cols = train_df.columns[2:-1]\n",
    "#     print(\"Features: \",feature_cols)\n",
    "#     assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "#     train_df = assembler.transform(train_df)\n",
    "#     test_df = assembler.transform(test_df)\n",
    "#     lr = LogisticRegression(labelCol='label', featuresCol = 'features')\n",
    "#     rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "#     gb = GBTClassifier(labelCol='label', featuresCol = 'features')\n",
    "    \n",
    "#     evaluator = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol=\"rawPrediction\", metricName='areaUnderROC')\n",
    "    \n",
    "#     lr = lr.fit(train_df)\n",
    "#     preds_train_df = lr.transform(train_df)\n",
    "#     preds_test_df = lr.transform(test_df)\n",
    "    \n",
    "    \n",
    "# def cross_val_pandas(train_df, test_df):\n",
    "#     train_df = train_df.toPandas()\n",
    "#     test_df = test_df.toPandas()\n",
    "#     print(\"Converted to Pandas\")\n",
    "#     exclude_columns = ['fsym_id', 'date', 'label']\n",
    "    \n",
    "#     X_train = train_df.drop(exclude_columns, axis=1)\n",
    "#     y_train = train_df['label']\n",
    "#     X_test = test_df.drop(exclude_columns, axis=1)\n",
    "#     y_test = test_df['label']\n",
    "#     models = {\n",
    "#         'Logistic Regression': LogisticRegression(),\n",
    "#         'Random Forest': RandomForestClassifier(),\n",
    "#         'SVM': SVC(probability=True),\n",
    "#         'Gradient Boosting': GradientBoostingClassifier()\n",
    "#     }\n",
    "\n",
    "#     param_grids = {\n",
    "#         'Logistic Regression': {\n",
    "#             'C': [0.01, 0.1, 1.0, 10.0],\n",
    "#             'max_iter': [100, 200, 300]\n",
    "#         },\n",
    "#         'Random Forest': {\n",
    "#             'n_estimators': [50, 100, 200],\n",
    "#             'max_depth': [None, 10, 20]\n",
    "#         },\n",
    "#         'SVM': {\n",
    "#             'C': [0.1, 1.0, 10.0],\n",
    "#             'kernel': ['linear', 'rbf']\n",
    "#         },\n",
    "#         'Gradient Boosting': {\n",
    "#             'n_estimators': [50, 100, 200],\n",
    "#             'learning_rate': [0.01, 0.1, 0.2],\n",
    "#             'max_depth': [3, 5, 7]\n",
    "#         }\n",
    "#     }\n",
    "#     tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "#     for model_name, model in models.items():\n",
    "#         param_grid = param_grids[model_name]\n",
    "\n",
    "#         grid_search = GridSearchCV(model, param_grid, cv=tscv, scoring='roc_auc')\n",
    "\n",
    "#         grid_search.fit(X_train, y_train)\n",
    "        \n",
    "#         best_model = grid_search.best_estimator_\n",
    "\n",
    "#         predictions = best_model.predict(X_test)\n",
    "\n",
    "#         auc = roc_auc_score(y_test, predictions)\n",
    "\n",
    "#         print(f\"\\nResults for {model_name}:\")\n",
    "#         print(f\"Area under the ROC curve (AUC): {auc}\")\n",
    "#         print(\"Best model hyperparameters:\")\n",
    "#         print(grid_search.best_params_)\n",
    "        \n",
    "#         # Confusion matrix\n",
    "#         cm = confusion_matrix(y_test, predictions)\n",
    "#         plt.figure(figsize=(8, 6))\n",
    "#         sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "#         plt.title(f'Confusion Matrix - {model_name}')\n",
    "#         plt.xlabel('Predicted')\n",
    "#         plt.ylabel('Actual')\n",
    "#         plt.show()\n",
    "        \n",
    "\n",
    "# #model_testing(train_df, test_df)\n",
    "# #cross_val_pandas(train_df, test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
