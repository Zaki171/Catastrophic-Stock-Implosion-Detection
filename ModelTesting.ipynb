{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b862fd7-75a8-4818-95c2-5375dde4b345",
   "metadata": {},
   "source": [
    "## Building the dataset that will be input into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda6c879-2ecc-475a-9f90-99bccea780bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"Building dataset\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "spark.sql(\"USE 2023_04_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54391b05-47b8-4d4b-b943-eda194771996",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import lit,col\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "#from boruta import BorutaPy\n",
    "#from fredapi import Fred\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import csv\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from CreateDataset import get_features_all_stocks_seq, get_full_series_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba39e2b3-07f5-4fd9-adf5-735d881bf565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(fsym_id='B01DPB-R', date=datetime.date(2001, 12, 31), ff_assets_gr=16.344051186523, ff_earn_yld=3.17269, ff_net_inc_basic_aft_xord=484.0, ff_net_inc_dil_bef_unusual=505.0, ff_net_inc_basic_beft_xord=484.0, ff_wkcap=759.2, ff_oper_inc_aft_unusual=744.6, ff_net_inc_bef_xord_gr=29.3080416777986, ff_fcf_yld=3.49776, ff_non_oper_exp=-60.8, ff_net_inc_dil=484.0, ff_net_inc_dil_aft_xord=484.0, ff_oper_inc_gr=-3.00525920360631, ff_ut_non_oper_inc_oth=-30.8), Row(fsym_id='B01DPB-R', date=datetime.date(2002, 12, 31), ff_assets_gr=27.46954403063, ff_earn_yld=6.48298, ff_net_inc_basic_aft_xord=611.8, ff_net_inc_dil_bef_unusual=668.92, ff_net_inc_basic_beft_xord=611.8, ff_wkcap=1437.4, ff_oper_inc_aft_unusual=917.1, ff_net_inc_bef_xord_gr=26.4049586776859, ff_fcf_yld=9.56695, ff_non_oper_exp=-124.8, ff_net_inc_dil=611.8, ff_net_inc_dil_aft_xord=611.8, ff_oper_inc_gr=28.9310611928737, ff_ut_non_oper_inc_oth=-43.2), Row(fsym_id='B01DPB-R', date=datetime.date(2003, 12, 31), ff_assets_gr=26.6779531429196, ff_earn_yld=1.7608, ff_net_inc_basic_aft_xord=330.3, ff_net_inc_dil_bef_unusual=684.85, ff_net_inc_basic_beft_xord=425.5, ff_wkcap=2017.5, ff_oper_inc_aft_unusual=550.3, ff_net_inc_bef_xord_gr=-30.4511278195489, ff_fcf_yld=0.44542, ff_non_oper_exp=-539.8, ff_net_inc_dil=330.3, ff_net_inc_dil_aft_xord=330.3, ff_oper_inc_gr=5.81756283168118, ff_ut_non_oper_inc_oth=-33.3)]\n",
      "+----+-----------------------+----------------------+------------------------------------+-------------------------------------+-------------------------------------+-------------------+----------------------------------+---------------------------------+---------------------+--------------------------+-------------------------+----------------------------------+-------------------------+---------------------------------+\n",
      "|year|ff_assets_gr_null_count|ff_earn_yld_null_count|ff_net_inc_basic_aft_xord_null_count|ff_net_inc_dil_bef_unusual_null_count|ff_net_inc_basic_beft_xord_null_count|ff_wkcap_null_count|ff_oper_inc_aft_unusual_null_count|ff_net_inc_bef_xord_gr_null_count|ff_fcf_yld_null_count|ff_non_oper_exp_null_count|ff_net_inc_dil_null_count|ff_net_inc_dil_aft_xord_null_count|ff_oper_inc_gr_null_count|ff_ut_non_oper_inc_oth_null_count|\n",
      "+----+-----------------------+----------------------+------------------------------------+-------------------------------------+-------------------------------------+-------------------+----------------------------------+---------------------------------+---------------------+--------------------------+-------------------------+----------------------------------+-------------------------+---------------------------------+\n",
      "|2003|                     39|                    63|                                  12|                                   61|                                    2|                261|                                29|                              364|                  136|                       208|                       61|                                61|                      284|                              799|\n",
      "|2007|                     31|                    57|                                  24|                                   37|                                    6|                248|                                22|                              203|                  115|                       126|                       37|                                37|                      149|                              544|\n",
      "|2018|                     31|                    23|                                  39|                                   11|                                    0|                332|                                 0|                              218|                   92|                        48|                       11|                                11|                      142|                              281|\n",
      "|2015|                     15|                    38|                                  32|                                    2|                                    0|                271|                                 0|                              139|                   90|                        24|                        2|                                 2|                      102|                              338|\n",
      "|2023|                      0|                     0|                                   5|                                    2|                                    0|                  0|                                 0|                                0|                    1|                         3|                        2|                                 2|                        1|                                2|\n",
      "|2006|                     22|                    62|                                  24|                                   44|                                    7|                241|                                22|                              249|                  115|                       140|                       44|                                44|                      188|                              599|\n",
      "|2022|                     15|                    22|                                  81|                                   34|                                    1|                273|                                 1|                              119|                  102|                        63|                       34|                                34|                       83|                              271|\n",
      "|2013|                     11|                    49|                                  24|                                    3|                                    1|                312|                                 2|                              190|                  104|                        84|                        3|                                 3|                      123|                              360|\n",
      "|2014|                     18|                    44|                                  19|                                    2|                                    1|                296|                                 0|                              168|                   98|                        52|                        2|                                 2|                      104|                              350|\n",
      "|2019|                     15|                    24|                                  34|                                    8|                                    0|                330|                                 0|                              183|                   87|                        58|                        8|                                 8|                      135|                              269|\n",
      "|2004|                     18|                    69|                                  10|                                   51|                                    3|                247|                                23|                              374|                  136|                       176|                       51|                                51|                      299|                              715|\n",
      "|2020|                      1|                    21|                                  33|                                    6|                                    0|                313|                                 0|                              121|                   79|                        82|                        6|                                 6|                       71|                              264|\n",
      "|2012|                      9|                    59|                                  21|                                    4|                                    1|                296|                                 3|                              179|                  105|                        73|                        4|                                 4|                      145|                              415|\n",
      "|2009|                     16|                    58|                                  34|                                   17|                                    9|                252|                                21|                              309|                  114|                        87|                       17|                                17|                      187|                              564|\n",
      "|2016|                     62|                    37|                                  24|                                    7|                                    0|                299|                                 0|                              207|                   84|                        20|                        7|                                 7|                      155|                              321|\n",
      "|2001|                     58|                   190|                                  18|                                   76|                                    2|                277|                                32|                              270|                  282|                       236|                       76|                                76|                      244|                              942|\n",
      "|2005|                     29|                    67|                                  23|                                   50|                                    9|                252|                                29|                              223|                  117|                       154|                       50|                                50|                      171|                              636|\n",
      "|2010|                     16|                    59|                                  16|                                   11|                                    6|                243|                                15|                              498|                  107|                        63|                       11|                                11|                      358|                              636|\n",
      "|2011|                     12|                    56|                                  11|                                    6|                                    2|                245|                                 9|                              255|                   99|                        61|                        6|                                 6|                      174|                              648|\n",
      "|2008|                     24|                    56|                                  24|                                   42|                                    9|                254|                                18|                              151|                  115|                       107|                       42|                                42|                      141|                              535|\n",
      "|2017|                     49|                    29|                                  27|                                   14|                                    0|                322|                                 1|                              232|                   87|                        14|                       14|                                14|                      156|                              300|\n",
      "|2002|                     38|                   102|                                  13|                                   73|                                    1|                268|                                27|                              369|                  178|                       201|                       73|                                73|                      324|                              878|\n",
      "|2021|                      4|                    20|                                  35|                                    5|                                    1|                285|                                 1|                              338|                   74|                        70|                        5|                                 5|                      175|                              252|\n",
      "+----+-----------------------+----------------------+------------------------------------+-------------------------------------+-------------------------------------+-------------------+----------------------------------+---------------------------------+---------------------+--------------------------+-------------------------+----------------------------------+-------------------------+---------------------------------+\n",
      "\n",
      "Number with 23 for all:  1479\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'length_freq_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m full_series_stocks \u001b[38;5;241m=\u001b[39m get_full_series_stocks(df) \u001b[38;5;66;03m#this gets the stocks that have data since 2001\u001b[39;00m\n\u001b[1;32m      3\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfsym_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39misin(full_series_stocks)]\n\u001b[0;32m----> 4\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m \u001b[43mget_features_all_stocks_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiltered_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Stock-Implosion-Prediction-FYP/CreateDataset.py:177\u001b[0m, in \u001b[0;36mget_features_all_stocks_seq\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    164\u001b[0m     row_count_23_values = grouped_df.filter(conditions).count()\n\u001b[1;32m    165\u001b[0m     print(\"Number with 23 for all: \", row_count_23_values)\n\u001b[1;32m    166\u001b[0m #     row_count_23_values = grouped_df.filter(\n\u001b[1;32m    167\u001b[0m #         *[F.size(F.col(col)) == 23 for col in feature_cols]\n\u001b[1;32m    168\u001b[0m #         ).count()\n\u001b[1;32m    169\u001b[0m \n\u001b[1;32m    170\u001b[0m #     print(f\"Number of rows with 23 values in all columns: {row_count_23_values}\")\n\u001b[1;32m    171\u001b[0m     \n\u001b[1;32m    172\u001b[0m #     row_count_23_values = grouped_df.filter(\n\u001b[1;32m    173\u001b[0m #         *[F.size(F.col(col)) > 15 for col in feature_cols]\n\u001b[1;32m    174\u001b[0m #         ).count()\n\u001b[1;32m    175\u001b[0m \n\u001b[1;32m    176\u001b[0m #     print(f\"Number of rows with more than 15 values in all columns: {row_count_23_values}\")\n\u001b[0;32m--> 177\u001b[0m     \n\u001b[1;32m    178\u001b[0m #     length_freq_info = {}\n\u001b[1;32m    179\u001b[0m #     for col_name in feature_cols:\n\u001b[1;32m    180\u001b[0m #         length_freq_info[col_name] = (\n\u001b[1;32m    181\u001b[0m #             grouped_df.select(F.size(col(col_name)).alias(\"length\"))\n\u001b[1;32m    182\u001b[0m #             .groupBy(\"length\")\n\u001b[1;32m    183\u001b[0m #             .count()\n\u001b[1;32m    184\u001b[0m #             .orderBy(\"length\")\n\u001b[1;32m    185\u001b[0m #             .collect()\n\u001b[1;32m    186\u001b[0m #         )\n\u001b[1;32m    187\u001b[0m \n\u001b[1;32m    188\u001b[0m     # Print or use the length and frequency information\n\u001b[1;32m    189\u001b[0m     for col_name, info in length_freq_info.items():\n\u001b[1;32m    190\u001b[0m         print(f\"Column: {col_name}\")\n",
      "\u001b[0;31mNameError\u001b[0m: name 'length_freq_info' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "full_series_stocks = get_full_series_stocks(df) #this gets the stocks that have data since 2001\n",
    "filtered_df = df[df['fsym_id'].isin(full_series_stocks)]\n",
    "filtered_df = get_features_all_stocks_seq(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63645eaf-dfa5-43d5-ab36-e94c6ced8c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_macro_features():\n",
    "    # fred_key = 'bdfdde3b7a21b7d528011d17996b0b8e'\n",
    "    # fred = Fred(api_key=fred_key)\n",
    "    # cpi = fred.get_series(series_id='CPIAUCSL')\n",
    "    # cpi_change = cpi.pct_change()\n",
    "    # unemp = fred.get_series(series_id='UNRATE')\n",
    "    # gdp = fred.get_series(series_id='GDP')\n",
    "    # gdp_change = gdp.pct_change()\n",
    "    # df = pd.DataFrame({'CPI_change': cpi_change,'Unemployment_Rate': unemp,'GDP_change': gdp_change})\n",
    "    # df.to_csv('macro.csv')\n",
    "    df = pd.read_csv('macro.csv')\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_test(df):\n",
    "    print(\"Number of records: \", df.count())\n",
    "    df.show()\n",
    "    df = df.filter(reduce(lambda acc, column: acc & (F.size(col(column)) == 23), df.columns[1:-1], lit(True)))\n",
    "    # print(filtered_df.count())\n",
    "\n",
    "    # average_lengths = df.agg(*[(F.avg(F.size(col(column))).alias(f'avg_length_{column}')) for column in df.columns[1:-1]])\n",
    "    \n",
    "    # test = padded_df.select('ff_non_oper_exp').filter(col('fsym_id')=='RTTY5P-R').collect()[0]\n",
    "    # print(test['ff_non_oper_exp'], len(test['ff_non_oper_exp']))\n",
    "\n",
    "    #need to decide whether to only include stocks that started from 2000, or include just from e.g. 2019\n",
    "    #temporary measure - replace with 0\n",
    "    #try imputer?\n",
    "    #look into masking\n",
    "    features = df.columns[1:-1]\n",
    "    list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "    for f in features:\n",
    "        df = df.withColumn(f, list_to_vector_udf(f))\n",
    "    df.show(2)\n",
    "    vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features_vector\")\n",
    "    df_assembled = vector_assembler.transform(df)\n",
    "    \n",
    "    lr = LogisticRegression(featuresCol=\"features_vector\", labelCol=\"label\")\n",
    "\n",
    "    # pipeline = Pipeline(stages=[vector_assembler, lr])\n",
    "    \n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam, [0.01, 0.1]) \\\n",
    "        .addGrid(lr.elasticNetParam, [0.0, 0.5]) \\\n",
    "        .addGrid(lr.maxIter, [10, 20]) \\\n",
    "        .build()\n",
    "\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "    crossval = CrossValidator(estimator=lr,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=evaluator,\n",
    "                              numFolds=5) \n",
    "\n",
    "    cvModel = crossval.fit(df_assembled)\n",
    "\n",
    "    avg_metrics = cvModel.avgMetrics\n",
    "\n",
    "    for i, acc in enumerate(avg_metrics):\n",
    "        print(f\"Fold {i + 1} - Validation Accuracy: {acc}\")\n",
    "\n",
    "    best_model = cvModel.bestModel\n",
    "\n",
    "    predictions = best_model.transform(df_assembled)\n",
    "    predictions.select('fsym_id', 'label', 'prediction').show(100)\n",
    "    tp = predictions.filter((predictions.label == 1) & (predictions.prediction == 1)).count()\n",
    "    tn = predictions.filter((predictions.label == 0) & (predictions.prediction == 0)).count()\n",
    "    fp = predictions.filter((predictions.label == 0) & (predictions.prediction == 1)).count()\n",
    "    fn = predictions.filter((predictions.label == 1) & (predictions.prediction == 0)).count()\n",
    "    \n",
    "    print(f\"True Positives: {tp}\")\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "\n",
    "#     # Filter and display predictions for label 1\n",
    "#     predictions.filter(predictions['label'] == 1).select(\"fsym_id\", \"label\", \"prediction\").show()\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def test_pandas(df1):\n",
    "    df1 = df1.toPandas()\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "    df = df1[df1.loc[:, ~df1.columns.isin(exclude_columns)].apply(lambda row: all(len(cell) == 23 for cell in row), axis=1)]\n",
    "    print(\"Number of records: \", len(df))\n",
    "    X = df.drop(exclude_columns, axis=1)\n",
    "    y = df['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    lr = LogisticRegression()\n",
    "\n",
    "    param_grid = {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.01, 0.1, 1.0, 10.0],\n",
    "        'max_iter': [100, 200, 300]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    predictions = best_model.predict(X_test)\n",
    "\n",
    "    auc = roc_auc_score(y_test, predictions)\n",
    "\n",
    "    print(f\"Area under the ROC curve (AUC): {auc}\")\n",
    "    print(\"Best model hyperparameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    #extra_df= df[df.apply(lambda row: any(len(cell) != 23 for cell in row), axis=1)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#create_dataset('FF_ADVANCED_DER_AF')\n",
    "\n",
    "train_test(filtered_df)\n",
    "#test_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7337e-6ca2-4212-b9d0-17953b8998d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT MIN(a.p_date) FROM fp_basic_prices a LEFT JOIN sym_ticker_region s ON s.fsym_id = a.fsym_id WHERE s.ticker_region = 'AACQU-US' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f034c3-4ed9-400e-a07b-6d2c6be018d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
