{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98d51d0-085d-4172-85cd-dc0f19412ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6802cbd1-16cf-4e4a-8137-9894bdec78e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='2022_10_22', catalog='spark_catalog', description='FactSet data version for the day', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse'),\n",
       " Database(name='2023_04_01', catalog='spark_catalog', description='FactSet data version for the day', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse'),\n",
       " Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"List available databases and tables\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2886cd22-2c06-4882-b434-e5ed726788c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "spark.sql(\"USE 2023_04_01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca1fc72f-006a-44b5-83fd-21ba1e382ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import when\n",
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "\n",
    "\n",
    "def query(ticker):\n",
    "    query = f\"\"\"SELECT d.ticker_region, a.date\n",
    "                FROM FF_ADVANCED_AF a \n",
    "                LEFT JOIN sym_ticker_region d ON d.fsym_id = a.fsym_id \n",
    "                WHERE d.ticker_region = \"{ticker}-US\"\n",
    "                ORDER BY a.date\n",
    "                \"\"\"\n",
    "\n",
    "    fund_df = spark.sql(query)\n",
    "    fund_df = fund_df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    \n",
    "    return fund_df\n",
    "\n",
    "def get_top_bottom_ten(df):\n",
    "    df = df.sort_values(by='Value')\n",
    "    df=df.dropna()\n",
    "    top10 = df.head(10)\n",
    "    down10 = df.tail(10)\n",
    "    print(top10,down10)\n",
    "    return top10['Metric'].tolist(), down10['Metric'].tolist()\n",
    "\n",
    "def avg_change_df(df, big_string, big_string2, avg_string):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.Ticker, {big_string}\n",
    "                FROM temp_table t  \n",
    "                LEFT JOIN sym_ticker_region s ON t.Ticker = SUBSTRING(s.ticker_region, 1, LENGTH(s.ticker_region)-3) AND SUBSTRING(s.ticker_region, -2, 3) = 'US'\n",
    "                LEFT JOIN FF_ADVANCED_AF a ON s.fsym_id = a.fsym_id AND a.date < t.Implosion_Date\n",
    "                GROUP BY t.Ticker\n",
    "                ORDER BY t.Ticker\n",
    "            \"\"\"\n",
    "    query2 = f\"\"\"\n",
    "                SELECT {avg_string}\n",
    "                FROM temp_table t  \n",
    "                LEFT JOIN sym_ticker_region s ON t.Ticker = SUBSTRING(s.ticker_region, 1, LENGTH(s.ticker_region)-3) AND SUBSTRING(s.ticker_region, -2, 3) = 'US'\n",
    "                LEFT JOIN FF_ADVANCED_AF a ON s.fsym_id = a.fsym_id AND a.date > t.Implosion_Date\n",
    "                GROUP BY t.Ticker\n",
    "                ORDER BY t.Ticker\n",
    "            \"\"\"\n",
    "    df1 = spark.sql(query1)\n",
    "    df2 = spark.sql(query2)\n",
    "    print(df1.show(5))\n",
    "    \n",
    "    df1 = df1.toPandas()\n",
    "    df2 = df2.toPandas()\n",
    "    \n",
    "    non_string_columns = df1.select_dtypes(exclude=['object']).columns\n",
    "    df1 = df1[non_string_columns]\n",
    "    df2 = df2[non_string_columns]\n",
    "    \n",
    "    null_threshold = 200\n",
    "    columns_to_drop = df1.columns[df1.isnull().sum() > null_threshold]\n",
    "    df1 = df1.drop(columns=columns_to_drop)\n",
    "    df2 = df2.drop(columns=columns_to_drop)\n",
    "    # print(\"NULLS:\")\n",
    "    # print(df1.isnull().sum())\n",
    "    # print(df2.isnull().sum())\n",
    "    \n",
    "    percentage_change_df = ((df1 - df2) / df2) * 100\n",
    "    #print(percentage_change_df)\n",
    "    #print(\"LENGTH: \",len(percentage_change_df))\n",
    "    \n",
    "    \n",
    "    metric_dict = {}\n",
    "    for column in percentage_change_df.columns:\n",
    "        percentage_change_df[column] = percentage_change_df[column].replace([np.inf, -np.inf], np.nan)\n",
    "        new_col = percentage_change_df[column].dropna()\n",
    "        mean_val = new_col.mean()\n",
    "        stddev_val = new_col.std()\n",
    "        z_score_threshold = 3.0\n",
    "        new_col = new_col[(new_col >= mean_val - z_score_threshold * stddev_val) &\n",
    "        (new_col <= mean_val + z_score_threshold * stddev_val)]\n",
    "        #if new_col.std() < 1000:\n",
    "        metric_dict[column] = new_col.mean()\n",
    "    #print(metric_dict)\n",
    "    metric_df = pd.DataFrame(list(metric_dict.items()), columns=['Metric', 'Value'])\n",
    "    #metric_df.to_csv('ChangesBeforeImplosionA4yrs.csv', index=False)\n",
    "    return metric_df\n",
    "\n",
    "def pct_change_df(df, big_string, big_string2):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    query1 = f\"\"\"\n",
    "                SELECT {big_string}\n",
    "                FROM temp_table t  \n",
    "                LEFT JOIN sym_ticker_region s ON t.Ticker = SUBSTRING(s.ticker_region, 1, LENGTH(s.ticker_region)-3) AND SUBSTRING(s.ticker_region, -2, 3) = 'US'\n",
    "                LEFT JOIN FF_ADVANCED_AF a ON s.fsym_id = a.fsym_id AND YEAR(a.date) = YEAR(t.Implosion_Date)\n",
    "                ORDER BY t.Ticker\n",
    "            \"\"\"\n",
    "    query2 = f\"\"\"\n",
    "                SELECT {big_string2}\n",
    "                FROM temp_table t  \n",
    "                LEFT JOIN sym_ticker_region s ON t.Ticker = SUBSTRING(s.ticker_region, 1, LENGTH(s.ticker_region)-3) AND SUBSTRING(s.ticker_region, -2, 3) = 'US'\n",
    "                LEFT JOIN FF_ADVANCED_AF b ON s.fsym_id = b.fsym_id AND YEAR(b.date) = YEAR(t.Implosion_Prev4Years)\n",
    "                ORDER BY t.Ticker\n",
    "            \"\"\"\n",
    "    df1 = spark.sql(query1)\n",
    "    df2 = spark.sql(query2)\n",
    "    \n",
    "    df1 = df1.toPandas()\n",
    "    df2 = df2.toPandas()\n",
    "    \n",
    "    non_string_columns = df1.select_dtypes(exclude=['object']).columns\n",
    "    df1 = df1[non_string_columns]\n",
    "    df2 = df2[non_string_columns]\n",
    "    \n",
    "    null_threshold = 200\n",
    "    columns_to_drop = df1.columns[df1.isnull().sum() > null_threshold]\n",
    "    df1 = df1.drop(columns=columns_to_drop)\n",
    "    df2 = df2.drop(columns=columns_to_drop)\n",
    "    # print(\"NULLS:\")\n",
    "    # print(df1.isnull().sum())\n",
    "    # print(df2.isnull().sum())\n",
    "    \n",
    "    percentage_change_df = ((df1 - df2) / df2) * 100\n",
    "    #print(percentage_change_df)\n",
    "    #print(\"LENGTH: \",len(percentage_change_df))\n",
    "    \n",
    "    \n",
    "    metric_dict = {}\n",
    "    for column in percentage_change_df.columns:\n",
    "        percentage_change_df[column] = percentage_change_df[column].replace([np.inf, -np.inf], np.nan)\n",
    "        new_col = percentage_change_df[column].dropna()\n",
    "        mean_val = new_col.mean()\n",
    "        stddev_val = new_col.std()\n",
    "        z_score_threshold = 3.0\n",
    "        new_col = new_col[(new_col >= mean_val - z_score_threshold * stddev_val) &\n",
    "        (new_col <= mean_val + z_score_threshold * stddev_val)]\n",
    "        #if new_col.std() < 1000:\n",
    "        metric_dict[column] = new_col.mean()\n",
    "    #print(metric_dict)\n",
    "    metric_df = pd.DataFrame(list(metric_dict.items()), columns=['Metric', 'Value'])\n",
    "    #metric_df.to_csv('ChangesBeforeImplosionA4yrs.csv', index=False)\n",
    "    return metric_df\n",
    "    \n",
    "    # df['pct_change'] = (df[metric_curr] - df[metric_prev])/df[metric_prev]\n",
    "    # df['pct_change'] = df['pct_change'].replace([np.inf, -np.inf], np.nan) \n",
    "    # df=df.dropna(axis=0)\n",
    "    # mean_val = df['pct_change'].mean()\n",
    "    # stddev_val = df['pct_change'].std()\n",
    "    # z_score_threshold = 3.0\n",
    "    # df = df[\n",
    "    # (df['pct_change'] >= mean_val - z_score_threshold * stddev_val) &\n",
    "    # (df['pct_change'] <= mean_val + z_score_threshold * stddev_val)]\n",
    "    # new_mean = df['pct_change'].mean()\n",
    "    # return new_mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_metric_changes(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Date']).dt.date\n",
    "    df_metrics = spark.sql(\"SELECT * FROM FF_ADVANCED_AF LIMIT 10\")\n",
    "    df_metrics = df_metrics.columns[:5]\n",
    "    result_string = ', '.join('a.' + item for item in df_metrics)\n",
    "    result_string2 = ', '.join('b.' + item for item in df_metrics)\n",
    "    result_list = result_string.split(',')\n",
    "    avg_string =  [f'AVG({element})' for element in result_list]\n",
    "    avg_string = ', '.join(avg_string)\n",
    "    top10s = []\n",
    "    bottom10s = []\n",
    "    for y in range(1,2):\n",
    "        #df['Implosion_Prev4Years'] = df['Implosion_Date'] - pd.DateOffset(years=y)\n",
    "    #print(result_string)\n",
    "        new_df = avg_change_df(df, result_string, result_string2, avg_string)\n",
    "        top10, bottom10 = get_top_bottom_ten(new_df)\n",
    "        top10s.append(top10)\n",
    "        bottom10s.append(bottom10)\n",
    "    return top10s,bottom10s\n",
    "\n",
    "\n",
    "# tops, bottoms = get_metric_changes('imploded_tickers_dates.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4454efe2-9115-402a-bc4c-c795839e6e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['avg(ff_amort_dfd_chrg)',\n",
       "  'avg(ff_amort_intang)',\n",
       "  'avg(ff_amort_cf)',\n",
       "  'avg(ff_assets_intl)']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88bc63eb-7d98-4de8-b665-808509638bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10:  corr(ff_loan_for, FF_PRICE_CLOSE_FP)               -1.000000\n",
      "corr(ff_ins_rsrv, FF_PRICE_CLOSE_FP)               -0.767037\n",
      "corr(ff_shs_repurch_auth_shs, FF_PRICE_CLOSE_FP)   -0.605042\n",
      "corr(ff_com_eq_unearn_comp, FF_PRICE_CLOSE_FP)     -0.591294\n",
      "corr(ff_pol_claims, FF_PRICE_CLOSE_FP)             -0.469525\n",
      "corr(ff_com_eq_apic, FF_PRICE_CLOSE_FP)            -0.372273\n",
      "corr(ff_com_shs_out_secs, FF_PRICE_CLOSE_FP)       -0.362745\n",
      "corr(ff_accel_dep, FF_PRICE_CLOSE_FP)              -0.357423\n",
      "corr(ff_ppe_dep, FF_PRICE_CLOSE_FP)                -0.329758\n",
      "corr(ff_intang_oth_amort, FF_PRICE_CLOSE_FP)       -0.303841\n",
      "dtype: float64\n",
      "Bottom 10:  corr(ff_dps_secs, FF_PRICE_CLOSE_FP)            0.379162\n",
      "corr(ff_loan_brkr, FF_PRICE_CLOSE_FP)           0.394672\n",
      "corr(ff_bps_secs, FF_PRICE_CLOSE_FP)            0.440450\n",
      "corr(ff_loan_bk, FF_PRICE_CLOSE_FP)             0.489035\n",
      "corr(ff_int_inc_deps, FF_PRICE_CLOSE_FP)        0.495577\n",
      "corr(ff_tot_invest_ret, FF_PRICE_CLOSE_FP)      0.497328\n",
      "corr(ff_assets_risk_wght, FF_PRICE_CLOSE_FP)    0.500173\n",
      "corr(ff_pbk_secs, FF_PRICE_CLOSE_FP)            0.509131\n",
      "corr(ff_trade_acct, FF_PRICE_CLOSE_FP)          0.523470\n",
      "corr(ff_mkt_val_secs, FF_PRICE_CLOSE_FP)        0.698171\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def consistent_changes(ticker, big_string):\n",
    "    start_date = pd.to_datetime(\"2009-01-01\")\n",
    "    query1 = f\"\"\"\n",
    "                SELECT b.FF_PRICE_CLOSE_FP, {big_string}\n",
    "                FROM FF_ADVANCED_AF a\n",
    "                LEFT JOIN sym_ticker_region d ON d.fsym_id = a.fsym_id\n",
    "                LEFT JOIN FF_BASIC_QF b ON b.date=a.date AND d.fsym_id=b.fsym_id\n",
    "                WHERE d.ticker_region = \"{ticker}-US\" \n",
    "                AND a.date >= \"{start_date}\"\n",
    "                ORDER BY a.date\n",
    "            \"\"\"\n",
    "    q_df = spark.sql(query1)\n",
    "    print(\"query done\")\n",
    "    q_df = ps.DataFrame(q_df)\n",
    "    non_string_columns = q_df.select_dtypes(exclude=['object']).columns\n",
    "    q_df = q_df[non_string_columns]\n",
    "    print(\"filtering done\")\n",
    "    # null_threshold = 10\n",
    "    # columns_to_drop = q_df.columns[q_df.isnull().sum() > null_threshold]\n",
    "    # q_df = q_df.drop(columns=columns_to_drop)\n",
    "    correlations = q_df.corr()['FF_PRICE_CLOSE_FP']\n",
    "    print(\"corr done\")\n",
    "    return correlations\n",
    "\n",
    "def corr_query(implosion_df, col_string):\n",
    "    df=spark.createDataFrame(implosion_df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    start_date = pd.to_datetime(\"2009-01-01\")\n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.Ticker, {col_string}\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN sym_ticker_region s ON s.ticker_region = CONCAT(t.Ticker, '-US')\n",
    "                LEFT JOIN FF_ADVANCED_QF a ON s.fsym_id = a.fsym_id\n",
    "                LEFT JOIN FF_BASIC_QF b ON b.date=a.date AND s.fsym_id=b.fsym_id\n",
    "                WHERE a.date >= \"{start_date}\"\n",
    "                GROUP BY t.Ticker\n",
    "                ORDER BY t.Ticker\n",
    "            \"\"\"\n",
    "    # query1 = f\"\"\"\n",
    "    #             SELECT t.Ticker, {col_string}\n",
    "    #             FROM temp_table t\n",
    "    #             LEFT JOIN sym_ticker_region s ON s.ticker_region = CONCAT(t.Ticker, '-US')\n",
    "    #             LEFT JOIN FF_BASIC_QF a ON s.fsym_id=a.fsym_id\n",
    "    #             WHERE a.date >= \"{start_date}\"\n",
    "    #             GROUP BY t.Ticker\n",
    "    #             ORDER BY t.Ticker\n",
    "    #         \"\"\"\n",
    "    q_df = spark.sql(query1)\n",
    "    q_df = ps.DataFrame(q_df)\n",
    "    \n",
    "    mean_vals = q_df.mean()\n",
    "    mean_vals = mean_vals.sort_values()\n",
    "    return mean_vals\n",
    "\n",
    "def corr_analysis(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Date']).dt.date\n",
    "    df_metrics = ps.DataFrame(spark.sql(\"SELECT * FROM FF_ADVANCED_QF LIMIT 10\")) #get all the metrics\n",
    "    cols = []\n",
    "    for c in df_metrics.columns:\n",
    "        if df_metrics[c].dtype=='float64':#get all the metrics we can calculate correlations with\n",
    "            cols.append(c)\n",
    "    corr_string = ', '.join('CORR(a.' + item + ', FF_PRICE_CLOSE_FP)' for item in cols) #make a dynamic string for the SQL query\n",
    "    mean_vals=corr_query(df, corr_string)\n",
    "    mean_vals=mean_vals.dropna()\n",
    "    print(\"Top 10: \", mean_vals.head(10))\n",
    "    print(\"Bottom 10: \", mean_vals.tail(10))\n",
    "    \n",
    "    \n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "corr_analysis('imploded_stocks.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ed286246-6e05-4ed4-8b91-6f2a14abe39b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|P_DIVS_EXDATE|   P_DIVS_PD|\n",
      "+-------------+------------+\n",
      "|   2009-02-11|0.0649999976|\n",
      "|   2009-03-13|0.0649999976|\n",
      "|   2009-04-14|0.0649999976|\n",
      "|   2009-05-13|0.0649999976|\n",
      "|   2009-06-12|0.0649999976|\n",
      "|   2009-07-15|0.0649999976|\n",
      "|   2009-08-13|0.0649999976|\n",
      "|   2009-09-14|0.0649999976|\n",
      "|   2009-10-14|0.0649999976|\n",
      "|   2009-11-12|0.0649999976|\n",
      "+-------------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+---------+\n",
      "|P_DIVS_EXDATE|P_DIVS_PD|\n",
      "+-------------+---------+\n",
      "+-------------+---------+\n",
      "\n",
      "+-------------+---------+\n",
      "|P_DIVS_EXDATE|P_DIVS_PD|\n",
      "+-------------+---------+\n",
      "|   2009-06-15|     1.75|\n",
      "+-------------+---------+\n",
      "\n",
      "+-------------+-----------+\n",
      "|P_DIVS_EXDATE|  P_DIVS_PD|\n",
      "+-------------+-----------+\n",
      "|   2009-03-09|0.189999998|\n",
      "|   2009-05-28|0.189999998|\n",
      "|   2009-08-28|0.189999998|\n",
      "|   2009-11-27|0.189999998|\n",
      "|   2010-03-05|0.209999993|\n",
      "|   2010-05-27|0.104999997|\n",
      "|   2010-08-30|0.104999997|\n",
      "|   2010-11-29|0.104999997|\n",
      "|   2011-03-01|0.109999999|\n",
      "|   2011-05-27|0.109999999|\n",
      "+-------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+-----------+\n",
      "|P_DIVS_EXDATE|  P_DIVS_PD|\n",
      "+-------------+-----------+\n",
      "|   2018-03-28|0.139579996|\n",
      "|   2018-06-28|     0.1875|\n",
      "|   2018-09-27|     0.1875|\n",
      "|   2018-12-28|     0.1875|\n",
      "|   2019-03-28|0.200000003|\n",
      "|   2019-06-27|0.200000003|\n",
      "|   2019-09-27|0.200000003|\n",
      "|   2019-12-30|0.200000003|\n",
      "|   2020-03-30|0.209999993|\n",
      "|   2020-06-29|0.209999993|\n",
      "+-------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-------------+---------+\n",
      "|P_DIVS_EXDATE|P_DIVS_PD|\n",
      "+-------------+---------+\n",
      "+-------------+---------+\n",
      "\n",
      "+-------------+---------+\n",
      "|P_DIVS_EXDATE|P_DIVS_PD|\n",
      "+-------------+---------+\n",
      "+-------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m t_list \u001b[38;5;241m=\u001b[39m get_all_stocks()\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m t_list:\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mget_total_annual_returns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [13], line 12\u001b[0m, in \u001b[0;36mget_total_annual_returns\u001b[0;34m(ticker)\u001b[0m\n\u001b[1;32m      3\u001b[0m query1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m            SELECT P_DIVS_EXDATE, P_DIVS_PD\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m            FROM fp_basic_dividends a\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m            ORDER BY a.P_DIVS_EXDATE\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     11\u001b[0m df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(query1)\n\u001b[0;32m---> 12\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def get_total_annual_returns(ticker):\n",
    "    start_date = pd.to_datetime(\"2009-01-01\")\n",
    "    query1 = f\"\"\"\n",
    "                SELECT P_DIVS_EXDATE, P_DIVS_PD\n",
    "                FROM fp_basic_dividends a\n",
    "                LEFT JOIN sym_ticker_region d ON d.fsym_id = a.fsym_id\n",
    "                WHERE d.ticker_region = \"{ticker}-US\" \n",
    "                AND a.P_DIVS_EXDATE >= \"{start_date}\"\n",
    "                ORDER BY a.P_DIVS_EXDATE\n",
    "            \"\"\"\n",
    "    df = spark.sql(query1)\n",
    "    df.show(10)\n",
    "\n",
    "def get_ticker_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Date']).dt.date\n",
    "    return df\n",
    "\n",
    "def get_all_stocks():\n",
    "    query = f\"\"\"SELECT s.ticker_region, sc.fref_listing_exchange FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    ticker_list = [row.ticker_region for row in df.collect()]\n",
    "    return ticker_list\n",
    "\n",
    "# df = get_ticker_df('imploded_stocks.csv')\n",
    "# t_list = get_all_stocks()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700de0e-2f78-48aa-8c16-e12e167d67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_bottom_five(df):\n",
    "    df = df.sort_values(by='Value')\n",
    "    df=df.dropna()\n",
    "    top5 = df.head(10)\n",
    "    down5 = df.tail(10)\n",
    "    print(top5)\n",
    "    print(down5)\n",
    "    print(len(df))\n",
    "\n",
    "\n",
    "df = pd.read_csv('Avg_Correlations.csv', index_col=None)\n",
    "get_top_bottom_five(df)\n",
    "#YOU'VE DONE WORST CHANGES NOW FIND OUT WHICH ONES DECREASE CONSISTENTLY\n",
    "#ALSO FIGURE OUT MEANS BEFORE PERIOD AND AFTER PERIOD USING QUARTERLY AND COMPARE DIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007cd3b-3e7f-4dfa-87a8-3c329fa9519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c66b85-a467-4538-91ee-a400c24c56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b54c9-5ddc-4459-afff-5247cc6b7b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
