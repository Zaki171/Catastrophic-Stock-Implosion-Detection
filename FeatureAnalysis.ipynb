{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe610780-dc3d-4ac8-9dd1-914ed92696c0",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6802cbd1-16cf-4e4a-8137-9894bdec78e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop-3.2.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.7-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "2024-02-04 21:24:30,609 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-02-04 21:24:33,544 WARN spark.ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Database(name='2023_11_01', description='FactSet data snapshot for 2023_11_01', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_01'),\n",
       " Database(name='2023_11_02', description='FactSet data snapshot for 2023_11_02', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_02'),\n",
       " Database(name='2023_11_03', description='FactSet data snapshot for 2023_11_03', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_03'),\n",
       " Database(name='2023_11_14', description='FactSet data snapshot for 2023_11_14', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_14'),\n",
       " Database(name='2023_11_19', description='FactSet data snapshot for 2023_11_19', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_19'),\n",
       " Database(name='2023_11_22', description='FactSet data snapshot for 2023_11_22', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_22'),\n",
       " Database(name='2024_01_25', description='FactSet data snapshot for 2024_01_25', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2024_01_25'),\n",
       " Database(name='2024_02_02', description='FactSet data snapshot for 2024_02_02', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2024_02_02'),\n",
       " Database(name='2024_02_03', description='FactSet data snapshot for 2024_02_03', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2024_02_03'),\n",
       " Database(name='default', description='Default Hive database', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"List available databases and tables\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2886cd22-2c06-4882-b434-e5ed726788c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.sql(\"USE 2023_11_02\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd18af3-d36d-4088-aa8c-6c410c3322ae",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694d984f-e6b5-42b9-afb9-1c9d98a7fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_all_stocks_df():\n",
    "    query = f\"\"\"SELECT s.ticker_region, s.fsym_id FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_not_null_cols(df, table='FF_ADVANCED_DER_AF'):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query1 = f\"\"\"SELECT t.fsym_id AS fsym_id2, a.*\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id\n",
    "                WHERE a.date > '2000-01-01'\n",
    "                ORDER BY t.fsym_id, a.date\n",
    "            \"\"\"\n",
    "    #we get all the available dates per stock, so these null values are only within the timeframe available\n",
    "    q_df = spark.sql(query1)\n",
    "    q_df = q_df.drop('date', 'adjdate', 'fsym_id2', 'fsym_id')\n",
    "    num_rows = q_df.count()\n",
    "    column_types = q_df.dtypes\n",
    "    good_cols = []\n",
    "    selected_columns = [F.col(c) for c, c_type in zip(q_df.columns, column_types) if c_type[1] == 'double']\n",
    "    q_df = q_df.select(selected_columns)\n",
    "    count_df = q_df.select( [(F.count(F.when(F.isnan(c) | F.col(c).isNull(), c))/num_rows).alias(c) for c in q_df.columns])\n",
    "    count_dict = count_df.first().asDict()\n",
    "    filtered_keys = [key for key, value in count_dict.items() if value <= 0.25]\n",
    "    return filtered_keys\n",
    "#     for c, c_type in zip(q_df.columns, column_types):\n",
    "#         if c_type[1] == 'double':\n",
    "#             null_count = F.sum(F.when(F.isnan(F.col(c)) | F.col(c).isNull(), 1).otherwise(0))\n",
    "#             null_pct = (null_count / num_rows).alias(f\"{c}_null_pct\")\n",
    "#             q_df_agg = q_df.agg(null_pct)\n",
    "#             actual_pct = q_df_agg.collect()[0][0]\n",
    "#             if actual_pct < 0.25:\n",
    "#                 good_cols.append(c)\n",
    "            \n",
    "#     return good_cols\n",
    "\n",
    "\n",
    "def write_features_file(data_list, csv_file_path='features.csv'):\n",
    "    data_list = [data_list]\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row in data_list:\n",
    "            writer.writerow(row)\n",
    "    print(\"Features written: \", data_list[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61704c74-e53c-424b-9a8f-78ea6f9d34ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "2024-02-04 21:25:34,737 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ff_assets_com_eq', 'ff_assets_eq', 'ff_assets_gr', 'ff_bps_gr', 'ff_capex_assets', 'ff_capex_ps_cf', 'ff_cash_div_cf', 'ff_cash_roce', 'ff_cf_sales', 'ff_com_eq_gr', 'ff_com_eq_tcap', 'ff_debt_com_eq', 'ff_debt_entrpr_val', 'ff_debt_eq', 'ff_debt_lt_cf', 'ff_debt_st_x_curr_port', 'ff_dil_adj', 'ff_earn_yld', 'ff_entrpr_val_sales', 'ff_eps_basic_gr', 'ff_fix_assets_com_eq', 'ff_free_ps_cf', 'ff_inc_adj', 'ff_inc_sund', 'ff_int_exp_oth', 'ff_invest_cap', 'ff_invest_lt', 'ff_ltd_com_eq', 'ff_ltd_tcap', 'ff_min_int_tcap', 'ff_mkt_val_gr', 'ff_net_inc_basic_aft_xord', 'ff_net_inc_basic_beft_xord', 'ff_net_inc_bef_xord_gr', 'ff_net_inc_dil', 'ff_net_inc_dil_aft_xord', 'ff_non_oper_exp', 'ff_oper_inc_aft_unusual', 'ff_oper_inc_gr', 'ff_oper_inc_tcap', 'ff_oper_ps_net_cf', 'ff_pfd_stk_tcap', 'ff_roa_ptx', 'ff_roce', 'ff_roic', 'ff_sales_gr', 'ff_sales_ps_gr', 'ff_tcap_assets', 'ff_tot_debt_tcap_std', 'ff_ut_gross_inc', 'ff_ut_operation_exp', 'ff_wkcap', 'ff_wkcap_pct', 'ff_xord', 'ff_xord_disc', 'ff_invest_receiv_lt', 'ff_ebit_bef_unusual', 'ff_ebitda_bef_unusual', 'ff_eps_dil_aft_xord', 'ff_eps_dil_gr', 'ff_psales_dil', 'ff_std_debt', 'ff_tang_assets_debt', 'ff_net_inc_dil_bef_unusual', 'ff_bk_oper_inc_oth', 'ff_bk_oper_inc_tot', 'ff_commiss_inc_net', 'ff_cf_roic', 'ff_div_yld', 'ff_div_yld_secs', 'ff_liabs_lease', 'ff_fcf_yld', 'ff_accr_exp', 'GDP', 'Unemployment_Rate', 'CPI']\n"
     ]
    }
   ],
   "source": [
    "from CreateDataset import get_tabular_dataset, get_feature_col_names, get_not_null_cols, get_tabular_dataset_qf\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "# import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def plot_nulls(df):\n",
    "    null_counts = df.agg(*[\n",
    "    (1 - (F.count(c) / F.count('*'))).alias(c + '_nulls') for c in df.columns])\n",
    "    null_counts_pd = null_counts.toPandas().transpose()\n",
    "    null_counts_pd.columns = ['null_percentage']\n",
    "\n",
    "    # Plot the bar chart\n",
    "    null_counts_pd.plot(kind='bar', legend=False, figsize=(20, 6))\n",
    "    plt.title('Percentage of Null Values in Each Column')\n",
    "    plt.ylabel('Percentage of Null Values')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_nulls_per_year(df):\n",
    "    # Extract year from the date_column\n",
    "    df = df.withColumn('Year', F.year('date'))\n",
    "    \n",
    "    # Group by year and sum null values for each column\n",
    "    nulls_per_year = df.groupBy('Year').agg(*[\n",
    "        (1 - (F.count(c) / F.count('*'))).alias(c + '_nulls') for c in df.columns\n",
    "    ]).toPandas()\n",
    "\n",
    "    # Plot\n",
    "    nulls_per_year.plot(kind='bar', x='Year', figsize=(20, 6))\n",
    "    plt.title('Total Null Values per Year')\n",
    "    plt.ylabel('Total Null Values')\n",
    "    plt.xlabel('Year')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def get_df(fn, all_feats=False, imploded_only=False, prediction=False):\n",
    "    df = get_tabular_dataset_qf(fn, all_feats=all_feats, imploded_only=imploded_only, prediction=prediction)\n",
    "\n",
    "    \n",
    "    return df\n",
    "\n",
    "def write_features_file(data_list, csv_file_path='features.csv'):\n",
    "    data_list = [data_list]\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row in data_list:\n",
    "            writer.writerow(row)\n",
    "    print(\"Features written: \", data_list[0])\n",
    "\n",
    "df = get_df('imploded_stocks_price.csv', all_feats=True, imploded_only=False, prediction=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4d8fb45-fc37-4eba-b9cd-1e2593b60d13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+----------------+-----------------+-----------------+-----------------+-----------------+-------------------+----------------+----------------+----------------+-----------------+-----------------+----------------+------------------+----------------+--------------------+----------------------+----------+-----------+-------------------+-----------------+--------------------+--------------------+----------+-----------+--------------+-------------+------------+----------------+----------------+----------------+-------------+-------------------------+--------------------------+----------------------+--------------+-----------------------+--------------------+-----------------------+------------------+----------------+-------------------+---------------+----------------+----------------+----------------+----------------+----------------+----------------+--------------------+---------------+-------------------+--------+------------------+-------+------------+-------------------+-------------------+---------------------+-------------------+-----------------+-------------+----------------+-------------------+--------------------------+------------------+------------------+------------------+----------------+----------+---------------+--------------+----------+-----------+-----------+-----------------+------------+--------------------+\n",
      "| fsym_id|      date|ff_assets_com_eq|     ff_assets_eq|     ff_assets_gr|        ff_bps_gr|  ff_capex_assets|     ff_capex_ps_cf|  ff_cash_div_cf|    ff_cash_roce|     ff_cf_sales|     ff_com_eq_gr|   ff_com_eq_tcap|  ff_debt_com_eq|ff_debt_entrpr_val|      ff_debt_eq|       ff_debt_lt_cf|ff_debt_st_x_curr_port|ff_dil_adj|ff_earn_yld|ff_entrpr_val_sales|  ff_eps_basic_gr|ff_fix_assets_com_eq|       ff_free_ps_cf|ff_inc_adj|ff_inc_sund|ff_int_exp_oth|ff_invest_cap|ff_invest_lt|   ff_ltd_com_eq|     ff_ltd_tcap| ff_min_int_tcap|ff_mkt_val_gr|ff_net_inc_basic_aft_xord|ff_net_inc_basic_beft_xord|ff_net_inc_bef_xord_gr|ff_net_inc_dil|ff_net_inc_dil_aft_xord|     ff_non_oper_exp|ff_oper_inc_aft_unusual|    ff_oper_inc_gr|ff_oper_inc_tcap|  ff_oper_ps_net_cf|ff_pfd_stk_tcap|      ff_roa_ptx|         ff_roce|         ff_roic|     ff_sales_gr|  ff_sales_ps_gr|  ff_tcap_assets|ff_tot_debt_tcap_std|ff_ut_gross_inc|ff_ut_operation_exp|ff_wkcap|      ff_wkcap_pct|ff_xord|ff_xord_disc|ff_invest_receiv_lt|ff_ebit_bef_unusual|ff_ebitda_bef_unusual|ff_eps_dil_aft_xord|    ff_eps_dil_gr|ff_psales_dil|     ff_std_debt|ff_tang_assets_debt|ff_net_inc_dil_bef_unusual|ff_bk_oper_inc_oth|ff_bk_oper_inc_tot|ff_commiss_inc_net|      ff_cf_roic|ff_div_yld|ff_div_yld_secs|ff_liabs_lease|ff_fcf_yld|ff_accr_exp|        GDP|Unemployment_Rate|         CPI|Implosion_Start_Date|\n",
      "+--------+----------+----------------+-----------------+-----------------+-----------------+-----------------+-------------------+----------------+----------------+----------------+-----------------+-----------------+----------------+------------------+----------------+--------------------+----------------------+----------+-----------+-------------------+-----------------+--------------------+--------------------+----------+-----------+--------------+-------------+------------+----------------+----------------+----------------+-------------+-------------------------+--------------------------+----------------------+--------------+-----------------------+--------------------+-----------------------+------------------+----------------+-------------------+---------------+----------------+----------------+----------------+----------------+----------------+----------------+--------------------+---------------+-------------------+--------+------------------+-------+------------+-------------------+-------------------+---------------------+-------------------+-----------------+-------------+----------------+-------------------+--------------------------+------------------+------------------+------------------+----------------+----------+---------------+--------------+----------+-----------+-----------+-----------------+------------+--------------------+\n",
      "|B00FG1-R|2014-03-31|1.00616433864516| 1.00616433864516|             null|             null|             null| 0.0406633207342084|             0.0|            null|            null|             null|            100.0|             0.0|              null|             0.0|                 0.0|                   0.0|       0.0|       null|               null|             null|    100.616433864516|  -0.108928601440476|       0.0|        0.0|           0.0|       30.498|         0.0|             0.0|             0.0|             0.0|         null|                    -2.46|                     -2.46|                  null|         -2.46|                  -2.46|                 0.0|                  -2.46|              null|            null|-0.0682652807062678|            0.0|            null|            null|            null|            null|            null|99.3873427621717|                 0.0|          -2.46|                0.0|  -0.188|-0.616433864515706|    0.0|         0.0|                0.0|              -2.46|               -2.169|-0.0774239698190035|             null|         null|            null|               null|                     -2.46|               0.0|               0.0|               0.0|            null|      null|           null|           0.0|      null|      0.188|0.006057606|              5.6|-0.003084609|                null|\n",
      "|B00FG1-R|2014-06-30|7.94730593607306| 7.94730593607306|             null|             null|             null| 0.0853514342156653|7198.07271134472|            null|58.6738627602159|             null|  12.739965095986|684.931506849315|          0.256353|684.931506849315|             297.707|                   0.0|       0.0|       null|               null|             null|    76.3127853881279|   -10.4263134191526|       0.0|        0.0|          0.36|        343.8|     299.996|684.931506849315| 87.260034904014|             0.0|         null|                    5.417|                     5.417|                  null|         5.417|                  5.417|                 0.0|                  4.374|              null|            null| 0.0026751002611842|            0.0|            null|            null|            null|            null|            null|98.7669926341312|     87.260034904014|          4.374|              1.697|   8.159|  2.37318208260617|    0.0|         0.0|            299.996|              4.374|                4.658|   0.17048256605688|             null|         null|             0.0|   116.030666666667|                     5.417|               0.0|               0.0|               0.0|            null|      null|           null|           0.0|      null|        0.0|0.006057606|              5.6|-0.003084609|                null|\n",
      "|B00FG1-R|2014-09-30|            null|-7.61645371467254|             null|             null|             null|  0.777880186187803|1793.05182341651|            null|53.0046120455779|             null|-13.2145093478844|            null|          0.338172|            null|             104.954|                   0.0|       0.0|       null|               null|             null|                null|   -4.98888589760678|       0.0|        0.0|         0.824|      357.728|     264.913|            null|113.214509347884|             0.0|         null|                    9.308|                     9.308|                  null|         9.308|                  9.308|                 0.0|                  7.519|              null|            null|  0.198104265522143|            0.0|            null|            null|            null|            null|            null|99.3564693302226|    113.214509347884|          7.519|               4.07|  15.553|  4.34771670095715|    0.0|         0.0|            264.913|              7.519|                8.105|    0.2928771447713|             null|         null|             0.0|               88.9|                     9.308|               0.0|               0.0|               0.0|            null|      null|           null|           0.0|      null|        0.0|0.006057606|              5.6|-0.003084609|                null|\n",
      "|B00FG1-R|2014-12-31|            null|-3.27492871571912|             null|             null| 11.9849930575122|  0.568918477942293|1752.46913580247|            null| 30.259302666276|             null|-30.8675492100476|            null|          0.428687|            null|             104.822|                   0.0|       0.0|    3.98829|            23.8747|             null|                null|   -4.92090328462802|       0.0|        0.0|         1.493|      389.707|      234.93|            null|130.867549210048|             0.0|         null|                   15.239|                    15.239|                  null|        15.239|                 15.239|               -1.04|                 14.679|              null|6.45407960339434|  0.103266288710819|            0.0|            null|            null|            null|            null|            null|98.9227086617371|    130.867549210048|         15.719|              8.275|  21.846|  5.60574996086804|    0.0|         0.0|             234.93|             15.719|               17.101|  0.468913877730683|             null|      13.9242|             0.0|   77.2452941176471|                    15.967|               0.0|               0.0|               0.0|            null|      null|           null|           0.0|  -94.1462|        0.0|0.006057606|              5.6|-0.003084609|                null|\n",
      "|B00FG1-R|2015-03-31|            null|-3.59169479116444| 1210.94635990354|-453.765614093575| 11.4199916972633|5.15091983005358E-4|56.5513300083126|            null|62.9739898576804|-467.243753688766| -28.141347443957|            null|          0.412477|            null|                 0.0|                   0.0|       0.0|    6.36062|            15.3795|             null|                null|     0.1821001657566|       0.0|        0.0|         1.965|      397.998|     234.939|            null|128.141347443957|             0.0|         null|                   16.709|                    16.709|                  null|        16.709|                 16.709|-3.5527136788005E-15|                 18.674|              null|11.6297066819431|  0.512425624505095|            0.0|18.7563371465922|            null|21.7845674171988|            null|            null|98.9363050833132|    128.141347443957|         18.674|              7.481|   30.09|  7.56033949919346|    0.0|         0.0|            234.939|             18.674|               20.121|  0.506274820237443|             null|       9.2868|             0.0|   78.8778431372549|                    16.709|               0.0|               0.0|               0.0|12.4383891564915|   3.49218|        3.49218|           0.0|  -86.2236|        0.0|0.001821089|              5.0|-0.001075553|                null|\n",
      "|B00FG1-R|2015-06-30|            null|-2.08944638668407|  20.015972788803|-522.564054203058| 10.3960111641445|0.00602588780770543|530.536189631132|            null|67.8759894459103|-556.486301369863|-48.9141523489391|            null|          0.432256|            null|              90.475|                   0.0|       0.0|    7.72629|            13.1018| 241.176470588235|                null|   -3.10760674236884|       0.0|        0.0|          4.93|      408.759|     234.249|            null|148.914152348939|             0.0|     -6.06422|                   19.583|                    19.583|      261.510060919328|        19.583|                 19.583|  -0.530000000000005|                 24.734|  477.594878829447| 16.434133560362|   0.62562370165123|            0.0|14.7270120570654|            null|16.1685661855084|348.059624775122|322.609909611486| 97.844008368321|    148.914152348939|         25.264|              4.768|  18.657|  4.56430317130632|    0.0|         0.0|            234.249|             25.264|               26.901|  0.581305226297022| 241.176470588235|      7.50043|             0.0|   68.6324954821751|                    19.954|               0.0|               0.0|               0.0|12.6608013458081|   4.76389|        4.76389|           0.0|  -51.7462|        0.0|0.001821089|              5.0|-0.001075553|                null|\n",
      "|B00FG1-R|2015-09-30|            null|-2.25949020037914| 20.1691455234762|-281.606763880331| 4.54649461590198| 0.0280122419903624|55.5023303987571|            null|62.4885389137587|-305.074885767473|-45.8967002466366|            null|          0.514897|            null|                 0.0|                   0.0|       0.0|    12.4753|            9.10636| 103.655172413793|                null|   0.337136941543052|       0.0|        0.0|          7.18|      417.213|     234.249|            null|145.896700246637|             0.0|     -27.7663|                   20.283|                    20.283|       117.90932531156|        20.283|                 20.283|                 0.0|                 27.463|  265.248038302966|20.8814202817266|  0.739645487315172|            0.0|17.6564384363473|            null|18.5340561410482|151.505697232773|132.750979530007|96.4290914637952|    145.896700246637|         27.463|              4.963|  27.833|  6.67117275827934|    0.0|         0.0|            234.249|             27.463|               29.112|  0.590615700925697| 103.655172413793|        4.558|             0.0|   71.0798422868408|                    20.283|               0.0|               0.0|               0.0|17.2258275146108|   7.83517|        7.83517|           0.0|   -41.747|        0.0|0.001821089|              5.0|-0.001075553|                null|\n",
      "|B00FG1-R|2015-12-31|            null|-2.27764642520533| 7.34888349058638|-48.0518554671434|0.483800029321214|   0.02515034316246|64.8342840978738|            null|56.8568058757304|-54.3522898256756|-44.8533674751184|            null|          0.456084|            null|              -0.099|                   0.0|       0.0|    10.2219|            9.40306| 7.22968650031991|                null|  0.0157480736700126|       0.0|        0.0|         7.189|       413.96|     234.258|            null|144.853367475118|             0.0|      4.30427|                   17.273|                    17.273|      13.3473325021327|        17.273|                 17.273|  -0.299999999999997|                 24.462|  57.5291049048922|23.2300222243695|  0.441208045501628|            0.0|18.1352091502388|            null|18.3777609382991|36.6319953120422|29.2545875742045|97.8855621396919|    144.853367475118|         24.762|               7.09|  34.154|  8.25055560923761|    0.0|         0.0|            234.258|             24.762|               26.425|  0.502803098894875| 7.22968650031991|      5.24806|             0.0|   70.5265703302843|                    17.483|               0.0|               0.0|               0.0|19.5469018884687|   6.74157|        6.74157|           0.0|  -11.7963|        0.0|0.001821089|              5.0|-0.001075553|                null|\n",
      "|B00FG1-R|2016-03-31|            null|-2.40001217777335| 7.78120548776093|-54.7276105887168|0.569215759988376| 0.0127673620855879|67.0748423649822|            null|59.8812553011026|-61.2980125354905|-43.0750045898794|            null|          0.492416|            null|                 0.0|                   0.0|       0.0|    11.8296|            8.35773| 8.88235294117647|                null|   0.311942427631015|       0.0|        0.0|         7.229|      419.401|     234.408|            null|143.075004589879|             0.0|     -13.1818|                   19.094|                    19.094|      14.2737446884912|        19.094|                 19.094| 3.5527136788005E-15|                 26.323|  40.9606940130663|24.7524445578337|  0.751645724605964|            0.0|18.2935816695699|            null|18.6525797070953|19.5779486340586|14.7762112698879|96.7300076802613|    143.075004589879|         26.323|              6.021|   40.63|  9.68762592363871|    0.0|         0.0|            234.408|             26.323|               27.963|  0.555307543649692| 8.88235294117647|      4.44295|             0.0|   72.2561819024161|                    19.094|               0.0|               0.0|               0.0|21.4042346516206|   8.06794|        8.06794|           0.0|   -12.634|        0.0| 0.01041358|              4.7| 0.002524522|                null|\n",
      "|B00FG1-R|2016-06-30|            null|-3.58230804683899| 9.77006266666028| 41.1400260595043|0.848703176313069| 0.0437939586570495|78.9412952545795|            null|51.1547258909467| 35.9746125106906|-28.9079983469917|            null|          0.460343|            null|             -29.836|                   0.0|       0.0|    9.06255|            8.17988|-24.5344827586207|                null| -0.0282090157790952|       0.0|        0.0|         7.634|      306.965|         0.0|            null|98.2270808822367|             0.0|      4.63936|                   16.254|                    16.254|     -16.9994382883113|        16.254|                 16.254|              -2.435|                 23.888|  4.19173527549082|23.6820533433899|  0.457535162417042|            0.0|16.6381391867158|            null|20.3720987419732|16.6083514970747|5.73805151776819|96.5648455456167|    128.907998346992|         26.323|               7.72|    65.8|   14.859008782171|    0.0|         0.0|                0.0|             26.323|               28.465|  0.437508914573868|-24.5689655172414|      5.64145|23.8006313480788|   80.3343131724715|                   17.9585|               0.0|               0.0|               0.0|23.3053523425231|   6.90704|        6.90704|           0.0|    2.5487|        0.0| 0.01041358|              4.7| 0.002524522|                null|\n",
      "|B00FG1-R|2016-09-30|            null| -19.092735863752| 69.9738595627544| 83.4408239492082|0.750325666903268| 0.0655883216698645|771.386086886629|            null|51.3761278467161| 79.8847963569329|-7.23016856252581|            null|          0.400791|            null|-0.00500000000000966|                   0.0|       0.0|    10.2149|            8.74717|-10.2438198442262|                null|   -4.27682488860883|       0.0|        0.0|         7.696|      473.076|         0.0|            null|96.0307091639449|33.0868716447047|      39.7344|                   20.874|                    20.874|      2.91377015234432|        20.874|                 20.874|               -1.31|                  26.06|-0.338637439464006|19.6677553778579|  0.653247519073581|           null|12.1541644942928|            null|16.5103691048637|30.6105388058896|13.6757084735152|72.4408292471995|    107.230168562526|          27.37|             12.814|  59.998|   11.262154146488|    0.0|         0.0|                0.0|              27.37|                32.51|   0.52901492524604|-10.4300711141212|      4.79949|10.4443176288122|   128.735877659481|                    21.791|               0.0|               0.0|               0.0|18.8199562164645|   8.32492|        8.32492|           0.0|  -20.2717|        0.0| 0.01041358|              4.7| 0.002524522|                null|\n",
      "|B00FG1-R|2016-12-31|            null|-22.3042752370163| 76.9038689814662| 85.1141023124192| 2.17970138879606|  0.278249379926283|56.9113524877365|            null|55.5126916717995| 81.9351016561196|-6.23303161114446|            null|          0.405581|            null| 0.00499999999999901|                   0.0|       0.0|    11.6071|            7.52407| 17.8997613365155|                null|-0.00346229812831984|       0.0|        0.0|         7.874|      498.469|         0.0|            null|98.8623630217809|33.4270524201266|      3.40202|                   24.738|                    24.738|      43.2177386672842|        24.738|                 24.738|  -0.100000000000005|                 35.784|  44.9155964784751|21.5374266212999|  0.740191583998528|            0.0|13.9401826764768|            null|17.7460383218859| 65.372862274165|35.6533518897916|71.9304131634876|    106.233031611144|         35.884|             17.607|  81.441|  15.1339910393899|    0.0|         0.0|                0.0|             35.884|               41.118|   0.59069193860949| 17.4821002386635|       4.0687|6.93820789784406|    130.86631390213|                    24.808|               0.0|               0.0|               0.0|  21.83578119503|   9.34066|        9.34066|           0.0|  -20.8896|        0.0| 0.01041358|              4.7| 0.002524522|                null|\n",
      "|B00FG1-R|2017-03-31|            null|-26.2049005557692| 72.9087432740054| 86.9619171423297| 4.71324376814069|  0.463345617653013|68.0597014925373|            null|57.0547480860492| 84.1639128292842|-5.55064045105138|            null|          0.351302|            null|             -39.664|                  11.6|       0.0|    9.94385|             7.3302|  5.6906176841347|                null|   0.258152081423156|       0.0|        0.0|         7.984|      503.818|         0.0|            null| 103.30003996756|34.9339759185748|      38.9312|                   24.608|                    24.608|      28.8781816277365|        24.608|                 24.608| 3.5527136788005E-15|                 36.041|  36.9182843900771|24.3720630633777|   1.28045431719727|            0.0|15.3108956075299|            null|18.7331499893308| 65.468275465813|35.4204075710588|68.7502667747994|    105.550640451051|         36.041|             15.769|  52.209|  10.1294483312573|    0.0|         0.0|                0.0|             36.041|               41.393|  0.585709609041216| 5.47451827840806|      4.28566|2.13224711273521|   137.804925123202|                    24.608|               0.0|               0.0|               0.0|27.6359130390514|   8.07425|        8.07425|           0.0|  -17.5726|        0.0| 0.01749353|              4.1| 0.002106889|                null|\n",
      "|B00FG1-R|2017-06-30|            null|-31.9355044428383| 63.7994949649136| 83.6236791525577| 8.05812382264646|  0.636552344593331|82.1276830546819|            null|58.1527058030772| 81.6260848507574|-4.51560139225123|            null|          0.365611|            null|                 0.0|                  11.6|       0.0|    11.3262|            6.39251| 15.1245145076537|                null|  -0.468330316388659|       0.0|        0.0|         7.886|      509.283|         0.0|            null|102.288613757792|33.4138376564411|     -4.95794|                   21.204|                    21.204|      30.4540420819491|        21.204|                 21.204|  -0.330999999999996|                 35.017|  34.2856057440261|25.8489910402144|  0.874716573294501|            0.0|16.7742244801969|            null| 22.401034979565|53.2969330283578| 35.165536007791|69.3442764808861|    104.515601392251|         35.348|             15.504|  20.958|  4.02355231405133|    0.0|         0.0|                0.0|             35.348|               41.058|  0.503242083246598| 15.0171428571429|      3.52732|2.13077053070881|   137.977494654705|                   21.4357|               0.0|               0.0|               0.0|36.1233350648332|   9.12821|        9.12821|           0.0|  -21.6385|        0.0| 0.01749353|              4.1| 0.002106889|                null|\n",
      "|B00FG1-R|2017-09-30|            null|-47.3709424248132| 2.59214537661779| 58.8128229851684| 9.67517896503141|  0.357107211478435|68.2123741153193|            null|60.1902464347879| 58.6505010644374|-3.01186250612225|            null|          0.348419|            null|                 0.0|                  11.6|       0.0|    10.9743|            6.25395| 18.8455008488964|                null|  -0.275942169124609|       0.0|        0.0|         7.748|      517.209|         0.0|            null|100.818253849689| 32.668127811743|       6.3724|                   26.347|                    26.347|      26.2192200823992|        26.347|                 26.347| -0.0280000000000058|                  40.42|  47.7822433321154|27.9346607187094|  0.718958142556743|            0.0|15.5407341879372|            null|19.5695178660689|35.2259822848058|26.5572161873726|70.0894792021493|    103.011862506122|         40.448|              15.93|  16.208|  3.06500078478241|    0.0|         0.0|                0.0|             40.448|               46.058|  0.624913901489262| 18.1285444234404|      3.54161|2.12947189097104|   138.503238265876|                   26.3666|               0.0|               0.0|               0.0|30.6909627026563|   8.66667|        8.66667|           0.0|  -2.33414|        0.0| 0.01749353|              4.1| 0.002106889|                null|\n",
      "|B00FG1-R|2017-12-31|            null|-49.8075364667747|-1.41419272051649| 56.0309541286687| 10.7710663683818|  0.429008279731793|75.9167043911272|            null| 53.138484206095| 55.8523642001073|-2.77311160425855|            null|          0.346281|            null|    3.69400000000002|                   0.0|       0.0|    10.5465|            6.21955|-16.2618083670715|                null|  -0.555876017223481|       0.0|        0.0|         9.745|      533.985|         0.0|            null|102.773111604259|32.1924773167786|      15.5789|                   20.909|                    20.909|     -15.4782116581777|        20.909|                 20.909|-3.5527136788005E-15|                 36.737|  2.37710400178353| 27.823628004532|  0.509177690829866|            0.0|15.4612029508373|            null| 18.028502964781|7.81113236295264|7.02919006656155|72.3998372991662|    102.773111604259|         36.737|              19.28|  34.113|  6.38838169611506|    0.0|         0.0|                0.0|             36.737|               43.896|  0.495642287595981|-16.0995429151854|      3.46839|             0.0|   134.394935795464|                    20.909|               0.0|               0.0|               0.0|27.5934811623569|   8.87828|        8.87828|           0.0|  -4.97528|        0.0| 0.01749353|              4.1| 0.002106889|                null|\n",
      "|B00FG1-R|2018-03-31|            null|-45.2359662195728| -2.8299470718798| 43.8944279812563| 8.77553261585768| 0.0935929394225204|85.1558469149291|            null|51.4514592670092| 43.7100213219616|-3.07708769623504|            null|           0.36757|            null|                -9.7|                   0.0|       0.0|    11.1463|            5.68022|-26.0862157096609|                null|   0.285182634794905|       0.0|        0.0|         9.948|      523.352|         0.0|            null|103.077087696235|32.6596630948196|     -14.1029|                   18.276|                    18.276|     -25.7314694408322|        18.276|                 18.276|  -0.483000000000001|                 35.205|-0.979440082128671|28.3214738837341|    1.0430889297239|            0.0|15.1608468815621|            null|16.8883437016268|5.88984241943219|5.33302191099898|71.8416428728311|    103.077087696235|         35.688|             18.048|  25.989|   4.9658738287042|    0.0|         0.0|                0.0|             35.688|               42.183|  0.432710488460912|-26.1225883558136|      3.01599|             0.0|   135.039743741844|                   18.6141|               0.0|               0.0|               0.0|25.8387608672372|    10.271|         10.271|           0.0|  -5.48883|        0.0| 0.00572809|              3.9|  6.84894E-4|                null|\n",
      "|B00FG1-R|2018-06-30|            null| -42.514266972778| 3.54134632665695| 22.5596181588274| 4.97263923389855| 0.0370732766553549|86.5535968402405|            null|51.0890530876287| 22.2226946133243|-3.12564605057006|            null|          0.367785|            null|                 9.7|                   0.0|       0.0|    9.25381|             6.2295|-22.6036912085731|                null|  -0.190497059956757|       0.0|        0.0|        10.425|      585.287|         0.0|            null| 103.12564605057|28.8812155404101|      8.16167|                   16.677|                    16.677|     -21.3497453310696|        16.677|                 16.677|  -0.668999999999997|                  34.88| 0.568631888649996|25.3588410472127|  0.551228553535041|            0.0|14.3077000557913|            null|15.0212412180126|8.13265093295255| 7.7240338897015|75.2532928064843|     103.12564605057|         35.549|             19.111|  33.599|  5.74060247365822|    0.0|         0.0|                0.0|             35.549|               42.468|  0.394305506875863|-22.4960254372019|      3.37145|             0.0|   127.807866715486|                   17.1453|               0.0|               0.0|               0.0|21.7732991037576|   9.16667|        9.16667|           0.0|  -3.50106|        0.0| 0.00572809|              3.9|  6.84894E-4|                null|\n",
      "|B00FG1-R|2018-09-30|30.5335856196783| 30.5335856196783| 6.94162976472445|             null| 5.79103922662205|   0.51792633467348| 43.012335502105|2604.17222328062|48.1419014683372|             null| 4.45182343655499|2146.27057710501|          0.337733|2146.27057710501|              17.103|                   0.0|       0.0|    8.06682|            6.25421|-33.3333333333333|    2788.55629139073|   0.280100730463075|       0.0|        0.0|        10.567|      593.577|         0.0|2146.27057710501| 95.548176563445|28.4315261541468|      11.0754|                   18.724|                    18.724|     -28.9330853607621|        18.724|                 18.724|  -0.831999999999997|                 37.577| -5.04104034810127|24.6611644319103|   1.12551359459122|            0.0|13.2853655896555|1420.95637264241|13.4294094452037|7.72895227043698|1.80911131869013|73.5672058003346|     95.548176563445|         38.409|             20.803|  15.084|  2.54120358437069|    0.0|         0.0|                0.0|             38.409|                45.86|  0.419702791068391|-32.7892462794047|      3.58055|             0.0|   140.154314892657|                   19.3064|               0.0|               0.0|               0.0|  25.03848626108|   9.04872|        9.04872|           0.0| -0.724568|        0.0| 0.00572809|              3.9|  6.84894E-4|                null|\n",
      "|B00FG1-R|2018-12-31|40.3218230879501| 40.3218230879501|  29.666192122568|             null| 18.3714590742121|   3.22567421644502|136.714621256606|3039.48372615039|42.5510650259229|             null| 3.40266440185814|2838.87342946285|          0.387121|2838.87342946285|             109.197|                   0.0|       0.0|    8.55224|            6.16694|-3.80741337630943|    3634.86381651067|   -3.98218956008387|       0.0|        0.0|        12.093|      697.042|         0.0|2838.87342946285|96.5973355981419|24.3130256139515|      3.83772|                   21.769|                    21.769|      4.11306136113635|        21.769|                 21.769|  -0.911999999999999|                 38.571|  7.47475297384107|21.3945501131926|   0.26479791672712|            0.0|12.1848771741947|1693.51290684624|12.2574078391457|20.3448949829357|11.3504420588842|72.8854303797865|    96.5973355981419|         39.483|             26.983|    3.09| 0.443301838339728|    0.0|         0.0|                0.0|             39.483|               48.107|  0.477460933424391|-3.89426957223567|      3.24929|             0.0|   141.094183483731|                   22.4074|               0.0|               0.0|               0.0|21.0645257983781|     9.801|          9.801|           0.0|  -18.0132|        0.0| 0.00572809|              3.9|  6.84894E-4|                null|\n",
      "+--------+----------+----------------+-----------------+-----------------+-----------------+-----------------+-------------------+----------------+----------------+----------------+-----------------+-----------------+----------------+------------------+----------------+--------------------+----------------------+----------+-----------+-------------------+-----------------+--------------------+--------------------+----------+-----------+--------------+-------------+------------+----------------+----------------+----------------+-------------+-------------------------+--------------------------+----------------------+--------------+-----------------------+--------------------+-----------------------+------------------+----------------+-------------------+---------------+----------------+----------------+----------------+----------------+----------------+----------------+--------------------+---------------+-------------------+--------+------------------+-------+------------+-------------------+-------------------+---------------------+-------------------+-----------------+-------------+----------------+-------------------+--------------------------+------------------+------------------+------------------+----------------+----------+---------------+--------------+----------+-----------+-----------+-----------------+------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79a0fd65-c039-4a2c-8719-e2a20b8404b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:178: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to Pandas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2318191/2614254797.py:10: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  corr_mat = corr_df.corr().abs()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable pairs with absolute correlation above 0.7:\n",
      "ff_assets_com_eq - ff_assets_eq: 0.9221654906857393\n",
      "ff_assets_com_eq - ff_debt_com_eq: 0.8031943996364546\n",
      "ff_assets_com_eq - ff_debt_eq: 0.7578472116511359\n",
      "ff_assets_com_eq - ff_fix_assets_com_eq: 0.7585821391223883\n",
      "ff_assets_com_eq - ff_ltd_com_eq: 0.7835918699012024\n",
      "ff_assets_eq - ff_debt_com_eq: 0.8111022681915349\n",
      "ff_assets_eq - ff_debt_eq: 0.9744926048929192\n",
      "ff_assets_eq - ff_fix_assets_com_eq: 0.7982664517160992\n",
      "ff_assets_eq - ff_ltd_com_eq: 0.7963589616864233\n",
      "ff_assets_gr - ff_com_eq_gr: 0.8774939688247103\n",
      "ff_assets_gr - ff_sales_gr: 0.9787031697803807\n",
      "ff_capex_assets - ff_oper_inc_tcap: 0.9999923126037884\n",
      "ff_capex_ps_cf - ff_eps_basic_gr: 0.9714446678135161\n",
      "ff_capex_ps_cf - ff_free_ps_cf: 0.9880326209285167\n",
      "ff_capex_ps_cf - ff_eps_dil_gr: 0.9714485750755496\n",
      "ff_com_eq_tcap - ff_tot_debt_tcap_std: 0.9953154245559508\n",
      "ff_com_eq_tcap - ff_wkcap_pct: 0.9787508788828343\n",
      "ff_debt_com_eq - ff_debt_eq: 0.9863144435863759\n",
      "ff_debt_com_eq - ff_fix_assets_com_eq: 0.9681467719787887\n",
      "ff_debt_com_eq - ff_ltd_com_eq: 0.9803451561161426\n",
      "ff_debt_eq - ff_fix_assets_com_eq: 0.9686070817868299\n",
      "ff_debt_eq - ff_ltd_com_eq: 0.9698776135237455\n",
      "ff_earn_yld - ff_eps_dil_aft_xord: 0.8233003326233564\n",
      "ff_earn_yld - ff_fcf_yld: 0.9999999999998352\n",
      "ff_entrpr_val_sales - ff_psales_dil: 0.9999999674974966\n",
      "ff_eps_basic_gr - ff_free_ps_cf: 0.976002833031754\n",
      "ff_eps_basic_gr - ff_eps_dil_gr: 0.9996861387904231\n",
      "ff_fix_assets_com_eq - ff_ltd_com_eq: 0.9854435661534277\n",
      "ff_free_ps_cf - ff_eps_dil_gr: 0.9760067598407011\n",
      "ff_invest_lt - ff_invest_receiv_lt: 1.0\n",
      "ff_net_inc_basic_aft_xord - ff_net_inc_basic_beft_xord: 0.9755032605332251\n",
      "ff_net_inc_basic_aft_xord - ff_net_inc_dil: 0.9732564439435686\n",
      "ff_net_inc_basic_aft_xord - ff_net_inc_dil_aft_xord: 0.9928216209538088\n",
      "ff_net_inc_basic_aft_xord - ff_oper_inc_aft_unusual: 0.848700536385765\n",
      "ff_net_inc_basic_aft_xord - ff_ut_gross_inc: 0.7418102279051093\n",
      "ff_net_inc_basic_aft_xord - ff_ebit_bef_unusual: 0.7602076372021579\n",
      "ff_net_inc_basic_aft_xord - ff_ebitda_bef_unusual: 0.7229088459436626\n",
      "ff_net_inc_basic_aft_xord - ff_net_inc_dil_bef_unusual: 0.8915485276005921\n",
      "ff_net_inc_basic_beft_xord - ff_net_inc_dil: 0.9884604264026092\n",
      "ff_net_inc_basic_beft_xord - ff_net_inc_dil_aft_xord: 0.9682015220290898\n",
      "ff_net_inc_basic_beft_xord - ff_oper_inc_aft_unusual: 0.87118600900084\n",
      "ff_net_inc_basic_beft_xord - ff_ut_gross_inc: 0.7662985162742765\n",
      "ff_net_inc_basic_beft_xord - ff_ebit_bef_unusual: 0.7803352618067643\n",
      "ff_net_inc_basic_beft_xord - ff_ebitda_bef_unusual: 0.7403997826758383\n",
      "ff_net_inc_basic_beft_xord - ff_net_inc_dil_bef_unusual: 0.9065042019788545\n",
      "ff_net_inc_bef_xord_gr - ff_oper_inc_gr: 0.9835226151104088\n",
      "ff_net_inc_bef_xord_gr - ff_sales_gr: 0.9972406046630995\n",
      "ff_net_inc_dil - ff_net_inc_dil_aft_xord: 0.9799764224211591\n",
      "ff_net_inc_dil - ff_oper_inc_aft_unusual: 0.8729841728668726\n",
      "ff_net_inc_dil - ff_ut_gross_inc: 0.7669121374932039\n",
      "ff_net_inc_dil - ff_ebit_bef_unusual: 0.7811215392866697\n",
      "ff_net_inc_dil - ff_ebitda_bef_unusual: 0.7408462152703932\n",
      "ff_net_inc_dil - ff_net_inc_dil_bef_unusual: 0.9192376701671102\n",
      "ff_net_inc_dil_aft_xord - ff_oper_inc_aft_unusual: 0.8548020342996129\n",
      "ff_net_inc_dil_aft_xord - ff_ut_gross_inc: 0.7503089348808892\n",
      "ff_net_inc_dil_aft_xord - ff_ebit_bef_unusual: 0.7660134646217276\n",
      "ff_net_inc_dil_aft_xord - ff_ebitda_bef_unusual: 0.7267526850081419\n",
      "ff_net_inc_dil_aft_xord - ff_net_inc_dil_bef_unusual: 0.900964771183068\n",
      "ff_oper_inc_aft_unusual - ff_ut_gross_inc: 0.7646284437166257\n",
      "ff_oper_inc_aft_unusual - ff_ebit_bef_unusual: 0.7929796145501576\n",
      "ff_oper_inc_aft_unusual - ff_ebitda_bef_unusual: 0.7561450896120925\n",
      "ff_oper_inc_aft_unusual - ff_net_inc_dil_bef_unusual: 0.7908551157101457\n",
      "ff_oper_inc_gr - ff_sales_gr: 0.9381865502912264\n",
      "ff_tot_debt_tcap_std - ff_wkcap_pct: 0.979056203862686\n",
      "ff_ut_gross_inc - ff_ebit_bef_unusual: 0.9003372009119369\n",
      "ff_ut_gross_inc - ff_ebitda_bef_unusual: 0.8621748509599112\n",
      "ff_ut_gross_inc - ff_net_inc_dil_bef_unusual: 0.9092598970347959\n",
      "ff_xord - ff_xord_disc: 0.7626694277467742\n",
      "ff_ebit_bef_unusual - ff_ebitda_bef_unusual: 0.9673989070804402\n",
      "ff_ebit_bef_unusual - ff_net_inc_dil_bef_unusual: 0.9037359985985469\n",
      "ff_ebitda_bef_unusual - ff_net_inc_dil_bef_unusual: 0.8647946113183922\n",
      "ff_eps_dil_aft_xord - ff_fcf_yld: 0.8233004086449334\n",
      "ff_bk_oper_inc_oth - ff_commiss_inc_net: 0.9999998408928279\n",
      "ff_div_yld - ff_div_yld_secs: 0.9995385309297414\n",
      "GDP - CPI: 0.8115994622320295\n",
      "Index(['fsym_id', 'date', 'ff_assets_com_eq', 'ff_assets_gr', 'ff_bps_gr',\n",
      "       'ff_capex_assets', 'ff_capex_ps_cf', 'ff_cash_div_cf', 'ff_cash_roce',\n",
      "       'ff_cf_sales', 'ff_com_eq_tcap', 'ff_debt_entrpr_val', 'ff_debt_lt_cf',\n",
      "       'ff_debt_st_x_curr_port', 'ff_dil_adj', 'ff_earn_yld',\n",
      "       'ff_entrpr_val_sales', 'ff_inc_adj', 'ff_inc_sund', 'ff_int_exp_oth',\n",
      "       'ff_invest_cap', 'ff_invest_lt', 'ff_ltd_tcap', 'ff_min_int_tcap',\n",
      "       'ff_mkt_val_gr', 'ff_net_inc_basic_aft_xord', 'ff_net_inc_bef_xord_gr',\n",
      "       'ff_non_oper_exp', 'ff_oper_ps_net_cf', 'ff_pfd_stk_tcap', 'ff_roa_ptx',\n",
      "       'ff_roce', 'ff_roic', 'ff_sales_ps_gr', 'ff_tcap_assets',\n",
      "       'ff_ut_operation_exp', 'ff_wkcap', 'ff_xord', 'ff_std_debt',\n",
      "       'ff_tang_assets_debt', 'ff_bk_oper_inc_oth', 'ff_bk_oper_inc_tot',\n",
      "       'ff_cf_roic', 'ff_div_yld', 'ff_liabs_lease', 'ff_accr_exp', 'GDP',\n",
      "       'Unemployment_Rate', 'Implosion_Start_Date'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def correlation_matrix(df):\n",
    "    df =df.toPandas()\n",
    "    print(\"Converted to Pandas\")\n",
    "    corr_df = df.drop(['date','fsym_id'], axis=1)\n",
    "    corr_mat = corr_df.corr().abs()\n",
    "    mask = np.triu(np.ones_like(corr_mat))\n",
    "    plt.figure(figsize=(50, 40))\n",
    "    sns.heatmap(corr_mat, annot=True, cmap='coolwarm', fmt=\".2f\", mask=mask)\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('corr_matrix_tab.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Variable pairs with absolute correlation above 0.7:\")\n",
    "    corr_dict = {}\n",
    "    for i in range(len(corr_mat.columns)):\n",
    "        for j in range(i+1, len(corr_mat.columns)):\n",
    "            if abs(corr_mat.iloc[i, j]) >= 0.7:\n",
    "                print(f\"{corr_mat.columns[i]} - {corr_mat.columns[j]}: {corr_mat.iloc[i, j]}\")\n",
    "                if corr_mat.columns[i] not in corr_dict.keys():\n",
    "                    corr_dict[corr_mat.columns[i]] = [corr_mat.columns[j]]\n",
    "                else:\n",
    "                    corr_dict[corr_mat.columns[i]].append(corr_mat.columns[j])\n",
    "                    \n",
    "    for k,v in corr_dict.items():\n",
    "        if len(corr_dict[k]) >= 1:\n",
    "            for col in corr_dict[k]:\n",
    "                if col in df.columns:\n",
    "                    df=df.drop(col,axis=1)\n",
    "    \n",
    "                \n",
    "\n",
    "    print(df.columns)\n",
    "    return df\n",
    "\n",
    "    \n",
    "                \n",
    "df=correlation_matrix(df)\n",
    "# df=correlation_matrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89221513-5684-4754-b418-402253cb0f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feats = df.columns[2:-1]\n",
    "df[feats] = df.groupby('fsym_id')[feats].transform(lambda x : x.fillna(method='ffill'))\n",
    "df[feats] = df.groupby('fsym_id')[feats].transform(lambda x: x.fillna(x.median()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b535ac71-10b5-4d62-99cf-0bc281522350",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop('Implosion_Start_Date', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1cd6de65-b425-4b4c-baac-ed948eef527a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fsym_id</th>\n",
       "      <th>date</th>\n",
       "      <th>ff_assets_com_eq</th>\n",
       "      <th>ff_assets_gr</th>\n",
       "      <th>ff_bps_gr</th>\n",
       "      <th>ff_capex_assets</th>\n",
       "      <th>ff_capex_ps_cf</th>\n",
       "      <th>ff_cash_div_cf</th>\n",
       "      <th>ff_cash_roce</th>\n",
       "      <th>ff_cf_sales</th>\n",
       "      <th>...</th>\n",
       "      <th>ff_tang_assets_debt</th>\n",
       "      <th>ff_bk_oper_inc_oth</th>\n",
       "      <th>ff_bk_oper_inc_tot</th>\n",
       "      <th>ff_cf_roic</th>\n",
       "      <th>ff_div_yld</th>\n",
       "      <th>ff_liabs_lease</th>\n",
       "      <th>ff_accr_exp</th>\n",
       "      <th>GDP</th>\n",
       "      <th>Unemployment_Rate</th>\n",
       "      <th>Implosion_Start_Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2014-03-31</td>\n",
       "      <td>1.006164</td>\n",
       "      <td>6.941630</td>\n",
       "      <td>42.755522</td>\n",
       "      <td>2.918292</td>\n",
       "      <td>0.040663</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>149.850206</td>\n",
       "      <td>54.927969</td>\n",
       "      <td>...</td>\n",
       "      <td>128.453561</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.476737</td>\n",
       "      <td>9.04872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.006058</td>\n",
       "      <td>5.6</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2014-06-30</td>\n",
       "      <td>7.947306</td>\n",
       "      <td>6.941630</td>\n",
       "      <td>42.755522</td>\n",
       "      <td>2.918292</td>\n",
       "      <td>0.085351</td>\n",
       "      <td>7198.072711</td>\n",
       "      <td>149.850206</td>\n",
       "      <td>58.673863</td>\n",
       "      <td>...</td>\n",
       "      <td>116.030667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.476737</td>\n",
       "      <td>9.04872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006058</td>\n",
       "      <td>5.6</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2014-09-30</td>\n",
       "      <td>7.947306</td>\n",
       "      <td>6.941630</td>\n",
       "      <td>42.755522</td>\n",
       "      <td>2.918292</td>\n",
       "      <td>0.777880</td>\n",
       "      <td>1793.051823</td>\n",
       "      <td>149.850206</td>\n",
       "      <td>53.004612</td>\n",
       "      <td>...</td>\n",
       "      <td>88.900000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.476737</td>\n",
       "      <td>9.04872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006058</td>\n",
       "      <td>5.6</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2014-12-31</td>\n",
       "      <td>7.947306</td>\n",
       "      <td>6.941630</td>\n",
       "      <td>42.755522</td>\n",
       "      <td>11.984993</td>\n",
       "      <td>0.568918</td>\n",
       "      <td>1752.469136</td>\n",
       "      <td>149.850206</td>\n",
       "      <td>30.259303</td>\n",
       "      <td>...</td>\n",
       "      <td>77.245294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.476737</td>\n",
       "      <td>9.04872</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.006058</td>\n",
       "      <td>5.6</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2015-03-31</td>\n",
       "      <td>7.947306</td>\n",
       "      <td>1210.946360</td>\n",
       "      <td>-453.765614</td>\n",
       "      <td>11.419992</td>\n",
       "      <td>0.000515</td>\n",
       "      <td>56.551330</td>\n",
       "      <td>149.850206</td>\n",
       "      <td>62.973990</td>\n",
       "      <td>...</td>\n",
       "      <td>78.877843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.438389</td>\n",
       "      <td>3.49218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2015-06-30</td>\n",
       "      <td>7.947306</td>\n",
       "      <td>20.015973</td>\n",
       "      <td>-522.564054</td>\n",
       "      <td>10.396011</td>\n",
       "      <td>0.006026</td>\n",
       "      <td>530.536190</td>\n",
       "      <td>149.850206</td>\n",
       "      <td>67.875989</td>\n",
       "      <td>...</td>\n",
       "      <td>68.632495</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.660801</td>\n",
       "      <td>4.76389</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2015-09-30</td>\n",
       "      <td>7.947306</td>\n",
       "      <td>20.169146</td>\n",
       "      <td>-281.606764</td>\n",
       "      <td>4.546495</td>\n",
       "      <td>0.028012</td>\n",
       "      <td>55.502330</td>\n",
       "      <td>149.850206</td>\n",
       "      <td>62.488539</td>\n",
       "      <td>...</td>\n",
       "      <td>71.079842</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.225828</td>\n",
       "      <td>7.83517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2015-12-31</td>\n",
       "      <td>7.947306</td>\n",
       "      <td>7.348883</td>\n",
       "      <td>-48.051855</td>\n",
       "      <td>0.483800</td>\n",
       "      <td>0.025150</td>\n",
       "      <td>64.834284</td>\n",
       "      <td>149.850206</td>\n",
       "      <td>56.856806</td>\n",
       "      <td>...</td>\n",
       "      <td>70.526570</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.546902</td>\n",
       "      <td>6.74157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>7.947306</td>\n",
       "      <td>7.781205</td>\n",
       "      <td>-54.727611</td>\n",
       "      <td>0.569216</td>\n",
       "      <td>0.012767</td>\n",
       "      <td>67.074842</td>\n",
       "      <td>149.850206</td>\n",
       "      <td>59.881255</td>\n",
       "      <td>...</td>\n",
       "      <td>72.256182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.404235</td>\n",
       "      <td>8.06794</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010414</td>\n",
       "      <td>4.7</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B00FG1-R</td>\n",
       "      <td>2016-06-30</td>\n",
       "      <td>7.947306</td>\n",
       "      <td>9.770063</td>\n",
       "      <td>41.140026</td>\n",
       "      <td>0.848703</td>\n",
       "      <td>0.043794</td>\n",
       "      <td>78.941295</td>\n",
       "      <td>149.850206</td>\n",
       "      <td>51.154726</td>\n",
       "      <td>...</td>\n",
       "      <td>80.334313</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.305352</td>\n",
       "      <td>6.90704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.010414</td>\n",
       "      <td>4.7</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    fsym_id        date  ff_assets_com_eq  ff_assets_gr   ff_bps_gr  \\\n",
       "0  B00FG1-R  2014-03-31          1.006164      6.941630   42.755522   \n",
       "1  B00FG1-R  2014-06-30          7.947306      6.941630   42.755522   \n",
       "2  B00FG1-R  2014-09-30          7.947306      6.941630   42.755522   \n",
       "3  B00FG1-R  2014-12-31          7.947306      6.941630   42.755522   \n",
       "4  B00FG1-R  2015-03-31          7.947306   1210.946360 -453.765614   \n",
       "5  B00FG1-R  2015-06-30          7.947306     20.015973 -522.564054   \n",
       "6  B00FG1-R  2015-09-30          7.947306     20.169146 -281.606764   \n",
       "7  B00FG1-R  2015-12-31          7.947306      7.348883  -48.051855   \n",
       "8  B00FG1-R  2016-03-31          7.947306      7.781205  -54.727611   \n",
       "9  B00FG1-R  2016-06-30          7.947306      9.770063   41.140026   \n",
       "\n",
       "   ff_capex_assets  ff_capex_ps_cf  ff_cash_div_cf  ff_cash_roce  ff_cf_sales  \\\n",
       "0         2.918292        0.040663        0.000000    149.850206    54.927969   \n",
       "1         2.918292        0.085351     7198.072711    149.850206    58.673863   \n",
       "2         2.918292        0.777880     1793.051823    149.850206    53.004612   \n",
       "3        11.984993        0.568918     1752.469136    149.850206    30.259303   \n",
       "4        11.419992        0.000515       56.551330    149.850206    62.973990   \n",
       "5        10.396011        0.006026      530.536190    149.850206    67.875989   \n",
       "6         4.546495        0.028012       55.502330    149.850206    62.488539   \n",
       "7         0.483800        0.025150       64.834284    149.850206    56.856806   \n",
       "8         0.569216        0.012767       67.074842    149.850206    59.881255   \n",
       "9         0.848703        0.043794       78.941295    149.850206    51.154726   \n",
       "\n",
       "   ...  ff_tang_assets_debt  ff_bk_oper_inc_oth  ff_bk_oper_inc_tot  \\\n",
       "0  ...           128.453561                 0.0                 0.0   \n",
       "1  ...           116.030667                 0.0                 0.0   \n",
       "2  ...            88.900000                 0.0                 0.0   \n",
       "3  ...            77.245294                 0.0                 0.0   \n",
       "4  ...            78.877843                 0.0                 0.0   \n",
       "5  ...            68.632495                 0.0                 0.0   \n",
       "6  ...            71.079842                 0.0                 0.0   \n",
       "7  ...            70.526570                 0.0                 0.0   \n",
       "8  ...            72.256182                 0.0                 0.0   \n",
       "9  ...            80.334313                 0.0                 0.0   \n",
       "\n",
       "   ff_cf_roic  ff_div_yld  ff_liabs_lease  ff_accr_exp       GDP  \\\n",
       "0   21.476737     9.04872             0.0        0.188  0.006058   \n",
       "1   21.476737     9.04872             0.0        0.000  0.006058   \n",
       "2   21.476737     9.04872             0.0        0.000  0.006058   \n",
       "3   21.476737     9.04872             0.0        0.000  0.006058   \n",
       "4   12.438389     3.49218             0.0        0.000  0.001821   \n",
       "5   12.660801     4.76389             0.0        0.000  0.001821   \n",
       "6   17.225828     7.83517             0.0        0.000  0.001821   \n",
       "7   19.546902     6.74157             0.0        0.000  0.001821   \n",
       "8   21.404235     8.06794             0.0        0.000  0.010414   \n",
       "9   23.305352     6.90704             0.0        0.000  0.010414   \n",
       "\n",
       "   Unemployment_Rate  Implosion_Start_Date  \n",
       "0                5.6                   NaT  \n",
       "1                5.6                   NaT  \n",
       "2                5.6                   NaT  \n",
       "3                5.6                   NaT  \n",
       "4                5.0                   NaT  \n",
       "5                5.0                   NaT  \n",
       "6                5.0                   NaT  \n",
       "7                5.0                   NaT  \n",
       "8                4.7                   NaT  \n",
       "9                4.7                   NaT  \n",
       "\n",
       "[10 rows x 49 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e94e371a-d1a1-44d8-bacd-ef3612ff6b8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tsfel\n",
    "\n",
    "def feature_extraction(df):\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df['year'] = df['date'].dt.year\n",
    "    df = df.set_index('date')\n",
    "\n",
    "    \n",
    "    cfg = tsfel.get_features_by_domain(\"statistical\")\n",
    "    \n",
    "    result_dfs = []\n",
    "    for fsym_id, group_df in df.groupby(['fsym_id', 'year']):\n",
    "        # Exclude 'fsym_id' column from group_df\n",
    "        # print(group_df.head())\n",
    "        # non_zero_cols = group_df.columns[(group_df != 0).any()]\n",
    "        # group_df = group_df[non_zero_cols]\n",
    "\n",
    "        if not group_df.empty:\n",
    "            try:\n",
    "                X = tsfel.time_series_features_extractor(cfg, group_df.drop(['fsym_id', 'year'], axis=1), verbose=0)\n",
    "                X['fsym_id'] = group_df['fsym_id'].iloc[0]\n",
    "                X['year'] = group_df['year'].iloc[0]\n",
    "                result_dfs.append(X)\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "    final_result = pd.concat(result_dfs, ignore_index=True)\n",
    "    final_result.reset_index(drop=True, inplace=True)\n",
    "    return final_result\n",
    "\n",
    "df2 = feature_extraction(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cccc7d61-1979-4ef3-ac59-28307f25b47e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2806\n"
     ]
    }
   ],
   "source": [
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fbbfa0a-2d53-4c01-921d-b4acad86441a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fsym_id                          0\n",
       "date                             0\n",
       "ff_assets_com_eq               692\n",
       "ff_assets_gr                   996\n",
       "ff_assets_oth_tot              211\n",
       "ff_assets_per_emp             5331\n",
       "ff_bps_gr                     1252\n",
       "ff_capex_assets                792\n",
       "ff_cash_div_cf                 858\n",
       "ff_cash_roce                  2670\n",
       "ff_cf_ps_gr                   3344\n",
       "ff_cf_sales                   3922\n",
       "ff_com_eq_gr                  1122\n",
       "ff_com_eq_tcap                 121\n",
       "ff_debt_entrpr_val            2236\n",
       "ff_debt_lt_cf                 1163\n",
       "ff_debt_st_x_curr_port          75\n",
       "ff_dfd_tax_assets_lt             0\n",
       "ff_dil_adj                      81\n",
       "ff_div_yld                    2495\n",
       "ff_earn_yld                   2268\n",
       "ff_ebit_oper_roa             14022\n",
       "ff_entrpr_val_sales           5279\n",
       "ff_eps_basic_gr               2443\n",
       "ff_for_assets_pct             7538\n",
       "ff_for_sales_pct              5638\n",
       "ff_gross_cf_debt              3006\n",
       "ff_inc_adj                       0\n",
       "ff_inc_sund                      0\n",
       "ff_inc_tax_curr                383\n",
       "ff_inc_tax_dfd                 408\n",
       "ff_int_exp_oth                  42\n",
       "ff_invest_cap                    4\n",
       "ff_invest_lt                   104\n",
       "ff_invest_st_tot             10811\n",
       "ff_ltd_tcap                    122\n",
       "ff_min_int_tcap                136\n",
       "ff_net_inc_basic_aft_xord       57\n",
       "ff_net_inc_per_emp            5339\n",
       "ff_non_oper_exp                751\n",
       "ff_oper_cf_fix_chrg           5027\n",
       "ff_oper_inc_tcap               161\n",
       "ff_oper_ps_net_cf             2111\n",
       "ff_roic                       1290\n",
       "ff_tcap_assets                  19\n",
       "ff_ut_non_oper_inc_oth        6444\n",
       "ff_ut_operation_exp              5\n",
       "ff_wkcap                      9988\n",
       "ff_xord                          6\n",
       "ff_std_debt                   2816\n",
       "ff_tang_assets_debt           2701\n",
       "ff_bk_oper_inc_oth               0\n",
       "ff_commiss_inc_net               0\n",
       "ff_cf_roic                    2083\n",
       "ff_liabs_lease                 114\n",
       "GDP                              0\n",
       "Unemployment_Rate                0\n",
       "label                            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e85588ba-9cb1-4814-a0cb-1f2a9b9eca3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop(['ff_ebit_oper_roa', 'ff_invest_st_tot', 'ff_wkcap', ],inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e4f86d3-0e44-4045-a5f5-89b30d98a284",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99581\n",
      "257\n"
     ]
    }
   ],
   "source": [
    "print(len(df))\n",
    "# df=df.dropna()\n",
    "# print(len(df))\n",
    "df_null = df.dropna()\n",
    "print(len(df_null[df_null['label']==1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cec2fc9d-2358-4ba0-bcd0-56954541f3f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df=df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71809d8a-11f6-463f-9f88-eae45ef3c97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73d9643f-ef4e-4eb0-8c37-88fd204bb19a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to Pandas\n",
      "{0: 0.501755944247062, 1: 142.87354085603113}\n",
      "  1%|          | 1/100 [02:39<4:23:57, 159.97s/trial, best loss: -0.06943950273282606]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 183\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;66;03m# f.write(f\"\\n\\nMatthews Correlation Coefficient: {matthews_corrcoef(true, preds)}\")\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m classifier_instance, X_train\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist(), X_train\n\u001b[0;32m--> 183\u001b[0m model, feats, X_train \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_testing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mRandomForest\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 147\u001b[0m, in \u001b[0;36mmodel_testing\u001b[0;34m(df, classifier)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mrename(index\u001b[38;5;241m=\u001b[39m{res\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m,res\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro_avg\u001b[39m\u001b[38;5;124m'\u001b[39m,res\u001b[38;5;241m.\u001b[39mindex[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted_avg\u001b[39m\u001b[38;5;124m'\u001b[39m})\n\u001b[1;32m    139\u001b[0m \u001b[38;5;66;03m#     initial_model = classifier_instance\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m#     initial_model = initial_model.fit(X_train, y_train)\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;66;03m#     initial_preds = pd.DataFrame()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;66;03m#     initial_preds['label'] = y_test\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m#     confusion_matrix_pandas(initial_preds)\u001b[39;00m\n\u001b[0;32m--> 147\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stop_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mno_progress_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m set_params(classifier, best_params)\n\u001b[1;32m    150\u001b[0m     classifier_instance\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_params)\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/hyperopt/fmin.py:586\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    583\u001b[0m rval\u001b[38;5;241m.\u001b[39mcatch_eval_exceptions \u001b[38;5;241m=\u001b[39m catch_eval_exceptions\n\u001b[1;32m    585\u001b[0m \u001b[38;5;66;03m# next line is where the fmin is actually executed\u001b[39;00m\n\u001b[0;32m--> 586\u001b[0m \u001b[43mrval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexhaust\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_argmin:\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/hyperopt/fmin.py:364\u001b[0m, in \u001b[0;36mFMinIter.exhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexhaust\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    363\u001b[0m     n_done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials)\n\u001b[0;32m--> 364\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_evals\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock_until_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masynchronous\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/hyperopt/fmin.py:300\u001b[0m, in \u001b[0;36mFMinIter.run\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    297\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpoll_interval_secs)\n\u001b[1;32m    298\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;66;03m# -- loop over trials and do the jobs directly\u001b[39;00m\n\u001b[0;32m--> 300\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mserial_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\u001b[38;5;241m.\u001b[39mrefresh()\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials_save_file \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/hyperopt/fmin.py:178\u001b[0m, in \u001b[0;36mFMinIter.serial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    176\u001b[0m ctrl \u001b[38;5;241m=\u001b[39m base\u001b[38;5;241m.\u001b[39mCtrl(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials, current_trial\u001b[38;5;241m=\u001b[39mtrial)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 178\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdomain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctrl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    180\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjob exception: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mstr\u001b[39m(e))\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/hyperopt/base.py:892\u001b[0m, in \u001b[0;36mDomain.evaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;66;03m# -- the \"work\" of evaluating `config` can be written\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     \u001b[38;5;66;03m#    either into the pyll part (self.expr)\u001b[39;00m\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;66;03m#    or the normal Python part (self.fn)\u001b[39;00m\n\u001b[1;32m    887\u001b[0m     pyll_rval \u001b[38;5;241m=\u001b[39m pyll\u001b[38;5;241m.\u001b[39mrec_eval(\n\u001b[1;32m    888\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexpr,\n\u001b[1;32m    889\u001b[0m         memo\u001b[38;5;241m=\u001b[39mmemo,\n\u001b[1;32m    890\u001b[0m         print_node_on_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrec_eval_print_node_on_error,\n\u001b[1;32m    891\u001b[0m     )\n\u001b[0;32m--> 892\u001b[0m     rval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyll_rval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rval, (\u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mint\u001b[39m, np\u001b[38;5;241m.\u001b[39mnumber)):\n\u001b[1;32m    895\u001b[0m     dict_rval \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mfloat\u001b[39m(rval), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m: STATUS_OK}\n",
      "Cell \u001b[0;32mIn[36], line 116\u001b[0m, in \u001b[0;36mmodel_testing.<locals>.objective\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    113\u001b[0m params \u001b[38;5;241m=\u001b[39m set_params(classifier, params)\n\u001b[1;32m    114\u001b[0m classifier_instance\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 116\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclassifier_instance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtscv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mf1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mscores\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:562\u001b[0m, in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[1;32m    560\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[0;32m--> 562\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:309\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[1;32m    308\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[0;32m--> 309\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:729\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    727\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m    728\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 729\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    445\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    448\u001b[0m ]\n\u001b[1;32m    450\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 456\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    469\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    472\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/sklearn/utils/parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     64\u001b[0m )\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/sklearn/utils/parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:188\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    186\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 188\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/sklearn/tree/_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \n\u001b[1;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 959\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/sklearn/tree/_classes.py:443\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    433\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    434\u001b[0m         splitter,\n\u001b[1;32m    435\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    441\u001b[0m     )\n\u001b[0;32m--> 443\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from sklearn import tree\n",
    "import shap\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import xgboost as xgb\n",
    "from collections import Counter\n",
    "from hyperopt.early_stop import no_progress_loss\n",
    "from functools import reduce\n",
    "\n",
    "def feature_importances(model, features):\n",
    "    feature_importances = model.feature_importances_\n",
    "\n",
    "    # print(\"Feature Importances:\")\n",
    "    # for feature, importance in zip(features, feature_importances):\n",
    "    #     print(f\"{feature}: {importance}\")\n",
    "\n",
    "    sorted_idx = np.argsort(feature_importances)\n",
    "    sorted_features = [features[i] for i in sorted_idx]\n",
    "    \n",
    "    half_len = (len(sorted_idx) // 4 ) * 3 # Calculate the index for the middle point\n",
    "\n",
    "    # Select the lowest 50% of features\n",
    "    selected_features = [features[i] for i in sorted_idx[:half_len]]\n",
    "\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.bar(range(len(feature_importances)), feature_importances[sorted_idx], align=\"center\")\n",
    "    plt.xticks(range(len(feature_importances)), sorted_features, rotation=45, ha=\"right\")\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"Feature Importance\")\n",
    "    plt.title(\"Feature Importances\")\n",
    "    plt.show()\n",
    "    return selected_features\n",
    "\n",
    "def model_testing(df, classifier):\n",
    "    seed = 42\n",
    "    print(\"Converted to Pandas\")\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "    X_train = df.drop(exclude_columns, axis=1)\n",
    "    y_train = df['label']\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    print(class_weight_dict)\n",
    "    \n",
    "    if classifier == 'LogisticRegression':\n",
    "        param_space = {\n",
    "            'C': hp.uniform('C', 0.01, 1.0) }\n",
    "        classifier_instance = LogisticRegression(class_weight = class_weight_dict, solver='sag', seed=42)\n",
    "        scaler = StandardScaler()\n",
    "        feats = X_train.columns\n",
    "        X_train[feats] = scaler.fit_transform(X_train[feats])\n",
    "        \n",
    "    elif classifier == 'RandomForest':\n",
    "        param_space = { \n",
    "            'n_estimators': hp.quniform('n_estimators', 100, 500, 1),\n",
    "            'max_depth': hp.quniform('max_depth', 5, 20, 1)\n",
    "        }\n",
    "        classifier_instance = RandomForestClassifier(class_weight = class_weight_dict, random_state=42)\n",
    "    elif classifier == 'GBT':\n",
    "        param_space = { 'n_estimators':hp.uniform('n_estimators',100,500),\n",
    "           'max_depth':hp.quniform('max_depth',5,20,1),\n",
    "           'min_samples_leaf':hp.quniform('min_samples_leaf',1,5,1),\n",
    "           'min_samples_split':hp.quniform('min_samples_split',2,6,1)}\n",
    "        classifier_instance = GradientBoostingClassifier(seed=42)\n",
    "    elif classifier == 'XGB':\n",
    "        param_space = { 'n_estimators':hp.quniform('n_estimators',100,500,1),\n",
    "           'max_depth':hp.quniform('max_depth',5,20,1)}\n",
    "        counter = Counter(y_train)\n",
    "        # estimate scale_pos_weight value\n",
    "        estimate = counter[0] / counter[1]\n",
    "        print('Estimate: %.3f' % estimate)\n",
    "        \n",
    "        classifier_instance = xgb.XGBClassifier(scale_pos_weight=estimate, seed=42)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported classifier\")\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    def set_params(classifier, params):\n",
    "        if classifier == 'RandomForest':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            # params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            # params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        elif classifier == 'GBT':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        elif classifier == 'XGB':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            # params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            # params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        \n",
    "        else:\n",
    "            return params\n",
    "        \n",
    "    def objective(params):\n",
    "        params = set_params(classifier, params)\n",
    "        classifier_instance.set_params(**params)\n",
    "        \n",
    "        scores = cross_val_score(classifier_instance, X_train, y_train, cv=tscv, scoring='f1')\n",
    "        score = -scores.mean()\n",
    "        return score\n",
    "\n",
    "    def report_average(*args):\n",
    "        report_list = list()\n",
    "        for report in args:\n",
    "            splited = [' '.join(x.split()) for x in report.split('\\n\\n')]\n",
    "            header = [x for x in splited[0].split(' ')]\n",
    "            data = np.array(splited[1].split(' ')).reshape(-1, len(header) + 1)\n",
    "            data = np.delete(data, 0, 1).astype(float)\n",
    "            rest = splited[2].split(' ')\n",
    "            accuarcy =np.array([0, 0, rest[1], rest[2]]).astype(float).reshape(-1, len(header))\n",
    "            macro_avg = np.array([rest[5:9]]).astype(float).reshape(-1, len(header))\n",
    "            weighted_avg = np.array([rest[11:]]).astype(float).reshape(-1, len(header))\n",
    "            #avg_total = np.array([x for x in avg]).astype(float).reshape(-1, len(header))\n",
    "            df = pd.DataFrame(np.concatenate((data, accuarcy,macro_avg,weighted_avg)), columns=header)\n",
    "            report_list.append(df)\n",
    "        res = reduce(lambda x, y: x.add(y, fill_value=0), report_list) / len(report_list)\n",
    "        res.to_csv(f'when_{classifier}_results.csv')\n",
    "        return res.rename(index={res.index[-3]: 'accuracy',res.index[-2]: 'macro_avg',res.index[-1]: 'weighted_avg'})\n",
    "    \n",
    "    \n",
    "#     initial_model = classifier_instance\n",
    "#     initial_model = initial_model.fit(X_train, y_train)\n",
    "#     initial_preds = pd.DataFrame()\n",
    "#     print(\"INITIAL: \")\n",
    "#     initial_preds['prediction'] = initial_model.predict(X_test)\n",
    "#     initial_preds['label'] = y_test\n",
    "#     confusion_matrix_pandas(initial_preds)\n",
    "    \n",
    "    best_params = fmin(fn=objective, space=param_space, algo=tpe.suggest, max_evals=100, early_stop_fn=no_progress_loss(10))\n",
    "    \n",
    "    best_params = set_params(classifier, best_params)\n",
    "    classifier_instance.set_params(**best_params)\n",
    "    i = 0\n",
    "    final_recall = None\n",
    "    all_reports = []\n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        x_train, x_test = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "        Y_train, Y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        \n",
    "        classifier_instance.fit(x_train, Y_train)\n",
    "        \n",
    "        preds = classifier_instance.predict(x_test)\n",
    "        report = classification_report(Y_test, preds)\n",
    "        print(report)\n",
    "        all_reports.append(report)\n",
    "        # cm = confusion_matrix(Y_test, preds, labels=classifier_instance.classes_)\n",
    "        # plt.figure(figsize=(8, 6))\n",
    "        # sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
    "        # plt.xlabel(\"Predicted\")\n",
    "        # plt.ylabel(\"Actual\")\n",
    "        # plt.title(\"Confusion Matrix\")\n",
    "        # plt.show()\n",
    "        # final_recall = recall_score(Y_test, preds, pos_label=1)\n",
    "        # return recall_minority_class\n",
    "    # return final_recall\n",
    "    final_report = report_average(*all_reports)\n",
    "    print(final_report)\n",
    "    # print(\"MCC: \", matthews_corrcoef(true, preds))\n",
    "    final_report.to_csv(f'report_{classifier}_pred')\n",
    "        # f.write(f\"\\n\\nMatthews Correlation Coefficient: {matthews_corrcoef(true, preds)}\")\n",
    "        \n",
    "    return classifier_instance, X_train.columns.tolist(), X_train\n",
    "\n",
    "\n",
    "model, feats, X_train = model_testing(df, 'RandomForest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76602b7c-3a5e-4c5a-a903-9e20808a9e6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feats_to_drop = feature_importances(model,feats)\n",
    "print(feats_to_drop)\n",
    "df2 = df.drop(*feats_to_drop)\n",
    "model2, feats2, X_train2 = model_testing(df2, 'XGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abc9139-0926-4acc-8b62-5db6cabcd2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_ = feature_importances(model2,feats2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9cabfc-6e31-40d8-9725-fd4252132958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def anomaly_det(df):\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    seed = 42\n",
    "    df=df.toPandas()\n",
    "    print(\"Converted to Pandas\")\n",
    "    exclude_columns = ['fsym_id']\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.set_index('date', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    X_train = df.drop(exclude_columns, axis=1)\n",
    "    Y_train = df['label']\n",
    "    \n",
    "    features = X_train.columns[:-1]\n",
    "    print(len(features))\n",
    "    \n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        x_train, x_test = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "        y_train, y_test = Y_train.iloc[train_index], Y_train.iloc[test_index]\n",
    "\n",
    "        num_pos = len(y_train[y_train == 1])\n",
    "        print(num_pos/len(y_train))\n",
    "        isol_for = IsolationForest(contamination=num_pos/len(y_train), random_state=42)\n",
    "\n",
    "        isol_for.fit(x_train[features])\n",
    "\n",
    "        train_df = x_train.copy()  # Create a copy of the training set for results\n",
    "        test_df = x_test.copy()    # Create a copy of the test set for results\n",
    "\n",
    "        train_df['anomaly_scores'] = isol_for.decision_function(train_df[features])\n",
    "        train_df['anomaly'] = isol_for.predict(train_df[features])\n",
    "        train_df['preds'] = np.where(train_df['anomaly'] == 1, 0, 1)\n",
    "\n",
    "        test_df['anomaly_scores'] = isol_for.decision_function(test_df[features])\n",
    "        test_df['anomaly'] = isol_for.predict(test_df[features])\n",
    "        test_df['preds'] = np.where(test_df['anomaly'] == 1, 0, 1)\n",
    "\n",
    "        print(f\"Classification Report: \")\n",
    "        print(classification_report(y_test, test_df['preds']))\n",
    "        # cm = confusion_matrix(y_test, test_df['preds'])\n",
    "        # plt.figure(figsize=(8, 6))\n",
    "        # sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
    "        # plt.xlabel(\"Predicted\")\n",
    "        # plt.ylabel(\"Actual\")\n",
    "        # plt.title(\"Confusion Matrix\")\n",
    "        # plt.show()\n",
    "        \n",
    "   \n",
    "    \n",
    "    \n",
    "    \n",
    "# anomaly_det(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928754ab-2e6b-4b22-b6b0-86b7bfb22e30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def shapley(model, train, test):\n",
    "    # exclude_columns = ['fsym_id',  'label']\n",
    "    # X_train = train.drop(exclude_columns, axis=1)\n",
    "    # X_test = test.drop(exclude_columns, axis=1)\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer(train)\n",
    "    shap.initjs()\n",
    "    print(shap_values.shape)\n",
    "    shap.plots.waterfall(shap_values[0])\n",
    "    \n",
    "shapley(model2, X_train2, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecc5653-4922-4eee-8262-2d32b2b4f72a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "\n",
    "def anomaly_det2(df):\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    df = df.toPandas()\n",
    "    print(\"Converted to Pandas\")\n",
    "\n",
    "    exclude_columns = ['fsym_id']\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.set_index('date', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    X_train = df.drop(exclude_columns, axis=1)\n",
    "    Y_train = df['label']\n",
    "    \n",
    "    features = X_train.columns[:-1]\n",
    "\n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        x_train, x_test = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "        y_train, y_test = Y_train.iloc[train_index], Y_train.iloc[test_index]\n",
    "\n",
    "        num_pos = len(y_train[y_train == 1])\n",
    "        clf = OneClassSVM(nu=num_pos/len(y_train))  # Adjust the nu parameter\n",
    "        clf.fit(x_train[features])\n",
    "\n",
    "        train_df = x_train.copy()\n",
    "        test_df = x_test.copy()\n",
    "\n",
    "        train_df['anomaly_scores'] = clf.decision_function(train_df[features])\n",
    "        train_df['preds'] = np.where(train_df['anomaly_scores'] < 0, -1, 1)\n",
    "\n",
    "        test_df['anomaly_scores'] = clf.decision_function(test_df[features])\n",
    "        test_df['preds'] = np.where(test_df['anomaly_scores'] < 0, -1, 1)\n",
    "        \n",
    "        print(f\"Classification Report: \")\n",
    "        print(classification_report(y_test, np.where(test_df['preds'] == -1, 1, 0)))\n",
    "        cm = confusion_matrix(y_test, np.where(test_df['preds'] == -1, 1, 0))\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "# Call the function with your data\n",
    "# anomaly_det2(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de2078-0d47-4339-805d-67e3099f0a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Boruta import BorutaPy\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "def boruta_fs(train_df, model_name): #HOW DOES BORUTA ACC WORK?\n",
    "    train_df = train_df.toPandas()\n",
    "    X_train = train_df.drop(['fsym_id', 'date', 'label'], axis=1)\n",
    "    y_train = train_df['label']\n",
    "    \n",
    "    if model_name == 'rf':\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        model = GradientBoostingClassifier\n",
    "    feat_selector = BorutaPy(model, n_estimators='auto', verbose=1, random_state=1)\n",
    "    feat_selector.fit(X_train, y_train)\n",
    "    features = X_train.columns.tolist()\n",
    "    print(\"Number of features: \", len(features) )\n",
    "    feature_ranks = list(zip(features, feat_selector.ranking_, feat_selector.support_))\n",
    "    selected_features = []\n",
    "    for feat in feature_ranks:\n",
    "        print(f\"Feature: {feat[0]}, Rank: {feat[1]}, Keep: {feat[2]}\")\n",
    "        if feat[1] <= 5:\n",
    "            selected_features.append(feat[0])\n",
    "    print(\"Selected features: \", selected_features)\n",
    "    return selected_features\n",
    "\n",
    "rf_feats = boruta_fs(df, 'rf')\n",
    "# gbt_feats = boruta_fs(df, 'gbt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f66c89-d4c3-485b-93f5-c9fa821c0a82",
   "metadata": {},
   "source": [
    "### Investigating metrics that changed the most before and after implosions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f38521f-bf73-407e-a6f1-33f7a17eb635",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1fc72f-006a-44b5-83fd-21ba1e382ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "# import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "\n",
    "def pct_change_df(df, big_string, table):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, t.Implosion_Start_Date, b.date, {big_string}\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN sym_ticker_region s ON s.fsym_id = t.fsym_id\n",
    "                LEFT JOIN {table} a ON s.fsym_id = a.fsym_id AND  YEAR(a.date) = YEAR(t.Implosion_Start_Date)\n",
    "                LEFT JOIN {table} b ON s.fsym_id = b.fsym_id AND  YEAR(b.date) = YEAR(t.Implosion_Start_Date)-1\n",
    "                ORDER BY t.fsym_id\n",
    "            \"\"\"\n",
    "    df1 = spark.sql(query1)\n",
    "    #print(df1.show())\n",
    "    df1 = df1.toPandas()\n",
    "    df1 = df1.drop(['fsym_id','Implosion_Start_Date','date'], axis=1)\n",
    "    \n",
    "    def remove_outliers(column):\n",
    "        Q1 = column.quantile(0.25)\n",
    "        Q3 = column.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return column[(column >= lower_bound) & (column <= upper_bound)]\n",
    "\n",
    "\n",
    "\n",
    "    df1 = df1.abs()\n",
    "    null_percentage = df1.isnull().sum() / len(df1)\n",
    "    columns_to_keep = null_percentage[null_percentage <= 0.3].index\n",
    "    df_nulls_removed = df1[columns_to_keep]\n",
    "    print(\"Columns kept: \", len(columns_to_keep)/len(df1.columns))\n",
    "    \n",
    "    df_no_outliers = df_nulls_removed.apply(remove_outliers)\n",
    "\n",
    "    \n",
    "    column_means_no_outliers = df_no_outliers.mean()\n",
    "    #column_means_no_outliers = column_means_no_outliers.dropna()\n",
    "    column_means_no_outliers = column_means_no_outliers.sort_values()\n",
    "    feats = column_means_no_outliers.tail(5)\n",
    "\n",
    "    print(\"Largest averages of differences between previous year and implosion year: \",feats)\n",
    "    return feats.index.tolist()\n",
    "    \n",
    "def avg_change_df(df, big_string, table):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, {big_string}\n",
    "                FROM temp_table t  \n",
    "                LEFT JOIN sym_ticker_region s ON s.fsym_id = t.fsym_id\n",
    "                LEFT JOIN {table} a ON s.fsym_id = a.fsym_id AND  YEAR(a.date) > YEAR(t.Implosion_Start_Date)\n",
    "                LEFT JOIN {table} b ON s.fsym_id = b.fsym_id AND  YEAR(b.date) < YEAR(t.Implosion_Start_Date)\n",
    "                GROUP BY t.fsym_id\n",
    "                ORDER BY t.fsym_id\n",
    "            \"\"\"\n",
    "    df1 = spark.sql(query1)\n",
    "    df1 = df1.toPandas()\n",
    "    df1 = df1.drop(['fsym_id'], axis=1)\n",
    "    \n",
    "    def remove_outliers(column):\n",
    "        Q1 = column.quantile(0.25)\n",
    "        Q3 = column.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return column[(column >= lower_bound) & (column <= upper_bound)]\n",
    "\n",
    "\n",
    "    df1 = df1.abs()\n",
    "    null_percentage = df1.isnull().sum() / len(df1)\n",
    "    columns_to_keep = null_percentage[null_percentage <= 0.3].index\n",
    "    df_nulls_removed = df1[columns_to_keep]\n",
    "    print(\"Columns kept: \", len(columns_to_keep)/len(df1.columns))\n",
    "    \n",
    "    df_no_outliers = df_nulls_removed.apply(remove_outliers)\n",
    "    \n",
    "    column_means_no_outliers = df_no_outliers.mean()\n",
    "    #column_means_no_outliers = column_means_no_outliers.dropna()\n",
    "    column_means_no_outliers = column_means_no_outliers.sort_values()\n",
    "    feats = column_means_no_outliers.tail(5)\n",
    "    print(\"Largest averages of differences in average before and after implosion date: \", feats)\n",
    "#     for feature in feats.index:\n",
    "#         before_implosion = df_no_outliers[feature][df_no_outliers.index.isin(df1[df1[feature].notnull() & (df1['date'] < df1['Implosion_Start_Date'])].index)]\n",
    "#         after_implosion = df_no_outliers[feature][df_no_outliers.index.isin(df1[df1[feature].notnull() & (df1['date'] > df1['Implosion_Start_Date'])].index)]\n",
    "        \n",
    "#         _, p_value = ttest_ind(before_implosion, after_implosion)\n",
    "        \n",
    "#         print(f\"T-test p-value for {feature}: {p_value}\")\n",
    "    return feats.index.tolist()\n",
    "\n",
    "def t_test():\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_metric_changes(filename, table):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df = df[df['Implosion_Start_Date'].notnull()]\n",
    "    df['Implosion_Start_Date'] = pd.to_datetime(df['Implosion_Start_Date']).dt.date\n",
    "    df['Implosion_End_Date'] = pd.to_datetime(df['Implosion_End_Date']).dt.date\n",
    "    cols = get_not_null_cols(df, table)\n",
    "    result_string = ', '.join('(a.' + item + '-b.' + item +')/b.'+item + ' AS ' + item for item in cols)\n",
    "    feats1 = pct_change_df(df, result_string, table) #change 1 year before\n",
    "    print(\"Features with greatest percentage change with year before implosion: \", feats1)\n",
    "    \n",
    "    result_string2 = ', '.join('(MEAN(a.' + item + ')-MEAN(b.' + item +'))/MEAN(b.'+item + ') AS ' + item for item in cols)\n",
    "    feats2 = avg_change_df(df, result_string2, table)\n",
    "    print(\"Features with greatest percentage change in mean before and after implosion\", feats2)\n",
    "    \n",
    "    write_features_file( list(set(feats1+feats2)) )\n",
    "\n",
    "\n",
    "get_metric_changes('imploded_stocks_price.csv', 'FF_ADVANCED_DER_AF')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7274d-d2b0-4016-a2b6-4f7633f51fe7",
   "metadata": {},
   "source": [
    "### Correlations with Market Value Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc63eb-7d98-4de8-b665-808509638bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from CreateDataset import get_feature_col_names, get_fund_data\n",
    "\n",
    "\n",
    "def corr_query(implosion_df, col_string, table): \n",
    "    df = get_fund_data(implosion_df)\n",
    "    df=df.withColumn('year', F.year('date'))\n",
    "    window_spec = Window.partitionBy('fsym_id', 'year').orderBy(col('date').desc())\n",
    "\n",
    "    df = df.withColumn('row_num', F.row_number().over(window_spec))\n",
    "\n",
    "    df = df.filter(col('row_num') == 1).orderBy('date') #should we compare correlations with market val?\n",
    "    #should we do quarterly?\n",
    "    \n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, t.adj_price, t.Market_Value, t.date, {col_string}\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id AND YEAR(t.date)=YEAR(a.date)\n",
    "                ORDER BY t.fsym_id, t.date\n",
    "            \"\"\"\n",
    " \n",
    "    q_df = spark.sql(query1)\n",
    "    #q_df.show()\n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy('date')\n",
    "    \n",
    "    q_df = q_df.withColumn(\"return_market_val\", (F.col('Market_Value') - F.lag('Market_Value').over(window_spec)) / F.lag('Market_Value').over(window_spec))\n",
    "    q_df = q_df.withColumn(\"return\", (F.col('adj_price') - F.lag('adj_price').over(window_spec)) / F.lag('adj_price').over(window_spec))\n",
    "    \n",
    "    return_columns = [c[2:] for c in col_string.split(\", \")]\n",
    "    mean_corrs = []\n",
    "    corr_vals = []\n",
    "    #I THINK U NEED TO GROUP BY DATE AND THEN CALCULATE CORRELATIONS\n",
    "\n",
    "    for column in return_columns:\n",
    "        return_col_name = f\"return_{column}\"\n",
    "        corr_col_name = f\"corr_with_{column}\"\n",
    "        q_df = q_df.withColumn(return_col_name, (F.col(column) - F.lag(column).over(window_spec)) / F.lag(column).over(window_spec))\n",
    "        q_df = q_df.withColumn(column, F.corr(return_col_name, 'return_market_val').over(window_spec)) #calculating correlations with market value return\n",
    "        q_df = q_df.drop(*[return_col_name])\n",
    "    q_df = q_df.drop(*['return_market_val', 'return'])\n",
    "    q_df = q_df.select(q_df.columns[4:])\n",
    "    mean_corrs = q_df.agg(*[F.mean(F.abs(F.col(column))).alias(column) for column in q_df.columns])\n",
    "    # mean_corrs.show()\n",
    "    \n",
    "    return mean_corrs.toPandas()\n",
    "\n",
    "def corr_analysis(table):\n",
    "    imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "    imp_df_price = imp_df_price.loc[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "    cols = get_not_null_cols(imp_df_price, 'FF_ADVANCED_DER_AF')\n",
    "    result_string = ', '.join('a.' + item for item in cols)\n",
    "    mean_corrs_df = corr_query(spark.createDataFrame(imp_df_price), result_string, 'FF_ADVANCED_DER_AF')\n",
    "    mean_corrs = mean_corrs_df.to_dict(orient='records')\n",
    "    sorted_corrs = dict(sorted(mean_corrs[0].items(), key=lambda item: item[1], reverse=True))\n",
    "    top_records = list(sorted_corrs.items())[:5]\n",
    "    top_10 = []\n",
    "    for r in top_records:\n",
    "        top_10.append(r[0])\n",
    "    print(top_10)\n",
    "    current_feature_list = get_feature_col_names()\n",
    "    new_feature_list = list(set(current_feature_list + top_10))\n",
    "    \n",
    "    write_features_file(new_feature_list)\n",
    "    \n",
    "    \n",
    "corr_analysis('FF_Advanced_Der_AF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3722ccc-0c58-4599-a464-6e153f4e1f13",
   "metadata": {},
   "source": [
    "### Adding the Extra Features From Literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8586949c-01ca-4b25-88c1-1488355015e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "imp_df_price['Implosion_Start_Date'] = pd.to_datetime(imp_df_price['Implosion_Start_Date'])\n",
    "imp_df_price['Implosion_End_Date'] = pd.to_datetime(imp_df_price['Implosion_End_Date'])\n",
    "available_feats = get_not_null_cols(imp_df_price)\n",
    "extra_feats = ['ff_capex_assets', 'ff_gross_cf_debt', 'ff_mkt_val_gr']\n",
    "\n",
    "current_feats = get_feature_col_names()\n",
    "final_feats = list(set(current_feats + extra_feats))\n",
    "write_features_file(final_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca6a8a0-1c51-42e5-8a36-18044f9e9bc4",
   "metadata": {},
   "source": [
    "### Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203b078-0417-4933-afe8-4442a98809ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(all_feats=False, imploded_only=False):\n",
    "    df = get_tabular_dataset(all_feats=all_feats, imploded_only=imploded_only)\n",
    "    df = forward_fill(df)\n",
    "    print(\"Number of rows: \", df.count())\n",
    "    print(\"Number of positives: \", df.filter(F.col('label')==1).count())\n",
    "    df=df.fillna(0.0)\n",
    "    print(\"Number of rows after dropping nulls: \", df.count())\n",
    "    print(\"Number of positives after dropping nulls: \", df.filter(F.col('label')==1).count())\n",
    "    return df\n",
    "\n",
    "\n",
    "def forward_fill(df):\n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy('date')\n",
    "    feature_cols = df.columns[2:-1]\n",
    "    for c in feature_cols:\n",
    "        df = df.withColumn(\n",
    "            c, F.last(c, ignorenulls=True).over(window_spec)\n",
    "        )\n",
    "    return df.orderBy('fsym_id','date')\n",
    "\n",
    "df = get_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0e0a7-413f-4d65-9fec-31092302bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Boruta import BorutaPy\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "def boruta_fs(train_df, model_name): #HOW DOES BORUTA ACC WORK?\n",
    "    train_df = train_df.toPandas()\n",
    "    X_train = train_df.drop(['fsym_id', 'date', 'label'], axis=1)\n",
    "    y_train = train_df['label']\n",
    "    \n",
    "    if model_name == 'rf':\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        model = GradientBoostingClassifier\n",
    "    feat_selector = BorutaPy(model, n_estimators='auto', verbose=2, random_state=1)\n",
    "    feat_selector.fit(X_train, y_train)\n",
    "    features = X_train.columns.tolist()\n",
    "    print(\"Number of features: \", len(features) )\n",
    "    feature_ranks = list(zip(features, feat_selector.ranking_, feat_selector.support_))\n",
    "    selected_features = []\n",
    "    for feat in feature_ranks:\n",
    "        print(f\"Feature: {feat[0]}, Rank: {feat[1]}, Keep: {feat[2]}\")\n",
    "        if feat[1] <= 5:\n",
    "            selected_features.append(feat[0])\n",
    "    print(\"Selected features: \", selected_features)\n",
    "    return selected_features\n",
    "\n",
    "rf_feats = boruta_fs(df, 'rf')\n",
    "gbt_feats = boruta_fs(df, 'gbt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f1dd31-27ce-42b8-ac91-41d2ebf9876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_features = get_feature_col_names()\n",
    "# for f in boruta_features:\n",
    "#     if f in current_features:\n",
    "#         print(f)\n",
    "# final_features = list(set(boruta_features + current_features))\n",
    "# write_features_file(final_features) #in the feature selection pipeline, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c21d5-a37f-4211-a2dc-a32a14a41cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def correlation_matrix(df):\n",
    "    df =df.toPandas()\n",
    "    print(\"Converted to Pandas\")\n",
    "    corr_df = df.drop(['date','fsym_id'], axis=1)\n",
    "    corr_mat = corr_df.corr()\n",
    "    mask = np.triu(np.ones_like(corr_mat))\n",
    "    plt.figure(figsize=(50, 40))\n",
    "    sns.heatmap(corr_mat, annot=True, cmap='coolwarm', fmt=\".2f\", mask=mask)\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('corr_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Variable pairs with absolute correlation above 0.7:\")\n",
    "    for i in range(len(corr_mat.columns)):\n",
    "        for j in range(i+1, len(corr_mat.columns)):\n",
    "            if abs(corr_mat.iloc[i, j]) >= 0.7:\n",
    "                print(f\"{corr_mat.columns[i]} - {corr_mat.columns[j]}: {corr_mat.iloc[i, j]}\")\n",
    "                \n",
    "# correlation_matrix(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ea6a0-81ff-4b2b-9dd0-f6fff61d7fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('ff_div_yld_secs', 'ff_earn_yld', 'ff_roa_ptx', 'ff_net_inc_basic_aft_xord', 'ff_net_inc_dil', 'ff_oper_inc_aft_unusual', \n",
    "                        'ff_net_inc_dil_aft_xord', 'ff_net_inc_dil_bef_unusual', 'ff_ebit_bef_unusual', 'ff_eps_dil_gr', 'GDP', 'ff_bk_oper_inc_tot')\n",
    "feats = df.columns[2:-1]\n",
    "# write_features_file(feats)\n",
    "feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb6e21-44f3-4b26-b63b-a45b4ba8056a",
   "metadata": {},
   "source": [
    "### Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385f503-98bc-4b81-938c-27e8225f53b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_dates(imp_df_price):\n",
    "    price_data = get_fund_data(spark.createDataFrame(imp_df_price))\n",
    "    #cols = get_not_null_cols(imp_df_price, 'FF_ADVANCED_DER_AF')\n",
    "    #result_string = ', '.join('a.' + item for item in cols)\n",
    "    \n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy(col('p_date'))\n",
    "\n",
    "    price_data = price_data.withColumn('row_num', F.row_number().over(window_spec))\n",
    "    price_data.show()\n",
    "\n",
    "    price_data = price_data.filter(col('row_num') == 1).orderBy(col('p_date').desc())\n",
    "    price_data.show()\n",
    "    \n",
    "    start_dates = price_data.groupBy('year').count().orderBy('year')\n",
    "    years = [row['year'] for row in start_dates.collect()]\n",
    "    counts = [row['count'] for row in start_dates.collect()]\n",
    "    plt.bar(years, counts)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Start Dates Count per Year')\n",
    "    plt.show()\n",
    "    #start_dates.show(25)\n",
    "    \n",
    "def null_vals(imp_df_price, table):\n",
    "    price_data = get_fund_data(spark.createDataFrame(imp_df_price))\n",
    "    cols = get_not_null_cols(imp_df_price, table)\n",
    "    col_string = ', '.join('a.' + item for item in cols)\n",
    "    price_data.createOrReplaceTempView('temp_table')\n",
    "    null_counts = []\n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, t.split_adj_price, t.Market_Value, t.p_date, {col_string}\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id AND YEAR(t.p_date)=YEAR(a.date)\n",
    "                ORDER BY t.fsym_id, t.p_date\n",
    "            \"\"\"\n",
    "    full_df = spark.sql(query1)\n",
    "    for column in cols:\n",
    "        null_count = full_df.select(column).filter(col(column).isNull()).count()\n",
    "        null_counts.append((column, null_count))\n",
    "    null_counts_df = pd.DataFrame(null_counts, columns=['Column', 'Null Count'])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(null_counts_df['Column'], null_counts_df['Null Count'])\n",
    "    plt.xlabel('Column')\n",
    "    plt.ylabel('Null Count')\n",
    "    plt.title('Null Counts for Each Column')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # null_counts = price_data.groupBy('year').agg(F.sum(col('p_price').isNull().cast('int')).alias('null_count'))\n",
    "    # null_counts.show()\n",
    "    \n",
    "imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "imp_df_price_imploded = imp_df_price.loc[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "start_dates(imp_df_price)\n",
    "start_dates(imp_df_price_imploded)\n",
    "\n",
    "#null_vals(imp_df_price, 'FF_ADVANCED_DER_AF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345755a7-1bf1-442d-9418-576cb9688733",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "imp_df_test = imp_df_price[imp_df_price['fsym_id']=='H7CTYF-R']\n",
    "df = get_fund_data(spark.createDataFrame(imp_df_test))\n",
    "df.show(1000)\n",
    "imp_df_imp = imp_df_price[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "print(len(imp_df_imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730ca87-195a-49a3-87e3-60f22f57b015",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df_imp = imp_df_price[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "print(len(imp_df_imp))\n",
    "print(len(imp_df_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19776f2-22bb-4ab6-a748-cb9928010b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols():\n",
    "    df_metrics = ps.DataFrame(spark.sql(\"SELECT * FROM FF_BASIC_AF LIMIT 10\")) #get all the metrics\n",
    "    cols = []\n",
    "    for c in df_metrics.columns:\n",
    "        if df_metrics[c].dtype=='float64':#get all the metrics we can calculate correlations with\n",
    "            cols.append(c)\n",
    "    return cols\n",
    "\n",
    "#%change average of each feature plotted for pharmacy industry\n",
    "def industry_analysis():\n",
    "    stock_df = get_all_stocks_df()\n",
    "    #stock_df = pd.read_csv('imploded_stocks.csv')\n",
    "    #stock_df = spark.createDataFrame(stock_df)\n",
    "    cols = ['ff_gross_inc', 'ff_sales', 'FF_OPER_EXP_TOT', 'FF_CASH_ST']\n",
    "    col_string = ', '.join('a.' + item for item in cols)\n",
    "    stock_df.createOrReplaceTempView(\"temp_table\")\n",
    "    q = f\"\"\"SELECT e.factset_industry_desc, t.ticker_region, a.date, {col_string} FROM temp_table t\n",
    "    LEFT JOIN FF_BASIC_AF a ON a.fsym_id = t.fsym_id\n",
    "    LEFT JOIN sym_coverage sc ON sc.fsym_id = t.fsym_id\n",
    "    LEFT JOIN ff_sec_entity_hist c on c.fsym_id=sc.fsym_security_id\n",
    "    LEFT JOIN sym_entity_sector d on d.factset_entity_id=c.factset_entity_id\n",
    "    LEFT JOIN factset_industry_map e on e.factset_industry_code=d.industry_code\n",
    "    WHERE a.date >= \"2009-01-01\" AND e.factset_industry_desc=\"Regional Banks\"\n",
    "    ORDER BY t.ticker_region,a.date\"\"\"\n",
    "    ind_df = spark.sql(q)\n",
    "    #print(ind_df.show(10))\n",
    "    ind_df =ind_df.toPandas()\n",
    "    ind_df['date'] = pd.to_datetime(ind_df['date'])\n",
    "    new_cols = []\n",
    "    for column in cols:\n",
    "        ind_df[f'{column}_percentage_change'] = ind_df.groupby('ticker_region')[column].pct_change() * 100\n",
    "        ind_df[f'{column}_percentage_change'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        ind_df.drop(column, axis=1, inplace=True)\n",
    "        new_cols.append(f'{column}_percentage_change')\n",
    "    ind_df['year'] = ind_df['date'].dt.year\n",
    "    avg_pct_change = ind_df.groupby(['year'])[new_cols].mean().reset_index()\n",
    "    print(avg_pct_change.head(20))\n",
    "    num_rows = (len(new_cols) + 1) // 2  # Adjust the number of rows as needed\n",
    "    num_cols = 2\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n",
    "    for i,column in enumerate(new_cols):\n",
    "        row = i//num_cols\n",
    "        col = i % num_cols \n",
    "        axes[row,col].plot(avg_pct_change['year'], avg_pct_change[column])\n",
    "        axes[row, col].set_title(f'Avg {column} Percentage Change Over Time')\n",
    "        axes[row, col].set_xlabel('Year')\n",
    "        axes[row, col].set_ylabel(f'Avg {column} Percentage Change')\n",
    "        axes[row, col].grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#industry_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700de0e-2f78-48aa-8c16-e12e167d67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#YOU'VE DONE WORST CHANGES NOW FIND OUT WHICH ONES DECREASE CONSISTENTLY\n",
    "#ALSO FIGURE OUT MEANS BEFORE PERIOD AND AFTER PERIOD USING QUARTERLY AND COMPARE DIFF\n",
    "#FINALLY WITH A HUGE LIST USE BORUTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b54c9-5ddc-4459-afff-5247cc6b7b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_not_null_cols(df, table='FF_ADVANCED_DER_AF'):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query1 = f\"\"\"SELECT t.fsym_id, a.*\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id\n",
    "                ORDER BY t.fsym_id, a.date\n",
    "            \"\"\"\n",
    "    #we get all the available dates per stock, so these null values are only within the timeframe available\n",
    "    q_df = spark.sql(query1)\n",
    "    column_types = q_df.dtypes\n",
    "    null_pcts = []\n",
    "    for c, dtype in zip(q_df.columns, column_types):\n",
    "        if dtype[1] == 'double':\n",
    "            null_count = q_df.filter(F.col(c).isNull()).count()\n",
    "            null_pcts.append(null_count/q_df.count())\n",
    "\n",
    "\n",
    "    columns_to_drop = [col_name for col_name, null_pct, dtype in zip(q_df.columns, null_pcts, column_types) if null_pct > 0.2 or dtype[1]!='double']\n",
    "\n",
    "    q_df = q_df.drop(*columns_to_drop)\n",
    "\n",
    "    cols = q_df.columns\n",
    "    print(cols)\n",
    "\n",
    "    return cols\n",
    "    \n",
    "df = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "df = df.loc[df['Implosion_Start_Date'].notnull()]\n",
    "get_not_null_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d7b62c-becd-45ee-bcb2-8c8c59e8e0b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4997ed0d-ad2e-4ec5-9b4b-7818b3a857f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
