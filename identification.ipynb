{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='2022_10_22', catalog='spark_catalog', description='FactSet data version for the day', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse'),\n",
       " Database(name='2023_04_01', catalog='spark_catalog', description='FactSet data version for the day', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse'),\n",
       " Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"Identification\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "spark.sql(\"USE 2023_04_01\")\n",
    "    # Assuming that 'ticker' is a valid Python variable\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n",
      "|ticker_region| fsym_id|\n",
      "+-------------+--------+\n",
      "|          UGI|RS288R-R|\n",
      "|          KOD|P713TW-R|\n",
      "|         NKLA|V9KKT0-R|\n",
      "|         TWTR|S9NVWV-R|\n",
      "|         NETI|FL275K-R|\n",
      "|         EDLG|DKMTLY-R|\n",
      "|         FXNC|T7H7M3-R|\n",
      "|         PLTE|VT8SCL-R|\n",
      "|         SMED|PY52L3-R|\n",
      "|         VTEC|V329V0-R|\n",
      "|         ZUMZ|MCRFNX-R|\n",
      "|          FRC|GYM5PB-R|\n",
      "|         TLCX|R1C6H0-R|\n",
      "|         LVOX|FNGK9P-R|\n",
      "|          TEA|MSQ612-R|\n",
      "|          SJA|M2X60Y-R|\n",
      "|         OVBC|VCNTLL-R|\n",
      "|          NVG|C9XQRP-R|\n",
      "|         NPCE|VC909R-R|\n",
      "|          FFC|PW0PZT-R|\n",
      "+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 405\u001b[0m\n\u001b[1;32m    370\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimploded_stocks2.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFalse\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# ticker_list = sorted(get_all_stocks()[:5000])\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# create_imploded_df(ticker_list)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;66;03m# df = pd.read_csv('imploded_stocks.csv')\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;66;03m# print(len(df['Ticker'].unique().tolist()))\u001b[39;00m\n\u001b[0;32m--> 405\u001b[0m \u001b[43mget_all_stocks_prices\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [3], line 263\u001b[0m, in \u001b[0;36mget_all_stocks_prices\u001b[0;34m()\u001b[0m\n\u001b[1;32m    235\u001b[0m df\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstocks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    236\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mSELECT DISTINCT s.fsym_id, sc.proper_name, p.p_date, p.p_price,\u001b[39m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;124m            divs.p_divs_exdate, divs.p_divs_s_pd, divs.p_divs_pd,\u001b[39m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124m            splits.p_split_date, splits.p_split_factor, str.ticker_region\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;124m            ORDER BY str.ticker_region ASC, p_date DESC\u001b[39m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124m            \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m--> 263\u001b[0m adj \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery done\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    265\u001b[0m adj\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import norm\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, to_date, lit, udf, pandas_udf, PandasUDFType, coalesce, \\\n",
    "                                month, year, concat, date_format, format_string, last_day, months_between, greatest, least\n",
    "from datetime import timedelta\n",
    "from pyspark.sql.types import*\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "\n",
    "\n",
    "start_date = '2000-01-01'\n",
    "end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "def get_stock_price_weekly(ticker):\n",
    "    # Suppress the progress message from yfinance\n",
    "    temp_df = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "    if temp_df.empty:\n",
    "        print(\"No data available for the specified date range.\")\n",
    "        return None\n",
    "    weekly_data = temp_df['Adj Close'].resample('W').last()\n",
    "    return weekly_data\n",
    "\n",
    "def get_stock_prices(ticker):\n",
    "    # Suppress the progress message from yfinance\n",
    "    query = f\"\"\"SELECT s.ticker_region, p.p_date, p.p_price FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                LEFT JOIN fp_basic_prices p ON p.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\n",
    "                ORDER BY p.p_date\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df.show(10)\n",
    "\n",
    "def plot_price(ticker):\n",
    "    # Suppress the progress message from yfinance\n",
    "    temp_df = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "    if temp_df.empty:\n",
    "        #print(\"No data available for the specified date range.\")\n",
    "        return None\n",
    "    weekly_data = temp_df['Adj Close'].resample('W').last()\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(weekly_data.index, weekly_data, label=ticker)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_cum_returns(ticker):\n",
    "    # Suppress the progress message from yfinance\n",
    "    temp_df = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "    if temp_df.empty:\n",
    "        #print(\"No data available for the specified date range.\")\n",
    "        return None\n",
    "    weekly_data = temp_df['Adj Close'].resample('W').last().to_frame()\n",
    "    print(weekly_data.head())\n",
    "    weekly_data['returns']  = weekly_data['Adj Close'].pct_change()\n",
    "    weekly_data['cumulative_returns'] = (1 + weekly_data['returns']).cumprod() - 1\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(weekly_data.index, weekly_data['cumulative_returns'], label=ticker)\n",
    "    #plt.plot(weekly_data.index, weekly_data['Adj Close'], color='yellow')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def check_implosion(idx, firm_price, imp_thresh):\n",
    "    i = idx\n",
    "    start_price=firm_price.iloc[idx]\n",
    "    i+=1\n",
    "    period=0\n",
    "    while i < len(firm_price):\n",
    "        current_date = firm_price.index[i]\n",
    "        current_price = firm_price.iloc[i]\n",
    "        if (current_price-start_price)/start_price > 0.2:\n",
    "            return period\n",
    "        period+=1\n",
    "        i+=1\n",
    "    return period\n",
    "\n",
    "def get_crash_dates(firm_price, price_drop_thresh=-0.6, period_thresh=52):\n",
    "    crash_dates = []\n",
    "    imp_dates = []\n",
    "    i = 52\n",
    "    while i < len(firm_price):\n",
    "        current_date = firm_price.index[i]\n",
    "        current_price = firm_price.iloc[i]\n",
    "        prev_year_data = firm_price.iloc[i-52:i]\n",
    "        if len(prev_year_data) != 0:\n",
    "            mean_price = prev_year_data.mean()\n",
    "            if (current_price - mean_price)/mean_price < price_drop_thresh:\n",
    "                imp_dates.append(current_date)\n",
    "        i+=1\n",
    "    return imp_dates\n",
    "\n",
    "def get_implosion_dates(firm_price, price_drop_thresh=-0.6, period_thresh=52):\n",
    "    crash_dates = []\n",
    "    imp_dates = []\n",
    "    i = 52\n",
    "    while i < len(firm_price):\n",
    "        current_date = firm_price.index[i]\n",
    "        current_price = firm_price.iloc[i]\n",
    "        prev_year_data = firm_price.iloc[i-52:i]\n",
    "        if len(prev_year_data) != 0:\n",
    "            mean_price = prev_year_data.mean()\n",
    "            if (current_price - mean_price)/mean_price < price_drop_thresh:\n",
    "                imp_period = check_implosion(i, firm_price,  price_drop_thresh)\n",
    "                st_date = current_date\n",
    "                end_date = firm_price.index[i+imp_period]\n",
    "                if imp_period > period_thresh:\n",
    "                    imp_dates.append((current_date, firm_price.index[i+imp_period]))\n",
    "                i+=imp_period\n",
    "        i+=1\n",
    "    return imp_dates\n",
    "\n",
    "def plot_implosions(stock_series, imp_dates, ticker, ax):\n",
    "    #plt.figure(figsize=(15, 5))\n",
    "    ax.plot(stock_series.index, stock_series, label=ticker)\n",
    "    for i in imp_dates:\n",
    "        ax.axvspan(i[0], i[1], alpha=0.5, color='blue')\n",
    "    ax.legend()\n",
    "    #plt.show()\n",
    "\n",
    "# def run_imps(stocks_list):\n",
    "#     num_imp = 0\n",
    "#     j = 0\n",
    "#     fig, axs = plt.subplots(nrows=len(stocks_list), figsize=(15, 5*len(stocks_list)))\n",
    "#     # for t in stocks_list:\n",
    "#     #     stock_series = get_stock_price_weekly(t)\n",
    "#     #     if stock_series is not None:\n",
    "#     #         imp_dates = get_implosion_dates(stock_series)\n",
    "#     #         if j % 10 == 0:\n",
    "#     #             plot_implosions(stock_series, imp_dates, t, axs)    \n",
    "#     #         j+=1\n",
    "#     #         if len(imp_dates) >= 1:\n",
    "#     #             num_imp+=1\n",
    "#     #         # if len(imp_dates) ==0:\n",
    "#     #         #     plot_implosions(stock_series, imp_dates, t)\n",
    "#     # print(f\"{num_imp} out of {j} imploded\")\n",
    "#     # plt.savefig('all_implosions.png')\n",
    "#     # plt.close()\n",
    "#     # return num_imp\n",
    "#     for t, ax in zip(stocks_list, axs):\n",
    "#         stock_series = get_stock_price_weekly(t)\n",
    "#         if stock_series is not None:\n",
    "#             imp_dates = get_implosion_dates(stock_series)\n",
    "            \n",
    "#             # Plot every 10th graph\n",
    "#             if j % 10 == 0:\n",
    "#                 plot_implosions(stock_series, imp_dates, t, ax=ax)\n",
    "            \n",
    "#             j += 1\n",
    "\n",
    "#             if len(imp_dates) >= 1:\n",
    "#                 num_imp += 1\n",
    "    \n",
    "#     # Save the final figure after the loop\n",
    "#     plt.savefig('all_implosions_subplots.png')\n",
    "    \n",
    "#     # Close the Matplotlib figure to release resources\n",
    "#     plt.close()\n",
    "    \n",
    "#     print(f\"{num_imp} out of {j} imploded\")\n",
    "#     return num_imp\n",
    "def run_imps(stocks_list, columns=3):\n",
    "    num_imp = 0\n",
    "    j = 0\n",
    "    num_rows = math.ceil(len(stocks_list) / columns)\n",
    "    fig, axs = plt.subplots(nrows=num_rows, ncols=columns, figsize=(15, 5*num_rows))\n",
    "\n",
    "    for t, ax in zip(stocks_list, axs.flatten()):\n",
    "        stock_series = get_stock_price_weekly(t)\n",
    "        if stock_series is not None:\n",
    "            imp_dates = get_implosion_dates(stock_series)   \n",
    "            plot_implosions(stock_series, imp_dates, t, ax=ax)       \n",
    "            j += 1\n",
    "            if len(imp_dates) >= 1:\n",
    "                num_imp += 1\n",
    "\n",
    "    for i in range(len(stocks_list), num_rows * columns):\n",
    "        fig.delaxes(axs.flatten()[i])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('all_implosions_subplots.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"{num_imp} out of {j} imploded\")\n",
    "    return num_imp\n",
    "\n",
    "\n",
    "def plot_crashes(ticker):\n",
    "    stock_series = get_stock_price_weekly(ticker)\n",
    "    crash_dates = get_crash_dates(stock_series)\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(stock_series.index, stock_series, label=ticker)\n",
    "    for c in crash_dates:\n",
    "        plt.axvspan(c,c, alpha=0.5, color='blue')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def get_all_stocks():\n",
    "    query = f\"\"\"SELECT s.ticker_region, sc.fref_listing_exchange FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    ticker_list = [row.ticker_region for row in df.collect()]\n",
    "    return ticker_list\n",
    "\n",
    "def get_all_stocks_df():\n",
    "    query = f\"\"\"SELECT s.ticker_region, s.fsym_id FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    return df\n",
    "\n",
    "def get_all_stocks_prices():\n",
    "    df = get_all_stocks_df()\n",
    "    df.show()\n",
    "    df.createOrReplaceTempView(\"stocks\")\n",
    "    query = \"\"\"SELECT DISTINCT s.fsym_id, sc.proper_name, p.p_date, p.p_price,\n",
    "                divs.p_divs_exdate, divs.p_divs_s_pd, divs.p_divs_pd,\n",
    "                splits.p_split_date, splits.p_split_factor, str.ticker_region\n",
    "\n",
    "                FROM sym_entity_sector AS e\n",
    "\n",
    "                INNER JOIN fp_sec_entity AS s ON e.factset_entity_id = s.factset_entity_id\n",
    "                INNER JOIN sym_coverage AS sc ON s.fsym_id = sc.fsym_security_id \n",
    "                INNER JOIN fp_basic_prices AS p ON sc.fsym_regional_id = p.fsym_id\n",
    "                INNER JOIN factset_sector_map AS sm ON e.sector_code = sm.factset_sector_code\n",
    "                INNER JOIN ff_basic_qf AS ff ON p.fsym_id = ff.fsym_id\n",
    "                LEFT JOIN fp_basic_dividends AS divs ON divs.p_divs_exdate = p.p_date AND p.fsym_id = divs.fsym_id\n",
    "                LEFT JOIN fp_basic_splits AS splits ON splits.p_split_date = p.p_date AND p.fsym_id = splits.fsym_id \n",
    "                LEFT JOIN sym_ticker_region str ON str.fsym_id = p.fsym_id\n",
    "\n",
    "                WHERE sc.fsym_security_id IS NOT NULL\n",
    "                        AND sc.fref_security_type IS NOT NULL AND sc.fref_security_type = 'SHARE'\n",
    "                        AND sc.currency IS NOT NULL AND sc.currency = 'USD'\n",
    "                        AND s.factset_entity_id IS NOT NULL\n",
    "                        AND e.sector_code IS NOT NULL\n",
    "                        AND p.p_date IS NOT NULL\n",
    "                        AND (sc.fref_listing_exchange = 'NAS' OR sc.fref_listing_exchange = 'NYS')\n",
    "                        AND p.p_date >= '2000-01-01'\n",
    "                        AND str.ticker_region = 'AAPL-US'\n",
    "\n",
    "                ORDER BY str.ticker_region ASC, p_date DESC\n",
    "                \"\"\"\n",
    "    adj = spark.sql(query)\n",
    "    print(\"query done\")\n",
    "    adj.show(10, False)\n",
    "    \n",
    "    adj = adj.withColumn(\"temp_cum_split_factor\", when(adj.p_date==adj.p_split_date, lit(adj.p_split_factor)).otherwise(lit(1.0)))\n",
    "    adj = adj.withColumn(\"div_split_factor\", lit(0.0)) # placeholders\n",
    "    adj = adj.withColumn(\"cum_split_factor\", lit(0.0)) # placeholders\n",
    "    adj = adj.withColumn(\"split_temp_i\", lit(0)) # placeholders - for ordering purposes\n",
    "\n",
    "    # creating udf to calculate cumulative split factor\n",
    "    @pandas_udf(adj.schema, FloatType(), PandasUDFType.GROUPED_MAP)\n",
    "    def calc_product_factor(df1):\n",
    "        \"\"\"\n",
    "        Calculates the cumulative split factor for each company based on unique fsym_id's,\n",
    "        for both the price split and the dividend split.\n",
    "        The data MUST be sorted within the function itself (no orderBy in the function call),\n",
    "        and spin_temp_i must be set to i during each iteration of the loop to guarantee\n",
    "        proper sorting - without these safeguards, the function is applied non-sequentially\n",
    "        to the data.\n",
    "        \"\"\"\n",
    "        df1 = df1.sort_values(by='p_date', ascending=False)\n",
    "        for i in range(0, len(df1)):\n",
    "            df1.loc[i, 'split_temp_i'] = i\n",
    "            if i == 0:\n",
    "                df1.loc[i, 'cum_split_factor'] = 1.0\n",
    "                df1.loc[i, 'div_split_factor'] = 1.0\n",
    "                continue\n",
    "            df1.loc[i-1, 'div_split_factor'] = df1.loc[i-1, 'cum_split_factor'] * df1.loc[i-1, 'temp_cum_split_factor']\n",
    "            df1.loc[i, 'cum_split_factor'] = df1.loc[i-1, 'cum_split_factor'] * df1.loc[i-1, 'temp_cum_split_factor']\n",
    "        return df1\n",
    "\n",
    "    adj = adj.groupBy('ticker_region').apply(calc_product_factor)\n",
    "    adj = adj.withColumn(\"split_adj_price\", (adj.p_price*adj.cum_split_factor))\n",
    "    adj = adj.sort(col('ticker_region').asc(), col('p_date').asc())\n",
    "    columns_to_drop = [\"fsym_id\", \"p_split_date\", 'p_split_factor']\n",
    "    adj = adj.drop(*columns_to_drop)\n",
    "    \n",
    "    print(adj.show())\n",
    "    \n",
    "    adj = (adj\n",
    "        .withColumn(\"year\", year(\"p_date\"))\n",
    "        .withColumn(\"week\", weekofyear(\"p_date\"))\n",
    "        .groupBy(\"ticker_region\", \"year\", \"week\")\n",
    "        .agg(max(\"p_date\").alias(\"date\"), last(\"split_adj_price\").alias(\"price\"))\n",
    "        .orderBy(\"ticker_region\", \"date\"))\n",
    "    print(\"Function applied\")\n",
    "\n",
    "def create_imploded_df(ticker_list):\n",
    "    df = pd.read_csv('imploded_tickers_dates_test.csv', index_col=None, usecols=['Ticker','Implosion_Date'])\n",
    "    #df = spark.createDataFrame([], schema)\n",
    "    i = 0\n",
    "    for t in ticker_list:\n",
    "        stock_series = get_stock_price_weekly(t)\n",
    "        if stock_series is not None and max(stock_series) >= 100:\n",
    "            imp_dates = get_implosion_dates(stock_series)\n",
    "            if len(imp_dates)!=0:\n",
    "                for date in imp_dates:\n",
    "                    date_str = pd.to_datetime(date[0]).strftime('%Y-%m-%d')\n",
    "                    new_row = pd.DataFrame({'Ticker': [t], 'Implosion_Date': [date_str]})\n",
    "                    df = pd.concat([df, new_row], ignore_index=True)\n",
    "        if i>0 and i % 100 == 0:\n",
    "            print(i)\n",
    "            #df=df.orderBy('Ticker')\n",
    "            df=df.sort_values(by='Ticker')\n",
    "            df.to_csv('imploded_stocks2.csv', index='False')\n",
    "        i+=1\n",
    "    print(df.head(10)) \n",
    "    df=df.sort_values(by='Ticker')\n",
    "    # df=df.orderBy('Ticker')\n",
    "    df.to_csv('imploded_stocks2.csv', index='False')\n",
    "\n",
    "def get_stock_price_from_df(df, t):\n",
    "    filtered_df = df[df['ticker_region'] == t][['date', 'price']]\n",
    "    selected_series = filtered_df.set_index('date')['price'].sort_index()\n",
    "    return selected_series\n",
    "\n",
    "\n",
    "def create_imploded_df2():\n",
    "    df = pd.read_csv('imploded_tickers_dates_test.csv', index_col=None, usecols=['Ticker','Implosion_Date'])\n",
    "    #df = spark.createDataFrame([], schema)\n",
    "    big_df = pd.read_csv('all_stocks_prices.csv', usecols=['ticker_region', 'date', 'price'])\n",
    "    print(big_df.head())\n",
    "    ticker_list = big_df['ticker_region'].unique().tolist()\n",
    "    print(len(ticker_list))\n",
    "    i = 0\n",
    "    failed = []\n",
    "    for t in ticker_list:\n",
    "        stock_series = get_stock_price_from_df(big_df, t)\n",
    "        if stock_series is not None and max(stock_series) >= 100:\n",
    "            imp_dates = get_implosion_dates(stock_series)\n",
    "            if len(imp_dates)!=0:\n",
    "                for date in imp_dates:\n",
    "                    date_str = pd.to_datetime(date[0]).strftime('%Y-%m-%d')\n",
    "                    new_row = pd.DataFrame({'Ticker': [t], 'Implosion_Date': [date_str]})\n",
    "                    df = pd.concat([df, new_row], ignore_index=True)\n",
    "        elif stock_series is None:\n",
    "            failed.append({'Ticker' : t})\n",
    "        if i>0 and i % 100 == 0:\n",
    "            print(i)\n",
    "            #df=df.orderBy('Ticker')\n",
    "            df=df.sort_values(by='Ticker')\n",
    "            df.to_csv('imploded_stocks2.csv', index='False')\n",
    "        i+=1\n",
    "    print(df.head(10)) \n",
    "    failed_df= pd.DataFrame(failed)\n",
    "    failed_df.to_csv('failed_tickers.csv', index=False)\n",
    "    df=df.sort_values(by='Ticker')\n",
    "    df.to_csv('imploded_stocks2.csv', index='False')\n",
    "    \n",
    "\n",
    "# ticker_list = sorted(get_all_stocks()[:5000])\n",
    "\n",
    "# create_imploded_df(ticker_list)\n",
    "# df = pd.read_csv('imploded_stocks2.csv')\n",
    "# run_imps(df['Ticker'].unique().tolist()[:25])\n",
    "#run_imps(['EMCMF'])\n",
    "#add_labels_to_df('imploded_only.csv')\n",
    "#plot_crashes('SEAC')\n",
    "#APPN,CPS, FOSL, GRPN,PRLB, SEAC\n",
    "#APPN has not imploded\n",
    "#CPS has not imploded\n",
    "#imploded: 377/433, sp500:  russell: 243/1754 imploded\n",
    "\n",
    "# ticker_list = get_all_stocks()\n",
    "# print(ticker_list)\n",
    "# file_name = \"all_stocks.csv\"\n",
    "\n",
    "# with open(file_name, mode='w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(ticker_list)\n",
    "\n",
    "# with open('all_stocks.csv', mode='r') as file:\n",
    "#     reader = csv.reader(file)\n",
    "    \n",
    "#     # Assuming there is only one row in the CSV file\n",
    "#     ticker_list = next(reader, None)\n",
    "\n",
    "# create_imploded_df(ticker_list)\n",
    "# # Display the data list\n",
    "\n",
    "# df = pd.read_csv('imploded_stocks.csv')\n",
    "# print(len(df['Ticker'].unique().tolist()))\n",
    "get_all_stocks_prices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_implosions(df):\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Date'])\n",
    "    df['year'] = df['Implosion_Date'].dt.year\n",
    "\n",
    "    implosions_per_year = df.groupby('year').size()\n",
    "\n",
    "    implosions_per_year.plot(kind='bar', color='skyblue')\n",
    "\n",
    "    plt.title('Number of Implosions per Year')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Implosions')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "df = pd.read_csv('imploded_stocks.csv')\n",
    "visualize_implosions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "def stock_start_analysis():\n",
    "    stocks_df = spark.createDataFrame(pd.read_csv('imploded_stocks.csv'))\n",
    "    stocks_df.createOrReplaceTempView(\"temp_table\")\n",
    "    query = f\"\"\"SELECT t.Ticker, c.p_first_date FROM temp_table t LEFT JOIN fp_sec_coverage c ON c.fsym_id=t.fsym_id\n",
    "                    ORDER BY ticker_region\n",
    "            \"\"\"\n",
    "    start_df = spark.sql(query)\n",
    "    start_df = ps.DataFrame(start_df)\n",
    "    start_df['Year'] = start_df['p_first_date'].dt.year\n",
    "    print(start_df.head())\n",
    "    starts_per_year = start_df.groupby('Year').size()\n",
    "    print(starts_per_year.head())\n",
    "    \n",
    "    \n",
    "    starts_per_year.plot(kind='bar')\n",
    "\n",
    "    plt.title('Earliest dates of stocks')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Stocks')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "#stock_start_analysis()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def industry_analysis_all():\n",
    "    #stock_df = get_all_stocks_df()\n",
    "    stock_df = pd.read_csv('imploded_stocks.csv')\n",
    "    stock_df = spark.createDataFrame(stock_df)\n",
    "    stock_df.createOrReplaceTempView(\"temp_table\")\n",
    "    q = \"\"\"SELECT t.ticker_region, e.factset_industry_desc FROM temp_table t\n",
    "    LEFT JOIN sym_coverage sc ON sc.fsym_id = t.fsym_id\n",
    "    LEFT JOIN ff_sec_entity_hist c on c.fsym_id=sc.fsym_security_id\n",
    "    LEFT JOIN sym_entity_sector d on d.factset_entity_id=c.factset_entity_id\n",
    "    LEFT JOIN factset_industry_map e on e.factset_industry_code=d.industry_code\n",
    "    ORDER BY t.ticker_region\n",
    "    \"\"\"\n",
    "    q2 = \"\"\"SELECT t.Ticker, e.factset_industry_desc FROM temp_table t\n",
    "    LEFT JOIN sym_ticker_region s ON s.ticker_region = CONCAT(t.Ticker, '-US')\n",
    "    LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "    LEFT JOIN ff_sec_entity_hist c on c.fsym_id=sc.fsym_security_id\n",
    "    LEFT JOIN sym_entity_sector d on d.factset_entity_id=c.factset_entity_id\n",
    "    LEFT JOIN factset_industry_map e on e.factset_industry_code=d.industry_code\n",
    "    ORDER BY t.Ticker\n",
    "    \"\"\"\n",
    "    ind_df = spark.sql(q2)\n",
    "    ind_df = ind_df.toPandas()\n",
    "    ind_df_grp = ind_df.groupby('factset_industry_desc').size()\n",
    "    ind_df_grp = ind_df_grp[ind_df_grp >= 10]\n",
    "    \n",
    "    plt.figure(figsize=(10,2))\n",
    "    ind_df_grp.plot(kind='bar')\n",
    "\n",
    "    plt.title('Imploded Stocks')\n",
    "    plt.xlabel('Industry')\n",
    "    plt.ylabel('Number of Stocks')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "industry_analysis_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mkt_vals():\n",
    "    imp_df = pd.read_csv('imploded_stocks.csv')\n",
    "    stock_df = spark.createDataFrame(imp_df)\n",
    "    stock_df.createOrReplaceTempView(\"temp_table\")\n",
    "    q1 = \"\"\"SELECT t.Ticker, t.Implosion_Date, s.ticker_region, f.date, f.ff_mkt_val FROM temp_table t\n",
    "    LEFT JOIN sym_ticker_region s ON s.ticker_region = CONCAT(t.Ticker, '-US') \n",
    "    LEFT JOIN FF_BASIC_DER_QF f ON f.fsym_id = s.fsym_id\n",
    "    ORDER BY t.Ticker, f.date\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.sql(q1).toPandas()\n",
    "    t_list  = imp_df['Ticker'].unique().tolist()[10:20]\n",
    "    num_stocks = len(t_list)\n",
    "    \n",
    "    num_rows = (len(t_list) + 1) // 2\n",
    "    num_cols = 2\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n",
    "    for i,column in enumerate(t_list):\n",
    "        row = i//num_cols\n",
    "        col = i % num_cols \n",
    "        t_df = df[df['Ticker']==column]\n",
    "        axes[row,col].plot(t_df['date'], t_df['ff_mkt_val'])\n",
    "        axes[row, col].axvspan(t_df['Implosion_Date'].iat[0], t_df['Implosion_Date'].iat[0], alpha=0.5, color='blue')\n",
    "        axes[row, col].set_title(f'{column}')\n",
    "        axes[row, col].set_xlabel('Year')\n",
    "        axes[row, col].set_ylabel(f'{column} Mkt Val')\n",
    "        axes[row, col].grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "#plot_mkt_vals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_implosions_from_file(filename):\n",
    "    df = pd.read_csv('imploded_stocks2.csv')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
