{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98d51d0-085d-4172-85cd-dc0f19412ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6802cbd1-16cf-4e4a-8137-9894bdec78e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='2022_10_22', catalog='spark_catalog', description='FactSet data version for the day', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse'),\n",
       " Database(name='2023_04_01', catalog='spark_catalog', description='FactSet data version for the day', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse'),\n",
       " Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"List available databases and tables\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2886cd22-2c06-4882-b434-e5ed726788c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "spark.sql(\"USE 2023_04_01\")\n",
    "    # Assuming that 'ticker' is a valid Python variable\n",
    "\n",
    "# query = f\"\"\"SELECT ticker_region FROM sym_ticker_region WHERE ticker_region LIKE \"%-US\" \"\"\"\n",
    "# df = spark.sql(query)\n",
    "# df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "# ticker_list = df.collect()\n",
    "# print(len(ticker_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca1fc72f-006a-44b5-83fd-21ba1e382ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import when\n",
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "\n",
    "#fund_df = fund_df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "\n",
    "def query(ticker):\n",
    "    query = f\"\"\"SELECT d.ticker_region, a.date\n",
    "                FROM FF_ADVANCED_AF a \n",
    "                LEFT JOIN sym_ticker_region d ON d.fsym_id = a.fsym_id \n",
    "                WHERE d.ticker_region = \"{ticker}-US\"\n",
    "                ORDER BY a.date\n",
    "                \"\"\"\n",
    "\n",
    "    fund_df = spark.sql(query)\n",
    "    fund_df = fund_df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    \n",
    "    return fund_df\n",
    "\n",
    "def get_top_bottom_ten(df):\n",
    "    df = df.sort_values(by='Value')\n",
    "    df=df.dropna()\n",
    "    top10 = df.head(10)\n",
    "    down10 = df.tail(10)\n",
    "    print(top10,down10)\n",
    "    return top10['Metric'].tolist(), down10['Metric'].tolist()\n",
    "\n",
    "def pct_change_df(df, big_string, big_string2):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    query1 = f\"\"\"\n",
    "                SELECT {big_string}\n",
    "                FROM temp_table t  \n",
    "                LEFT JOIN sym_ticker_region s ON t.Ticker = SUBSTRING(s.ticker_region, 1, LENGTH(s.ticker_region)-3) AND SUBSTRING(s.ticker_region, -2, 3) = 'US'\n",
    "                LEFT JOIN FF_ADVANCED_AF a ON s.fsym_id = a.fsym_id AND YEAR(a.date) = YEAR(t.Implosion_Date)\n",
    "                ORDER BY t.Ticker\n",
    "            \"\"\"\n",
    "    query2 = f\"\"\"\n",
    "                SELECT {big_string2}\n",
    "                FROM temp_table t  \n",
    "                LEFT JOIN sym_ticker_region s ON t.Ticker = SUBSTRING(s.ticker_region, 1, LENGTH(s.ticker_region)-3) AND SUBSTRING(s.ticker_region, -2, 3) = 'US'\n",
    "                LEFT JOIN FF_ADVANCED_AF b ON s.fsym_id = b.fsym_id AND YEAR(b.date) = YEAR(t.Implosion_Prev4Years)\n",
    "                ORDER BY t.Ticker\n",
    "            \"\"\"\n",
    "    df1 = spark.sql(query1)\n",
    "    df2 = spark.sql(query2)\n",
    "    \n",
    "    df1 = df1.toPandas()\n",
    "    df2 = df2.toPandas()\n",
    "    \n",
    "    non_string_columns = df1.select_dtypes(exclude=['object']).columns\n",
    "    df1 = df1[non_string_columns]\n",
    "    df2 = df2[non_string_columns]\n",
    "    \n",
    "    null_threshold = 200\n",
    "    columns_to_drop = df1.columns[df1.isnull().sum() > null_threshold]\n",
    "    df1 = df1.drop(columns=columns_to_drop)\n",
    "    df2 = df2.drop(columns=columns_to_drop)\n",
    "    # print(\"NULLS:\")\n",
    "    # print(df1.isnull().sum())\n",
    "    # print(df2.isnull().sum())\n",
    "    \n",
    "    percentage_change_df = ((df1 - df2) / df2) * 100\n",
    "    #print(percentage_change_df)\n",
    "    #print(\"LENGTH: \",len(percentage_change_df))\n",
    "    \n",
    "    \n",
    "    metric_dict = {}\n",
    "    for column in percentage_change_df.columns:\n",
    "        percentage_change_df[column] = percentage_change_df[column].replace([np.inf, -np.inf], np.nan)\n",
    "        new_col = percentage_change_df[column].dropna()\n",
    "        mean_val = new_col.mean()\n",
    "        stddev_val = new_col.std()\n",
    "        z_score_threshold = 3.0\n",
    "        new_col = new_col[(new_col >= mean_val - z_score_threshold * stddev_val) &\n",
    "        (new_col <= mean_val + z_score_threshold * stddev_val)]\n",
    "        #if new_col.std() < 1000:\n",
    "        metric_dict[column] = new_col.mean()\n",
    "    #print(metric_dict)\n",
    "    metric_df = pd.DataFrame(list(metric_dict.items()), columns=['Metric', 'Value'])\n",
    "    #metric_df.to_csv('ChangesBeforeImplosionA4yrs.csv', index=False)\n",
    "    return metric_df\n",
    "    \n",
    "    # df['pct_change'] = (df[metric_curr] - df[metric_prev])/df[metric_prev]\n",
    "    # df['pct_change'] = df['pct_change'].replace([np.inf, -np.inf], np.nan) \n",
    "    # df=df.dropna(axis=0)\n",
    "    # mean_val = df['pct_change'].mean()\n",
    "    # stddev_val = df['pct_change'].std()\n",
    "    # z_score_threshold = 3.0\n",
    "    # df = df[\n",
    "    # (df['pct_change'] >= mean_val - z_score_threshold * stddev_val) &\n",
    "    # (df['pct_change'] <= mean_val + z_score_threshold * stddev_val)]\n",
    "    # new_mean = df['pct_change'].mean()\n",
    "    # return new_mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_metric_changes(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Date']).dt.date\n",
    "    df_metrics = spark.sql(\"SELECT * FROM FF_ADVANCED_AF LIMIT 10\")\n",
    "    df_metrics = df_metrics.columns\n",
    "    result_string = ', '.join('a.' + item for item in df_metrics)\n",
    "    result_string2 = ', '.join('b.' + item for item in df_metrics)\n",
    "    top10s = []\n",
    "    bottom10s = []\n",
    "    for y in range(1,5):\n",
    "        df['Implosion_Prev4Years'] = df['Implosion_Date'] - pd.DateOffset(years=y)\n",
    "    #print(result_string)\n",
    "        new_df = pct_change_df(df, result_string, result_string2)\n",
    "        top10, bottom10 = get_top_bottom_ten(new_df)\n",
    "        top10s.append(top10)\n",
    "    intersection1 = sorted(list(set(top10s[0]).intersection(top10s[1])))\n",
    "    print(intersection1)\n",
    "    intersection1 = sorted(list(set(top10s[0]).intersection(top10s[2])))\n",
    "    print(intersection1)\n",
    "    intersection1 = sorted(list(set(top10s[0]).intersection(top10s[3])))\n",
    "    print(intersection1)\n",
    "    intersection1 = sorted(list(set(top10s[1]).intersection(top10s[2])))\n",
    "    print(intersection1)\n",
    "    intersection1 = sorted(list(set(top10s[1]).intersection(top10s[3])))\n",
    "    print(intersection1)\n",
    "    intersection1 = sorted(list(set(top10s[2]).intersection(top10s[3])))\n",
    "    print(intersection1)\n",
    "\n",
    "#get_metric_changes('imploded_tickers_dates.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88bc63eb-7d98-4de8-b665-808509638bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "EMCMF\n",
      "          date  FF_PRICE_CLOSE_FP\n",
      "0   2018-12-31           0.726803\n",
      "1   2018-12-31           0.891117\n",
      "2   2018-12-31           0.560445\n",
      "3   2018-12-31           0.499095\n",
      "4   2018-12-31           0.490836\n",
      "5   2018-12-31           0.557477\n",
      "6   2018-12-31           0.503525\n",
      "7   2018-12-31           0.499021\n",
      "8   2018-12-31           0.510172\n",
      "9   2018-12-31           0.489476\n",
      "10  2018-12-31           0.514106\n",
      "11  2018-12-31           0.515756\n",
      "12  2018-12-31           0.464353\n",
      "13  2018-12-31           0.468379\n",
      "14  2018-12-31           0.507639\n",
      "15  2018-12-31           0.517097\n",
      "16  2018-12-31           0.145555\n",
      "17  2018-12-31           0.158921\n",
      "18  2019-12-31           0.726803\n",
      "19  2019-12-31           0.891117\n",
      "20  2019-12-31           0.560445\n",
      "21  2019-12-31           0.499095\n",
      "22  2019-12-31           0.490836\n",
      "23  2019-12-31           0.557477\n",
      "24  2019-12-31           0.503525\n",
      "25  2019-12-31           0.499021\n",
      "26  2019-12-31           0.510172\n",
      "27  2019-12-31           0.489476\n",
      "28  2019-12-31           0.514106\n",
      "29  2019-12-31           0.515756\n",
      "30  2019-12-31           0.464353\n",
      "31  2019-12-31           0.468379\n",
      "32  2019-12-31           0.507639\n",
      "33  2019-12-31           0.517097\n",
      "34  2019-12-31           0.145555\n",
      "35  2019-12-31           0.158921\n",
      "36  2020-12-31           0.726803\n",
      "37  2020-12-31           0.891117\n",
      "38  2020-12-31           0.560445\n",
      "39  2020-12-31           0.499095\n",
      "40  2020-12-31           0.490836\n",
      "41  2020-12-31           0.557477\n",
      "42  2020-12-31           0.503525\n",
      "43  2020-12-31           0.499021\n",
      "44  2020-12-31           0.510172\n",
      "45  2020-12-31           0.489476\n",
      "46  2020-12-31           0.514106\n",
      "47  2020-12-31           0.515756\n",
      "48  2020-12-31           0.464353\n",
      "49  2020-12-31           0.468379\n",
      "50  2020-12-31           0.507639\n",
      "51  2020-12-31           0.517097\n",
      "52  2020-12-31           0.145555\n",
      "53  2020-12-31           0.158921\n",
      "54  2021-12-31           0.726803\n",
      "55  2021-12-31           0.891117\n",
      "56  2021-12-31           0.560445\n",
      "57  2021-12-31           0.499095\n",
      "58  2021-12-31           0.490836\n",
      "59  2021-12-31           0.557477\n",
      "60  2021-12-31           0.503525\n",
      "61  2021-12-31           0.499021\n",
      "62  2021-12-31           0.510172\n",
      "63  2021-12-31           0.489476\n",
      "64  2021-12-31           0.514106\n",
      "65  2021-12-31           0.515756\n",
      "66  2021-12-31           0.464353\n",
      "67  2021-12-31           0.468379\n",
      "68  2021-12-31           0.507639\n",
      "69  2021-12-31           0.517097\n",
      "70  2021-12-31           0.145555\n",
      "71  2021-12-31           0.158921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [19], line 60\u001b[0m\n\u001b[1;32m     54\u001b[0m     metric_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mlist\u001b[39m(avg_dict\u001b[38;5;241m.\u001b[39mitems()), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetric\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     55\u001b[0m     metric_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvg_CorrelationsADVANCEDQF.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m \u001b[43mcorr_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimploded_tickers_dates.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [19], line 35\u001b[0m, in \u001b[0;36mcorr_analysis\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28mprint\u001b[39m(t)\n\u001b[1;32m     34\u001b[0m     mat \u001b[38;5;241m=\u001b[39m consistent_changes(t, result_string)\n\u001b[0;32m---> 35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m(\u001b[43mmat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m):\n\u001b[1;32m     36\u001b[0m         corr_matrices\u001b[38;5;241m.\u001b[39mappend(consistent_changes(t, result_string))\n\u001b[1;32m     37\u001b[0m avg_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/pandas/base.py:508\u001b[0m, in \u001b[0;36mIndexOpsMixin.empty\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mempty\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    496\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;124;03m    Returns true if the current object is empty. Otherwise, it returns false.\u001b[39;00m\n\u001b[1;32m    498\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresolved_copy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark_frame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[38;5;241m.\u001b[39misEmpty()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:215\u001b[0m, in \u001b[0;36mDataFrame.rdd\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03m\"\"\"Returns the content as an :class:`pyspark.RDD` of :class:`Row`.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m<class 'pyspark.rdd.RDD'>\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m     jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjavaToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd \u001b[38;5;241m=\u001b[39m RDD(\n\u001b[1;32m    217\u001b[0m         jrdd, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession\u001b[38;5;241m.\u001b[39m_sc, BatchedSerializer(CPickleSerializer())\n\u001b[1;32m    218\u001b[0m     )\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lazy_rdd\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def consistent_changes(ticker, big_string):\n",
    "    start_date = pd.to_datetime(\"2009-01-01\")\n",
    "    query1 = f\"\"\"\n",
    "                SELECT a.date, b.FF_PRICE_CLOSE_FP\n",
    "                FROM FF_ADVANCED_AF a\n",
    "                LEFT JOIN sym_ticker_region d ON d.fsym_id = a.fsym_id \n",
    "                LEFT JOIN FF_BASIC_QF b ON b.fsym_id = a.fsym_id\n",
    "                WHERE d.ticker_region = \"{ticker}-US\" \n",
    "                AND a.date >= \"{start_date}\"\n",
    "                ORDER BY a.date\n",
    "            \"\"\"\n",
    "    q_df = spark.sql(query1)\n",
    "    q_df = ps.DataFrame(q_df)\n",
    "    print(q_df)\n",
    "    non_string_columns = q_df.select_dtypes(exclude=['object']).columns\n",
    "    q_df = q_df[non_string_columns]\n",
    "    # null_threshold = 10\n",
    "    # columns_to_drop = q_df.columns[q_df.isnull().sum() > null_threshold]\n",
    "    # q_df = q_df.drop(columns=columns_to_drop)\n",
    "    correlations = q_df.corr()['FF_PRICE_CLOSE_FP']\n",
    "    return correlations\n",
    "\n",
    "def corr_analysis(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Date']).dt.date\n",
    "    df_metrics = spark.sql(\"SELECT * FROM ff_advanced_qf LIMIT 10\")\n",
    "    df_metrics = df_metrics.columns[:50]\n",
    "    result_string = ', '.join('a.' + item for item in df_metrics)\n",
    "    result_string2 = ', '.join('b.' + item for item in df_metrics)\n",
    "    print(len(df_metrics))\n",
    "    corr_matrices = []\n",
    "    for t in df['Ticker'].unique().tolist():\n",
    "        print(t)\n",
    "        mat = consistent_changes(t, result_string)\n",
    "        if not(mat.empty):\n",
    "            corr_matrices.append(consistent_changes(t, result_string))\n",
    "    avg_dict = {}\n",
    "    df_metrics = corr_matrices[0].index.tolist()\n",
    "    corr_count = 0\n",
    "    for metric in df_metrics:\n",
    "        met_values = [corr_mat[metric] for corr_mat in corr_matrices if not pd.isna(corr_mat[metric])]\n",
    "        met_count = len(met_values)\n",
    "\n",
    "        if met_count != 0:\n",
    "            avg_dict[metric] = np.mean(met_values)\n",
    "            corr_count += 1\n",
    "\n",
    "        if corr_count % 25 == 0:\n",
    "            metric_df = pd.DataFrame(list(avg_dict.items()), columns=['Metric', 'Value'])\n",
    "            metric_df.to_csv(\"Avg_CorrelationsADVANCEDQF.csv\")\n",
    "        \n",
    "        \n",
    "    print(avg_dict)\n",
    "    metric_df = pd.DataFrame(list(avg_dict.items()), columns=['Metric', 'Value'])\n",
    "    metric_df.to_csv(\"Avg_CorrelationsADVANCEDQF.csv\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "corr_analysis('imploded_tickers_dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed286246-6e05-4ed4-8b91-6f2a14abe39b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a700de0e-2f78-48aa-8c16-e12e167d67ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Unnamed: 0                Metric     Value\n",
      "7            7        ff_ptx_xord_cr -0.351692\n",
      "19          19  ff_assets_oth_intang -0.317973\n",
      "3            3       ff_oper_exp_oth -0.304189\n",
      "11          11         ff_eq_aff_inc -0.264472\n",
      "23          23            ff_debt_st -0.248122\n",
      "30          30               ff_debt -0.242477\n",
      "22          22         ff_liabs_curr -0.096941\n",
      "24          24           ff_pay_acct  0.020510\n",
      "16          16      ff_bk_invest_tot  0.046414\n",
      "17          17              ff_inven  0.062547\n",
      "    Unnamed: 0             Metric     Value\n",
      "1            1           ff_sales  0.285308\n",
      "13          13         ff_div_pfd  0.295916\n",
      "28          28         ff_pfd_stk  0.318619\n",
      "2            2       ff_gross_inc  0.327146\n",
      "15          15       ff_cash_only  0.327874\n",
      "29          29          ff_com_eq  0.337377\n",
      "31          31       ff_shldrs_eq  0.337402\n",
      "18          18     ff_assets_curr  0.366452\n",
      "5            5        ff_oper_inc  0.400717\n",
      "0            0  FF_PRICE_CLOSE_FP  1.000000\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "def get_top_bottom_five(df):\n",
    "    df = df.sort_values(by='Value')\n",
    "    df=df.dropna()\n",
    "    top5 = df.head(10)\n",
    "    down5 = df.tail(10)\n",
    "    print(top5)\n",
    "    print(down5)\n",
    "    print(len(df))\n",
    "\n",
    "\n",
    "df = pd.read_csv('Avg_Correlations.csv', index_col=None)\n",
    "get_top_bottom_five(df)\n",
    "#YOU'VE DONE WORST CHANGES NOW FIND OUT WHICH ONES DECREASE CONSISTENTLY\n",
    "#ALSO FIGURE OUT MEANS BEFORE PERIOD AND AFTER PERIOD USING QUARTERLY AND COMPARE DIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007cd3b-3e7f-4dfa-87a8-3c329fa9519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c66b85-a467-4538-91ee-a400c24c56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b54c9-5ddc-4459-afff-5247cc6b7b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
