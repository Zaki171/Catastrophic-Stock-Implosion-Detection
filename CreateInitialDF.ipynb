{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98d51d0-085d-4172-85cd-dc0f19412ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6802cbd1-16cf-4e4a-8137-9894bdec78e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='2022_10_22', catalog='spark_catalog', description='FactSet data version for the day', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse'),\n",
       " Database(name='2023_04_01', catalog='spark_catalog', description='FactSet data version for the day', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse'),\n",
       " Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"List available databases and tables\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2886cd22-2c06-4882-b434-e5ed726788c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "spark.sql(\"USE 2023_04_01\")\n",
    "    # Assuming that 'ticker' is a valid Python variable\n",
    "\n",
    "# query = f\"\"\"SELECT ticker_region FROM sym_ticker_region WHERE ticker_region LIKE \"%-US\" \"\"\"\n",
    "# df = spark.sql(query)\n",
    "# df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "# ticker_list = df.collect()\n",
    "# print(len(ticker_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ca1fc72f-006a-44b5-83fd-21ba1e382ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import when\n",
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#fund_df = fund_df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "\n",
    "def query(ticker,date):\n",
    "    query = f\"\"\"SELECT d.ticker_region, a.date, FF_PRICE_CLOSE_FP\n",
    "                FROM FF_BASIC_AF a \n",
    "                LEFT JOIN sym_ticker_region d ON d.fsym_id = a.fsym_id \n",
    "                WHERE d.ticker_region = \"{ticker}-US\" AND a.date = \"{date}\"\n",
    "                \"\"\"\n",
    "\n",
    "    fund_df = spark.sql(query)\n",
    "    fund_df = fund_df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    \n",
    "    return fund_df\n",
    "\n",
    "def pull_prev_data(df, metric):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    metric_curr = f'{metric}_curr'\n",
    "    metric_prev = f'{metric}_prev'\n",
    "    query = f\"\"\"\n",
    "                SELECT t.Ticker, t.Implosion_Date, t.Implosion_Year, t.Implosion_Prev4Years, s.ticker_region, a.{metric} AS {metric_curr},\n",
    "                       b.{metric} AS {metric_prev}\n",
    "                FROM temp_table t  \n",
    "                LEFT JOIN sym_ticker_region s ON t.Ticker = SUBSTRING(s.ticker_region, 1, LENGTH(s.ticker_region)-3) AND SUBSTRING(s.ticker_region, -2, 3) = 'US'\n",
    "                LEFT JOIN FF_BASIC_AF a ON s.fsym_id = a.fsym_id AND YEAR(a.date) = t.Implosion_Year\n",
    "                LEFT JOIN FF_BASIC_AF b ON s.fsym_id = b.fsym_id AND YEAR(b.date) = t.Implosion_Prev4Years\n",
    "                ORDER BY t.Ticker\n",
    "            \"\"\"\n",
    "    result_df_spark = spark.sql(query)\n",
    "    df = ps.DataFrame(result_df_spark)\n",
    "    \n",
    "    df['pct_change'] = (df[metric_curr] - df[metric_prev])/df[metric_prev]\n",
    "    df['pct_change'] = df['pct_change'].replace([np.inf, -np.inf], np.nan) \n",
    "    df=df.dropna(axis=0)\n",
    "    mean_val = df['pct_change'].mean()\n",
    "    stddev_val = df['pct_change'].std()\n",
    "    z_score_threshold = 3.0\n",
    "    df = df[\n",
    "    (df['pct_change'] >= mean_val - z_score_threshold * stddev_val) &\n",
    "    (df['pct_change'] <= mean_val + z_score_threshold * stddev_val)]\n",
    "    new_mean = df['pct_change'].mean()\n",
    "    return new_mean\n",
    "\n",
    "\n",
    "def create_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Date']).dt.date\n",
    "    df['Implosion_Year'] = df['Implosion_Date']\n",
    "    df['Implosion_Year'] =  pd.to_datetime(df['Implosion_Year']).dt.year\n",
    "    df['Implosion_Prev4Years'] = df['Implosion_Date'] - pd.DateOffset(years=4)\n",
    "    df['Implosion_Prev4Years'] =  pd.to_datetime(df['Implosion_Prev4Years']).dt.year\n",
    "    df = df.sort_values(by='Ticker')\n",
    "    df_metrics = spark.sql(\"SELECT * FROM FF_BASIC_AF LIMIT 10\")\n",
    "    df_metrics = df_metrics.columns\n",
    "    print(len(df_metrics))\n",
    "    df_metrics = df_metrics[6:]\n",
    "    df_metrics.remove('ff_actg_standard')\n",
    "    metric_dict = {}\n",
    "    for m in df_metrics:\n",
    "        print(m)\n",
    "        avg = pull_prev_data(df, m)\n",
    "        metric_dict[m] = avg\n",
    "    # df=pull_prev_data(df, 'FF_PRICE_CLOSE_FP')\n",
    "    # return df\n",
    "    metric_df = pd.DataFrame(list(metric_dict.items()), columns=['Metric', 'Value'])\n",
    "    metric_df.to_csv('ChangesBeforeImplosion.csv', index=False)\n",
    "    return metric_df\n",
    "    \n",
    "\n",
    "#df = create_df('imploded_tickers_dates.csv')\n",
    "# df.show(100)\n",
    "# rdf=query('AAPL','2012-01-01')\n",
    "# rdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc63eb-7d98-4de8-b665-808509638bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a700de0e-2f78-48aa-8c16-e12e167d67ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Metric      Value\n",
      "3     ff_gross_inc -32.281477\n",
      "40       ff_com_eq -28.976753\n",
      "43    ff_shldrs_eq -25.431693\n",
      "13      ff_inc_tax  -6.418365\n",
      "14  ff_min_int_exp  -1.021970\n",
      "            Metric      Value\n",
      "25  ff_assets_curr  41.846761\n",
      "31      ff_debt_st  42.718225\n",
      "41         ff_debt  45.450333\n",
      "18      ff_cash_st  57.866520\n",
      "30   ff_liabs_curr  65.506074\n"
     ]
    }
   ],
   "source": [
    "def get_top_bottom_five(df):\n",
    "    df = df.sort_values(by='Value')\n",
    "    df=df.dropna()\n",
    "    top5 = df.head(5)\n",
    "    down5 = df.tail(5)\n",
    "    print(top5)\n",
    "    print(down5)\n",
    "\n",
    "\n",
    "df = pd.read_csv('ChangesBeforeImplosion.csv')\n",
    "get_top_bottom_five(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1007cd3b-3e7f-4dfa-87a8-3c329fa9519f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'metric_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmetric_df\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metric_df' is not defined"
     ]
    }
   ],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c66b85-a467-4538-91ee-a400c24c56ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
