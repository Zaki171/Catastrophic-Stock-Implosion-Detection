{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='2022_10_22', catalog='spark_catalog', description='FactSet data version for the day', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse'),\n",
       " Database(name='2023_04_01', catalog='spark_catalog', description='FactSet data version for the day', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse'),\n",
       " Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"Identification\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "spark.sql(\"USE 2023_04_01\")\n",
    "    # Assuming that 'ticker' is a valid Python variable\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import norm\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import col, to_date, lit\n",
    "from datetime import timedelta\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DateType\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "\n",
    "\n",
    "start_date = '2000-01-01'\n",
    "end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "def get_stock_price_weekly(ticker):\n",
    "    # Suppress the progress message from yfinance\n",
    "    temp_df = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "    if temp_df.empty:\n",
    "        print(\"No data available for the specified date range.\")\n",
    "        return None\n",
    "    weekly_data = temp_df['Adj Close'].resample('W').last()\n",
    "    return weekly_data\n",
    "\n",
    "def get_stock_prices(ticker):\n",
    "    # Suppress the progress message from yfinance\n",
    "    query = f\"\"\"SELECT s.ticker_region, p.p_date, p.p_price FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                LEFT JOIN fp_basic_prices p ON p.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\n",
    "                ORDER BY p.p_date\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df.show(10)\n",
    "\n",
    "def plot_price(ticker):\n",
    "    # Suppress the progress message from yfinance\n",
    "    temp_df = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "    if temp_df.empty:\n",
    "        #print(\"No data available for the specified date range.\")\n",
    "        return None\n",
    "    weekly_data = temp_df['Adj Close'].resample('W').last()\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(weekly_data.index, weekly_data, label=ticker)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_cum_returns(ticker):\n",
    "    # Suppress the progress message from yfinance\n",
    "    temp_df = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
    "    if temp_df.empty:\n",
    "        #print(\"No data available for the specified date range.\")\n",
    "        return None\n",
    "    weekly_data = temp_df['Adj Close'].resample('W').last().to_frame()\n",
    "    print(weekly_data.head())\n",
    "    weekly_data['returns']  = weekly_data['Adj Close'].pct_change()\n",
    "    weekly_data['cumulative_returns'] = (1 + weekly_data['returns']).cumprod() - 1\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(weekly_data.index, weekly_data['cumulative_returns'], label=ticker)\n",
    "    #plt.plot(weekly_data.index, weekly_data['Adj Close'], color='yellow')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def check_implosion(idx, firm_price, imp_thresh):\n",
    "    i = idx\n",
    "    start_price=firm_price.iloc[idx]\n",
    "    i+=1\n",
    "    period=0\n",
    "    while i < len(firm_price):\n",
    "        current_date = firm_price.index[i]\n",
    "        current_price = firm_price.iloc[i]\n",
    "        if (current_price-start_price)/start_price > 0.2:\n",
    "            return period\n",
    "        period+=1\n",
    "        i+=1\n",
    "    return period\n",
    "\n",
    "def get_crash_dates(firm_price, price_drop_thresh=-0.6, period_thresh=52):\n",
    "    crash_dates = []\n",
    "    imp_dates = []\n",
    "    i = 52\n",
    "    while i < len(firm_price):\n",
    "        current_date = firm_price.index[i]\n",
    "        current_price = firm_price.iloc[i]\n",
    "        prev_year_data = firm_price.iloc[i-52:i]\n",
    "        if len(prev_year_data) != 0:\n",
    "            mean_price = prev_year_data.mean()\n",
    "            if (current_price - mean_price)/mean_price < price_drop_thresh:\n",
    "                imp_dates.append(current_date)\n",
    "        i+=1\n",
    "    return imp_dates\n",
    "\n",
    "def get_implosion_dates(firm_price, price_drop_thresh=-0.6, period_thresh=52):\n",
    "    crash_dates = []\n",
    "    imp_dates = []\n",
    "    i = 52\n",
    "    while i < len(firm_price):\n",
    "        current_date = firm_price.index[i]\n",
    "        current_price = firm_price.iloc[i]\n",
    "        prev_year_data = firm_price.iloc[i-52:i]\n",
    "        if len(prev_year_data) != 0:\n",
    "            mean_price = prev_year_data.mean()\n",
    "            if (current_price - mean_price)/mean_price < price_drop_thresh:\n",
    "                imp_period = check_implosion(i, firm_price,  price_drop_thresh)\n",
    "                st_date = current_date\n",
    "                end_date = firm_price.index[i+imp_period]\n",
    "                if imp_period > period_thresh:\n",
    "                    imp_dates.append((current_date, firm_price.index[i+imp_period]))\n",
    "                i+=imp_period\n",
    "        i+=1\n",
    "    return imp_dates\n",
    "\n",
    "def plot_implosions(stock_series, imp_dates, ticker, ax):\n",
    "    #plt.figure(figsize=(15, 5))\n",
    "    ax.plot(stock_series.index, stock_series, label=ticker)\n",
    "    for i in imp_dates:\n",
    "        ax.axvspan(i[0], i[1], alpha=0.5, color='blue')\n",
    "    ax.legend()\n",
    "    #plt.show()\n",
    "\n",
    "# def run_imps(stocks_list):\n",
    "#     num_imp = 0\n",
    "#     j = 0\n",
    "#     fig, axs = plt.subplots(nrows=len(stocks_list), figsize=(15, 5*len(stocks_list)))\n",
    "#     # for t in stocks_list:\n",
    "#     #     stock_series = get_stock_price_weekly(t)\n",
    "#     #     if stock_series is not None:\n",
    "#     #         imp_dates = get_implosion_dates(stock_series)\n",
    "#     #         if j % 10 == 0:\n",
    "#     #             plot_implosions(stock_series, imp_dates, t, axs)    \n",
    "#     #         j+=1\n",
    "#     #         if len(imp_dates) >= 1:\n",
    "#     #             num_imp+=1\n",
    "#     #         # if len(imp_dates) ==0:\n",
    "#     #         #     plot_implosions(stock_series, imp_dates, t)\n",
    "#     # print(f\"{num_imp} out of {j} imploded\")\n",
    "#     # plt.savefig('all_implosions.png')\n",
    "#     # plt.close()\n",
    "#     # return num_imp\n",
    "#     for t, ax in zip(stocks_list, axs):\n",
    "#         stock_series = get_stock_price_weekly(t)\n",
    "#         if stock_series is not None:\n",
    "#             imp_dates = get_implosion_dates(stock_series)\n",
    "            \n",
    "#             # Plot every 10th graph\n",
    "#             if j % 10 == 0:\n",
    "#                 plot_implosions(stock_series, imp_dates, t, ax=ax)\n",
    "            \n",
    "#             j += 1\n",
    "\n",
    "#             if len(imp_dates) >= 1:\n",
    "#                 num_imp += 1\n",
    "    \n",
    "#     # Save the final figure after the loop\n",
    "#     plt.savefig('all_implosions_subplots.png')\n",
    "    \n",
    "#     # Close the Matplotlib figure to release resources\n",
    "#     plt.close()\n",
    "    \n",
    "#     print(f\"{num_imp} out of {j} imploded\")\n",
    "#     return num_imp\n",
    "def run_imps(stocks_list, columns=3):\n",
    "    num_imp = 0\n",
    "    j = 0\n",
    "    num_rows = math.ceil(len(stocks_list) / columns)\n",
    "    fig, axs = plt.subplots(nrows=num_rows, ncols=columns, figsize=(15, 5*num_rows))\n",
    "\n",
    "    for t, ax in zip(stocks_list, axs.flatten()):\n",
    "        stock_series = get_stock_price_weekly(t)\n",
    "        if stock_series is not None:\n",
    "            imp_dates = get_implosion_dates(stock_series)   \n",
    "            plot_implosions(stock_series, imp_dates, t, ax=ax)       \n",
    "            j += 1\n",
    "            if len(imp_dates) >= 1:\n",
    "                num_imp += 1\n",
    "\n",
    "    for i in range(len(stocks_list), num_rows * columns):\n",
    "        fig.delaxes(axs.flatten()[i])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('all_implosions_subplots.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"{num_imp} out of {j} imploded\")\n",
    "    return num_imp\n",
    "\n",
    "\n",
    "def plot_crashes(ticker):\n",
    "    stock_series = get_stock_price_weekly(ticker)\n",
    "    crash_dates = get_crash_dates(stock_series)\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.plot(stock_series.index, stock_series, label=ticker)\n",
    "    for c in crash_dates:\n",
    "        plt.axvspan(c,c, alpha=0.5, color='blue')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def get_all_stocks():\n",
    "    query = f\"\"\"SELECT s.ticker_region, sc.fref_listing_exchange FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    ticker_list = [row.ticker_region for row in df.collect()]\n",
    "    return ticker_list\n",
    "\n",
    "def get_all_stocks_df():\n",
    "    query = f\"\"\"SELECT s.ticker_region, s.fsym_id FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    return df\n",
    "\n",
    "def get_all_stocks_prices():\n",
    "    df = get_all_stocks_df()\n",
    "    df.show()\n",
    "    df.createOrReplaceTempView(\"stocks\")\n",
    "    query = \"\"\"SELECT * FROM fp_basic_prices\"\"\"\n",
    "    adj = spark.sql(query)\n",
    "    print(\"query done\")\n",
    "    adj.show(100, False)\n",
    "    \n",
    "    adj = adj.withColumn(\"temp_cum_split_factor\", when(adj.p_date==adj.p_split_date, lit(adj.p_split_factor)).otherwise(lit(1.0)))\n",
    "    adj = adj.withColumn(\"div_split_factor\", lit(0.0)) # placeholders\n",
    "    adj = adj.withColumn(\"cum_split_factor\", lit(0.0)) # placeholders\n",
    "    adj = adj.withColumn(\"split_temp_i\", lit(0)) # placeholders - for ordering purposes\n",
    "\n",
    "    # creating udf to calculate cumulative split factor\n",
    "    @pandas_udf(adj.schema, FloatType(), PandasUDFType.GROUPED_MAP)\n",
    "    def calc_product_factor(df1):\n",
    "        \"\"\"\n",
    "        Calculates the cumulative split factor for each company based on unique fsym_id's,\n",
    "        for both the price split and the dividend split.\n",
    "        The data MUST be sorted within the function itself (no orderBy in the function call),\n",
    "        and spin_temp_i must be set to i during each iteration of the loop to guarantee\n",
    "        proper sorting - without these safeguards, the function is applied non-sequentially\n",
    "        to the data.\n",
    "        \"\"\"\n",
    "        df1 = df1.sort_values(by='p_date', ascending=False)\n",
    "        for i in range(0, len(df1)):\n",
    "            df1.loc[i, 'split_temp_i'] = i\n",
    "            if i == 0:\n",
    "                df1.loc[i, 'cum_split_factor'] = 1.0\n",
    "                df1.loc[i, 'div_split_factor'] = 1.0\n",
    "                continue\n",
    "            df1.loc[i-1, 'div_split_factor'] = df1.loc[i-1, 'cum_split_factor'] * df1.loc[i-1, 'temp_cum_split_factor']\n",
    "            df1.loc[i, 'cum_split_factor'] = df1.loc[i-1, 'cum_split_factor'] * df1.loc[i-1, 'temp_cum_split_factor']\n",
    "        return df1\n",
    "\n",
    "    adj = adj.groupBy('ticker_region').apply(calc_product_factor)\n",
    "    adj = adj.withColumn(\"split_adj_price\", (adj.p_price*adj.cum_split_factor))\n",
    "    adj = adj.sort(col('ticker_region').asc(), col('p_date').asc())\n",
    "    columns_to_drop = [\"fsym_id\", \"p_split_date\", 'p_split_factor']\n",
    "    adj = adj.drop(*columns_to_drop)\n",
    "    \n",
    "    print(adj.show())\n",
    "    \n",
    "    adj = (adj\n",
    "        .withColumn(\"year\", year(\"p_date\"))\n",
    "        .withColumn(\"week\", weekofyear(\"p_date\"))\n",
    "        .groupBy(\"ticker_region\", \"year\", \"week\")\n",
    "        .agg(max(\"p_date\").alias(\"date\"), last(\"split_adj_price\").alias(\"price\"))\n",
    "        .orderBy(\"ticker_region\", \"date\"))\n",
    "    print(\"Function applied\")\n",
    "\n",
    "def create_imploded_df(ticker_list):\n",
    "    df = pd.read_csv('imploded_tickers_dates_test.csv', index_col=None, usecols=['Ticker','Implosion_Date'])\n",
    "    #df = spark.createDataFrame([], schema)\n",
    "    i = 0\n",
    "    for t in ticker_list:\n",
    "        stock_series = get_stock_price_weekly(t)\n",
    "        if stock_series is not None and max(stock_series) >= 100:\n",
    "            imp_dates = get_implosion_dates(stock_series)\n",
    "            if len(imp_dates)!=0:\n",
    "                for date in imp_dates:\n",
    "                    date_str = pd.to_datetime(date[0]).strftime('%Y-%m-%d')\n",
    "                    new_row = pd.DataFrame({'Ticker': [t], 'Implosion_Date': [date_str]})\n",
    "                    df = pd.concat([df, new_row], ignore_index=True)\n",
    "        if i>0 and i % 100 == 0:\n",
    "            print(i)\n",
    "            #df=df.orderBy('Ticker')\n",
    "            df=df.sort_values(by='Ticker')\n",
    "            df.to_csv('imploded_stocks2.csv', index='False')\n",
    "        i+=1\n",
    "    print(df.head(10)) \n",
    "    df=df.sort_values(by='Ticker')\n",
    "    # df=df.orderBy('Ticker')\n",
    "    df.to_csv('imploded_stocks2.csv', index='False')\n",
    "\n",
    "def get_stock_price_from_df(df, t):\n",
    "    filtered_df = df[df['ticker_region'] == t][['date', 'price']]\n",
    "    selected_series = filtered_df.set_index('date')['price'].sort_index()\n",
    "    return selected_series\n",
    "\n",
    "\n",
    "def create_imploded_df2():\n",
    "    df = pd.read_csv('imploded_tickers_dates_test.csv', index_col=None, usecols=['Ticker','Implosion_Date'])\n",
    "    #df = spark.createDataFrame([], schema)\n",
    "    big_df = pd.read_csv('all_stocks_prices.csv', usecols=['ticker_region', 'date', 'price'])\n",
    "    print(big_df.head())\n",
    "    ticker_list = big_df['ticker_region'].unique().tolist()\n",
    "    print(len(ticker_list))\n",
    "    i = 0\n",
    "    failed = []\n",
    "    for t in ticker_list:\n",
    "        stock_series = get_stock_price_from_df(big_df, t)\n",
    "        if stock_series is not None and max(stock_series) >= 100:\n",
    "            imp_dates = get_implosion_dates(stock_series)\n",
    "            if len(imp_dates)!=0:\n",
    "                for date in imp_dates:\n",
    "                    date_str = pd.to_datetime(date[0]).strftime('%Y-%m-%d')\n",
    "                    new_row = pd.DataFrame({'Ticker': [t], 'Implosion_Date': [date_str]})\n",
    "                    df = pd.concat([df, new_row], ignore_index=True)\n",
    "        elif stock_series is None:\n",
    "            failed.append({'Ticker' : t})\n",
    "        if i>0 and i % 100 == 0:\n",
    "            print(i)\n",
    "            #df=df.orderBy('Ticker')\n",
    "            df=df.sort_values(by='Ticker')\n",
    "            df.to_csv('imploded_stocks2.csv', index='False')\n",
    "        i+=1\n",
    "    print(df.head(10)) \n",
    "    failed_df= pd.DataFrame(failed)\n",
    "    failed_df.to_csv('failed_tickers.csv', index=False)\n",
    "    df=df.sort_values(by='Ticker')\n",
    "    df.to_csv('imploded_stocks2.csv', index='False')\n",
    "    \n",
    "\n",
    "# ticker_list = sorted(get_all_stocks()[:5000])\n",
    "\n",
    "# create_imploded_df(ticker_list)\n",
    "# df = pd.read_csv('imploded_stocks2.csv')\n",
    "# run_imps(df['Ticker'].unique().tolist()[:25])\n",
    "#run_imps(['EMCMF'])\n",
    "#add_labels_to_df('imploded_only.csv')\n",
    "#plot_crashes('SEAC')\n",
    "#APPN,CPS, FOSL, GRPN,PRLB, SEAC\n",
    "#APPN has not imploded\n",
    "#CPS has not imploded\n",
    "#imploded: 377/433, sp500:  russell: 243/1754 imploded\n",
    "\n",
    "# ticker_list = get_all_stocks()\n",
    "# print(ticker_list)\n",
    "# file_name = \"all_stocks.csv\"\n",
    "\n",
    "# with open(file_name, mode='w', newline='') as file:\n",
    "#     writer = csv.writer(file)\n",
    "#     writer.writerow(ticker_list)\n",
    "\n",
    "# with open('all_stocks.csv', mode='r') as file:\n",
    "#     reader = csv.reader(file)\n",
    "    \n",
    "#     # Assuming there is only one row in the CSV file\n",
    "#     ticker_list = next(reader, None)\n",
    "\n",
    "# create_imploded_df(ticker_list)\n",
    "# # Display the data list\n",
    "\n",
    "# df = pd.read_csv('imploded_stocks.csv')\n",
    "# print(len(df['Ticker'].unique().tolist()))\n",
    "get_all_stocks_prices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_implosions(df):\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Date'])\n",
    "    df['year'] = df['Implosion_Date'].dt.year\n",
    "\n",
    "    implosions_per_year = df.groupby('year').size()\n",
    "\n",
    "    implosions_per_year.plot(kind='bar', color='skyblue')\n",
    "\n",
    "    plt.title('Number of Implosions per Year')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Implosions')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "df = pd.read_csv('imploded_stocks.csv')\n",
    "visualize_implosions(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "\n",
    "def stock_start_analysis():\n",
    "    stocks_df = spark.createDataFrame(pd.read_csv('imploded_stocks.csv'))\n",
    "    stocks_df.createOrReplaceTempView(\"temp_table\")\n",
    "    query = f\"\"\"SELECT t.Ticker, c.p_first_date FROM temp_table t LEFT JOIN fp_sec_coverage c ON c.fsym_id=t.fsym_id\n",
    "                    ORDER BY ticker_region\n",
    "            \"\"\"\n",
    "    start_df = spark.sql(query)\n",
    "    start_df = ps.DataFrame(start_df)\n",
    "    start_df['Year'] = start_df['p_first_date'].dt.year\n",
    "    print(start_df.head())\n",
    "    starts_per_year = start_df.groupby('Year').size()\n",
    "    print(starts_per_year.head())\n",
    "    \n",
    "    \n",
    "    starts_per_year.plot(kind='bar')\n",
    "\n",
    "    plt.title('Earliest dates of stocks')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Stocks')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "#stock_start_analysis()\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def industry_analysis_all():\n",
    "    #stock_df = get_all_stocks_df()\n",
    "    stock_df = pd.read_csv('imploded_stocks.csv')\n",
    "    stock_df = spark.createDataFrame(stock_df)\n",
    "    stock_df.createOrReplaceTempView(\"temp_table\")\n",
    "    q = \"\"\"SELECT t.ticker_region, e.factset_industry_desc FROM temp_table t\n",
    "    LEFT JOIN sym_coverage sc ON sc.fsym_id = t.fsym_id\n",
    "    LEFT JOIN ff_sec_entity_hist c on c.fsym_id=sc.fsym_security_id\n",
    "    LEFT JOIN sym_entity_sector d on d.factset_entity_id=c.factset_entity_id\n",
    "    LEFT JOIN factset_industry_map e on e.factset_industry_code=d.industry_code\n",
    "    ORDER BY t.ticker_region\n",
    "    \"\"\"\n",
    "    q2 = \"\"\"SELECT t.Ticker, e.factset_industry_desc FROM temp_table t\n",
    "    LEFT JOIN sym_ticker_region s ON s.ticker_region = CONCAT(t.Ticker, '-US')\n",
    "    LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "    LEFT JOIN ff_sec_entity_hist c on c.fsym_id=sc.fsym_security_id\n",
    "    LEFT JOIN sym_entity_sector d on d.factset_entity_id=c.factset_entity_id\n",
    "    LEFT JOIN factset_industry_map e on e.factset_industry_code=d.industry_code\n",
    "    ORDER BY t.Ticker\n",
    "    \"\"\"\n",
    "    ind_df = spark.sql(q2)\n",
    "    ind_df = ind_df.toPandas()\n",
    "    ind_df_grp = ind_df.groupby('factset_industry_desc').size()\n",
    "    ind_df_grp = ind_df_grp[ind_df_grp >= 10]\n",
    "    \n",
    "    plt.figure(figsize=(10,2))\n",
    "    ind_df_grp.plot(kind='bar')\n",
    "\n",
    "    plt.title('Imploded Stocks')\n",
    "    plt.xlabel('Industry')\n",
    "    plt.ylabel('Number of Stocks')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "industry_analysis_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mkt_vals():\n",
    "    imp_df = pd.read_csv('imploded_stocks.csv')\n",
    "    stock_df = spark.createDataFrame(imp_df)\n",
    "    stock_df.createOrReplaceTempView(\"temp_table\")\n",
    "    q1 = \"\"\"SELECT t.Ticker, t.Implosion_Date, s.ticker_region, f.date, f.ff_mkt_val FROM temp_table t\n",
    "    LEFT JOIN sym_ticker_region s ON s.ticker_region = CONCAT(t.Ticker, '-US') \n",
    "    LEFT JOIN FF_BASIC_DER_QF f ON f.fsym_id = s.fsym_id\n",
    "    ORDER BY t.Ticker, f.date\n",
    "    \"\"\"\n",
    "    \n",
    "    df = spark.sql(q1).toPandas()\n",
    "    t_list  = imp_df['Ticker'].unique().tolist()[10:20]\n",
    "    num_stocks = len(t_list)\n",
    "    \n",
    "    num_rows = (len(t_list) + 1) // 2\n",
    "    num_cols = 2\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n",
    "    for i,column in enumerate(t_list):\n",
    "        row = i//num_cols\n",
    "        col = i % num_cols \n",
    "        t_df = df[df['Ticker']==column]\n",
    "        axes[row,col].plot(t_df['date'], t_df['ff_mkt_val'])\n",
    "        axes[row, col].axvspan(t_df['Implosion_Date'].iat[0], t_df['Implosion_Date'].iat[0], alpha=0.5, color='blue')\n",
    "        axes[row, col].set_title(f'{column}')\n",
    "        axes[row, col].set_xlabel('Year')\n",
    "        axes[row, col].set_ylabel(f'{column} Mkt Val')\n",
    "        axes[row, col].grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "#plot_mkt_vals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_implosions_from_file(filename):\n",
    "    df = pd.read_csv('imploded_stocks2.csv')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
