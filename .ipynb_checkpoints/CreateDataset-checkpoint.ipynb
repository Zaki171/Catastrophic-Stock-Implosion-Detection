{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b862fd7-75a8-4818-95c2-5375dde4b345",
   "metadata": {},
   "source": [
    "## Building the dataset that will be input into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9cedb4-51ae-42f3-8064-60ce28dd207c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"Building dataset\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "spark.sql(\"USE 2023_04_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63645eaf-dfa5-43d5-ab36-e94c6ced8c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2776387/3553371542.py:222: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  result_df['month_year'] = result_df['date'].dt.to_period(\"M\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ticker       date  ff_accr_exp  ff_assets_com_eq  ff_assets_eq  \\\n",
      "0   A-US 2019-10-31        444.0          1.990733      1.990733   \n",
      "1   A-US 2020-10-31        491.0          1.975580      1.975580   \n",
      "2   A-US 2021-10-31        626.0          1.986454      1.986454   \n",
      "3   A-US 2022-10-31        647.0          1.985297      1.985297   \n",
      "4  AA-US 2019-12-31        413.0          3.558123      3.558123   \n",
      "\n",
      "   ff_assets_gr  ff_assets_oth_tot  ff_assets_per_emp  \\\n",
      "0     10.666198              191.0           0.579877   \n",
      "1      1.851460              209.0           0.587012   \n",
      "2     11.197673              321.0           0.629706   \n",
      "3     -1.616067              276.0           0.581878   \n",
      "4     -8.200527             1210.0           1.060217   \n",
      "\n",
      "   ff_bnfit_loss_rsrv_tcap  ff_bps_gr  ...  ff_compr_inc_pens_liabs_adj  \\\n",
      "0                      NaN   6.966683  ...                        -99.0   \n",
      "1                      NaN   3.595651  ...                        -11.0   \n",
      "2                      NaN  12.049044  ...                        217.0   \n",
      "3                      NaN   0.758128  ...                         68.0   \n",
      "4                      NaN -24.029425  ...                          1.0   \n",
      "\n",
      "   ff_compr_inc_tot  ff_compr_inc_unreal_gl_secs  ff_int_fin_cf  \\\n",
      "0             972.0                        -10.0            NaN   \n",
      "1             711.0                         -7.0            NaN   \n",
      "2            1450.0                         14.0            NaN   \n",
      "3            1189.0                         17.0            NaN   \n",
      "4           -1534.0                       -321.0            NaN   \n",
      "\n",
      "   ff_int_oper_cf  ff_liabs_lease_lt  ff_liabs_lease_st  Implosion_Next_Year  \\\n",
      "0             NaN                NaN                NaN                    0   \n",
      "1             NaN                NaN                NaN                    0   \n",
      "2             NaN                NaN                NaN                    0   \n",
      "3             NaN                NaN                NaN                    0   \n",
      "4             NaN                NaN                NaN                    0   \n",
      "\n",
      "        CPI  Unemployment Rate  \n",
      "0  0.002858                3.6  \n",
      "1  0.000988                6.9  \n",
      "2  0.009101                4.5  \n",
      "3  0.004883                3.7  \n",
      "4  0.003154                3.6  \n",
      "\n",
      "[5 rows x 234 columns]\n",
      "Ticker                 0.000000\n",
      "date                   0.011415\n",
      "ff_accr_exp            0.222364\n",
      "ff_assets_com_eq       0.109824\n",
      "ff_assets_eq           0.018648\n",
      "                         ...   \n",
      "ff_liabs_lease_lt      0.954316\n",
      "ff_liabs_lease_st      0.957656\n",
      "Implosion_Next_Year    0.000000\n",
      "CPI                    0.011415\n",
      "Unemployment Rate      0.011415\n",
      "Length: 234, dtype: float64\n",
      "['ff_accr_exp', 'ff_assets_per_emp', 'ff_bnfit_loss_rsrv_tcap', 'ff_capex_5ygr', 'ff_capex_fix_assets', 'ff_cash_curr_assets', 'ff_cash_div_covg_ratio', 'ff_cash_secs_deps', 'ff_cf_ps_gr', 'ff_claims_net_prem', 'ff_cogs_sales', 'ff_com_eq_deps', 'ff_demand_deps_pct', 'ff_dep_accum_fix_assets', 'ff_deps_assets', 'ff_dps_gr', 'ff_earn_assets_avail_funds', 'ff_earn_assets_eff', 'ff_earn_assets_pct', 'ff_ebit_oper_mgn', 'ff_ebit_oper_roa', 'ff_ebitda_cf', 'ff_ebitda_oper_mgn', 'ff_eff_int_rate', 'ff_emp_gr', 'ff_entrpr_val_ebit_oper', 'ff_entrpr_val_ebitda_oper', 'ff_entrpr_val_sales', 'ff_eps_basic_gr', 'ff_exp_ratio', 'ff_fed_repos_asset', 'ff_ebit_oper_fix_chrg_covg', 'ff_for_asset_turn', 'ff_for_deps_pct', 'ff_for_inc_pct', 'ff_for_net_mgn', 'ff_for_roa', 'ff_ins_rsrv_gr', 'ff_ebit_oper_int_covg', 'ff_int_exp_ib_liabs', 'ff_int_inc_avg_deps', 'ff_int_inc_earn_assets', 'ff_intang_oth', 'ff_inven_curr_assets', 'ff_inven_days', 'ff_inven_turn', 'ff_invest_assets_deps', 'ff_invest_assets_liabs', 'ff_invest_assets_loan_deps', 'ff_invest_assets_pct', 'ff_invest_inc_invest_assets', 'ff_invest_prop', 'ff_invest_tcap', 'ff_invest_yld', 'ff_loan_gr', 'ff_loan_loss_actual_rsrv', 'ff_loan_loss_covg', 'ff_loan_loss_pct', 'ff_loan_loss_prov_pct', 'ff_loan_loss_rsrv_assets', 'ff_loan_loss_rsrv_pct', 'ff_loan_loss_rsrv_tcap', 'ff_loan_tcap', 'ff_loss_exp_ratio', 'ff_loss_ratio', 'ff_mkt_val_gr', 'ff_mkt_val_public', 'ff_net_inc_per_emp', 'ff_net_int_inc_earn_assets', 'ff_net_mgn_gr', 'ff_non_int_inc_rev', 'ff_nonperf_loan_com_eq', 'ff_nonperf_loan_loss_rsrv', 'ff_nonperf_loan_pct', 'ff_oper_cf_fix_chrg', 'ff_oper_inc_prem_earn', 'ff_oper_inc_prem_written', 'ff_pay_acct_sales', 'ff_ppe_net_bldg', 'ff_ppe_net_constr', 'ff_ppe_net_equip', 'ff_ppe_net_land', 'ff_ppe_net_leased_prop', 'ff_ppe_net_leases', 'ff_ppe_net_oth', 'ff_ppe_net_soft_equip', 'ff_ppe_net_trans_equip', 'ff_prem_written_com_eq', 'ff_rd_sales', 'ff_receiv_curr_assets', 'ff_receiv_gross', 'ff_receiv_st_oth', 'ff_receiv_turn', 'ff_receiv_turn_days', 'ff_roea', 'ff_sales_fix_assets', 'ff_sales_inven_turn', 'ff_sales_per_emp', 'ff_sales_wkcap', 'ff_sav_deps_pct', 'ff_secs_re_cap', 'ff_secs_re_invest_assets', 'ff_sga_oth', 'ff_sga_sales', 'ff_shs_float', 'ff_spec_items', 'ff_tax_rate', 'ff_tcap_deps', 'ff_unearn_prem_tcap', 'ff_us_gaap_adj', 'ff_zscore', 'ff_debt_serv', 'ff_eps_dil_gr', 'ff_pbk_tang', 'ff_pe_dil', 'ff_pfcf', 'ff_pfcf_dil', 'ff_psales_dil', 'ff_pay_turn_days', 'ff_int_inc_misc', 'ff_lending_misc', 'ff_re_invest', 'ff_tang_fix_assets_xre', 'ff_fin_invest_tot', 'ff_trade_inc_net', 'ff_bk_oper_exp_oth', 'ff_bk_oper_exp_tot', 'ff_acct_pay_oper', 'ff_loan_gross', 'ff_loan_assets', 'ff_loan_deps', 'ff_assets_disc_oper', 'ff_liabs_disc_oper', 'ff_fscore', 'ff_fcf_yld', 'ff_impair', 'ff_oth_xcept_chrg', 'ff_restruct_debt', 'ff_aud_fees_xnonaud', 'ff_avg_emp_num', 'ff_compr_inc', 'ff_compr_inc_accum', 'ff_compr_inc_for_curn_adj', 'ff_compr_inc_oth', 'ff_compr_inc_pens_liabs_adj', 'ff_compr_inc_tot', 'ff_compr_inc_unreal_gl_secs', 'ff_int_fin_cf', 'ff_int_oper_cf', 'ff_liabs_lease_lt', 'ff_liabs_lease_st']\n",
      "before dropping nulls:  41612\n",
      "after dropping nulls:  9902\n",
      "dataset written\n",
      "   ff_assets_com_eq  ff_assets_eq  ff_assets_gr  ff_assets_oth_tot  ff_bps_gr  \\\n",
      "0          1.990733      1.990733     10.666198              191.0   6.966683   \n",
      "1          1.975580      1.975580      1.851460              209.0   3.595651   \n",
      "2          1.986454      1.986454     11.197673              321.0  12.049044   \n",
      "3          1.985297      1.985297     -1.616067              276.0   0.758128   \n",
      "4          3.558123      3.558123     -8.200527             1210.0 -24.029425   \n",
      "\n",
      "   ff_capex_assets  ff_capex_ps_cf  ff_cash_div_cf  ff_cash_roce  ff_cf_sales  \\\n",
      "0         1.650444        0.490566       17.758621     24.906065    22.467558   \n",
      "1         1.236107        0.381410       17.817014     25.901673    23.337704   \n",
      "2         1.765530        0.615635       14.870825     30.929643    25.114733   \n",
      "3         2.763008        0.970000       13.966480     33.476716    26.139019   \n",
      "4         2.590390        2.048649        0.000000     23.660667    10.709862   \n",
      "\n",
      "   ...  ff_bk_oper_inc_tot  ff_bk_non_oper_inc  ff_commiss_inc_net  \\\n",
      "0  ...                 0.0                 0.0                 0.0   \n",
      "1  ...                 0.0                 0.0                 0.0   \n",
      "2  ...                 0.0                 0.0                 0.0   \n",
      "3  ...                 0.0                 0.0                 0.0   \n",
      "4  ...                 0.0                 0.0                 0.0   \n",
      "\n",
      "   ff_cf_roic  ff_liabs_lease       CPI  Unemployment Rate  Year  Month  \\\n",
      "0   15.823324             5.0  0.002858                3.6  2019     10   \n",
      "1   13.325617           127.0  0.000988                6.9  2020     10   \n",
      "2   19.121813           130.0  0.009101                4.5  2021     10   \n",
      "3   16.012693           101.0  0.004883                3.7  2022     10   \n",
      "4   10.393152           100.0  0.003154                3.6  2019     12   \n",
      "\n",
      "   DayOfWeek  \n",
      "0          3  \n",
      "1          5  \n",
      "2          6  \n",
      "3          0  \n",
      "4          1  \n",
      "\n",
      "[5 rows x 83 columns]\n",
      "Feature Importances for Fold 5:\n",
      "              Feature  Importance\n",
      "47   ff_oper_inc_tcap   -0.010039\n",
      "21        ff_earn_yld   -0.007351\n",
      "78                CPI    0.006103\n",
      "38     ff_net_cf_debt   -0.005833\n",
      "51         ff_roa_ptx   -0.003537\n",
      "..                ...         ...\n",
      "26   ff_gross_cf_debt   -0.000000\n",
      "25      ff_free_ps_cf   -0.000000\n",
      "24   ff_for_sales_pct   -0.000000\n",
      "23  ff_for_assets_pct   -0.000000\n",
      "82          DayOfWeek    0.000000\n",
      "\n",
      "[83 rows x 2 columns]\n",
      "Feature Importances for Fold 5:\n",
      "             Feature  Importance\n",
      "51        ff_roa_ptx   -0.019922\n",
      "78               CPI    0.003386\n",
      "2       ff_assets_gr    0.001722\n",
      "54       ff_sales_gr    0.001467\n",
      "52           ff_roce   -0.000000\n",
      "..               ...         ...\n",
      "27        ff_inc_adj   -0.000000\n",
      "26  ff_gross_cf_debt   -0.000000\n",
      "25     ff_free_ps_cf    0.000000\n",
      "24  ff_for_sales_pct   -0.000000\n",
      "82         DayOfWeek    0.000000\n",
      "\n",
      "[83 rows x 2 columns]\n",
      "Feature Importances for Fold 5:\n",
      "             Feature  Importance\n",
      "51        ff_roa_ptx   -0.016497\n",
      "2       ff_assets_gr    0.006763\n",
      "6     ff_capex_ps_cf    0.003729\n",
      "78               CPI    0.003686\n",
      "76        ff_cf_roic   -0.003572\n",
      "..               ...         ...\n",
      "28       ff_inc_sund   -0.000000\n",
      "27        ff_inc_adj   -0.000000\n",
      "26  ff_gross_cf_debt    0.000000\n",
      "25     ff_free_ps_cf   -0.000000\n",
      "82         DayOfWeek    0.000000\n",
      "\n",
      "[83 rows x 2 columns]\n",
      "Feature Importances for Fold 5:\n",
      "                Feature  Importance\n",
      "51           ff_roa_ptx   -0.013659\n",
      "76           ff_cf_roic   -0.009524\n",
      "2          ff_assets_gr    0.004318\n",
      "78                  CPI    0.001936\n",
      "68  ff_eps_dil_aft_xord   -0.001359\n",
      "..                  ...         ...\n",
      "28          ff_inc_sund   -0.000000\n",
      "27           ff_inc_adj   -0.000000\n",
      "26     ff_gross_cf_debt   -0.000000\n",
      "24     ff_for_sales_pct   -0.000000\n",
      "82            DayOfWeek    0.000000\n",
      "\n",
      "[83 rows x 2 columns]\n",
      "Feature Importances for Fold 5:\n",
      "              Feature  Importance\n",
      "51         ff_roa_ptx   -0.019382\n",
      "2        ff_assets_gr    0.005977\n",
      "78                CPI    0.000939\n",
      "25      ff_free_ps_cf   -0.000464\n",
      "48  ff_oper_ps_net_cf   -0.000157\n",
      "..                ...         ...\n",
      "28        ff_inc_sund   -0.000000\n",
      "27         ff_inc_adj   -0.000000\n",
      "26   ff_gross_cf_debt   -0.000000\n",
      "24   ff_for_sales_pct   -0.000000\n",
      "82          DayOfWeek    0.000000\n",
      "\n",
      "[83 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import lit,col\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "#from boruta import BorutaPy\n",
    "#from fredapi import Fred\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def get_macro_features():\n",
    "    # fred_key = 'bdfdde3b7a21b7d528011d17996b0b8e'\n",
    "    # fred = Fred(api_key=fred_key)\n",
    "    # cpi = fred.get_series(series_id='CPIAUCSL')\n",
    "    # cpi_change = cpi.pct_change()\n",
    "    # unemp = fred.get_series(series_id='UNRATE')\n",
    "    # gdp = fred.get_series(series_id='GDP')\n",
    "    # gdp_change = gdp.pct_change()\n",
    "    # df = pd.DataFrame({'CPI_change': cpi_change,'Unemployment_Rate': unemp,'GDP_change': gdp_change})\n",
    "    # df.to_csv('macro.csv')\n",
    "    df = pd.read_csv('macro.csv')\n",
    "    return df\n",
    "\n",
    "def get_all_stocks():\n",
    "    query = f\"\"\"SELECT s.ticker_region, sc.fref_listing_exchange FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    ticker_list = [row.ticker_region for row in df.collect()]\n",
    "    return ticker_list\n",
    "\n",
    "\n",
    "\n",
    "def get_non_imp_stocks_query():\n",
    "    df2 = spark.createDataFrame(get_implosion_df('imploded_stocks.csv'))\n",
    "    df2.createOrReplaceTempView(\"imp_table\")\n",
    "    query = f\"\"\"SELECT s.ticker_region, s.fsym_id FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\n",
    "                AND NOT EXISTS (\n",
    "                SELECT 1\n",
    "                FROM imp_table\n",
    "                WHERE s.ticker_region = CONCAT(imp_table.Ticker, '-US') )    \n",
    "                \"\"\"\n",
    "    df = spark.sql(query)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_implosion_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Date'])\n",
    "    return df\n",
    "\n",
    "def get_features_for_imploded_stocks(df, big_string, table):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    # query = \"\"\"SELECT t.Ticker, t.Implosion_Date, t.Implosion_Next_Year, a.date, a.ff_gross_inc, b.date, b.ff_gross_inc, c.date, c.ff_gross_inc\n",
    "    #             FROM temp_table t \n",
    "    #             LEFT JOIN sym_ticker_region s ON s.ticker_region = CONCAT(t.Ticker, '-US')\n",
    "    #             LEFT JOIN FF_BASIC_AF a ON s.fsym_id = a.fsym_id AND YEAR(a.date) = YEAR(t.Implosion_Date)-1\n",
    "    #             LEFT JOIN FF_BASIC_AF b ON s.fsym_id = b.fsym_id AND YEAR(b.date) = YEAR(t.Implosion_Date)-2\n",
    "    #             LEFT JOIN FF_BASIC_AF c ON s.fsym_id = c.fsym_id AND YEAR(c.date) = YEAR(t.Implosion_Date)-3\n",
    "    #             ORDER BY t.Ticker, a.date\n",
    "    # \"\"\"\n",
    "    query = f\"\"\"SELECT t.Ticker, a.date, {big_string}, t.Implosion_Next_Year FROM temp_table t\n",
    "                    LEFT JOIN sym_ticker_region s ON s.ticker_region = CONCAT(t.Ticker, '-US')\n",
    "                    LEFT JOIN {table} a ON a.fsym_id = s.fsym_id AND YEAR(a.date) = t.Year\n",
    "                    ORDER BY t.Ticker, a.date\n",
    "    \"\"\"\n",
    "    df2 = spark.sql(query)\n",
    "    return df2\n",
    "    \n",
    "    \n",
    "def get_features_for_non_imploded(metric_string, metric_string2,table):\n",
    "    df = get_non_imp_stocks_query()\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query = f\"\"\"WITH RankedData AS (\n",
    "    SELECT\n",
    "        t.ticker_region,\n",
    "        a.date,\n",
    "        {metric_string},\n",
    "        ROW_NUMBER() OVER (PARTITION BY t.ticker_region ORDER BY a.date DESC) AS row_num\n",
    "    FROM\n",
    "        temp_table t\n",
    "        LEFT JOIN {table} a ON a.fsym_id = t.fsym_id\n",
    "        WHERE YEAR(a.date) < 2023\n",
    ")\n",
    "SELECT\n",
    "    ticker_region AS Ticker,\n",
    "    date,\n",
    "    {metric_string2}\n",
    "FROM\n",
    "    RankedData r\n",
    "WHERE\n",
    "    row_num <= 4\n",
    "ORDER BY\n",
    "    ticker_region,\n",
    "    date\"\"\"\n",
    "    new_df = spark.sql(query)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def create_non_imploded_ds(table):\n",
    "    df_metrics = ps.DataFrame(spark.sql(f\"SELECT * FROM {table} LIMIT 10\")) #get all the metrics\n",
    "    cols = []\n",
    "    for c in df_metrics.columns:\n",
    "        if df_metrics[c].dtype=='float64':#get all the metrics we can calculate correlations with\n",
    "            cols.append(c)\n",
    "    metric_string = ', '.join('a.' + item for item in cols)\n",
    "    metric_string2 = ', '.join('r.' + item for item in cols)\n",
    "    df = get_features_for_non_imploded(metric_string, metric_string2, table)\n",
    "    df = df.withColumn(\"Implosion_Next_Year\", lit(0))\n",
    "    return df\n",
    "\n",
    "def create_imploded_df(table):\n",
    "    df = get_implosion_df('imploded_stocks.csv')\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    df['Implosion_Year'] = df['Implosion_Date'].dt.year-1\n",
    "    df['Implosion_Next_Year'] = 1\n",
    "    additional_rows_1 = df.copy()\n",
    "    additional_rows_1['Implosion_Year'] = df['Implosion_Year'] - 1\n",
    "    additional_rows_1['Implosion_Next_Year'] = 0\n",
    "    additional_rows_2 = df.copy()\n",
    "    additional_rows_2['Implosion_Year'] = df['Implosion_Year'] - 2\n",
    "    additional_rows_2['Implosion_Next_Year'] = 0\n",
    "    additional_rows_3 = df.copy()\n",
    "    additional_rows_3['Implosion_Year'] = df['Implosion_Year'] - 3\n",
    "    additional_rows_3['Implosion_Next_Year'] = 0\n",
    "    df = pd.concat([df, additional_rows_1, additional_rows_2, additional_rows_3])\n",
    "    df = df.sort_values(by=['Ticker', 'Implosion_Year'])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df =df.rename({'Implosion_Year' : 'Year'},axis=1)\n",
    "    \n",
    "    df_metrics = ps.DataFrame(spark.sql(f\"SELECT * FROM {table} LIMIT 10\")) #get all the metrics\n",
    "    cols = []\n",
    "    for c in df_metrics.columns:\n",
    "        if df_metrics[c].dtype=='float64':#get all the metrics we can calculate correlations with\n",
    "            cols.append(c)\n",
    "    metric_string = ', '.join('a.' + item for item in cols)\n",
    "    \n",
    "    df = get_features_for_imploded_stocks(df, metric_string, table)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def boruta_fs():\n",
    "    result_df = pd.read_csv('Advanced_AF_DER_Dataset.csv', index_col=None)\n",
    "    X = result_df.drop(['Ticker', 'date', 'Implosion_Next_Year'], axis=1)\n",
    "    Y = result_df['Implosion_Next_Year']\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X)\n",
    "    X=scaler.transform(X)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = xgb.XGBClassifier()\n",
    "    feat_selector = BorutaPy(model, n_estimators='auto', verbose=2, random_state=1)\n",
    "    feat_selector.fit(X_train, y_train)\n",
    "    \n",
    "    print(feat_selector.support_)\n",
    "    print(feat_selector.ranking_)\n",
    "    \n",
    "    \n",
    "def lasso_fs():\n",
    "    result_df = pd.read_csv('Advanced_AF_DER_Dataset.csv', index_col=None)\n",
    "    result_df['date'] = pd.to_datetime(result_df['date'])\n",
    "    result_df['Year'] = result_df['date'].dt.year\n",
    "    result_df['Month'] = result_df['date'].dt.month\n",
    "    result_df['DayOfWeek'] = result_df['date'].dt.dayofweek\n",
    "    X = result_df.drop(['Ticker', 'date', 'Implosion_Next_Year'], axis=1)\n",
    "    cols = X.columns\n",
    "    print(X.head())\n",
    "    Y = result_df['Implosion_Next_Year']\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "    for train_index, test_index in tscv.split(X):\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(X_train)\n",
    "        X_train_scaled = scaler.transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        alpha = 0.01\n",
    "        lasso_model = Lasso(alpha=alpha)\n",
    "        lasso_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "        feature_importances = lasso_model.coef_\n",
    "\n",
    "        feature_importance_df = pd.DataFrame({'Feature': cols, 'Importance': feature_importances})\n",
    "\n",
    "        feature_importance_df['Absolute Importance'] = feature_importance_df['Importance'].abs()\n",
    "        feature_importance_df = feature_importance_df.sort_values(by='Absolute Importance', ascending=False).drop('Absolute Importance', axis=1)\n",
    "        print(f'Feature Importances for Fold {tscv.get_n_splits()}:')\n",
    "        print(feature_importance_df)\n",
    "    \n",
    "\n",
    "def create_dataset(table):\n",
    "    # df = get_implosion_df('imploded_stocks.csv')\n",
    "    # df = df.drop(df.columns[0], axis=1)\n",
    "    # df['Implosion_Year'] = df['Implosion_Date'].dt.year\n",
    "    # df['Implosion_Next_Year'] = 1\n",
    "    # get_features_for_imploded_stocks(df)\n",
    "    #print(df.head())\n",
    "    #df=spark.createDataFrame(df)\n",
    "    #df.createOrReplaceTempView(\"temp_table\")\n",
    "    cols_to_get = get_not_null_cols()\n",
    "    \n",
    "    imp_df = create_imploded_df(table).toPandas()\n",
    "    non_imp_df =create_non_imploded_ds(table).toPandas()\n",
    "    result_df = pd.concat([non_imp_df,imp_df], ignore_index=True)\n",
    "    #print(result_df.head())\n",
    "    result_df['date'] = pd.to_datetime(result_df['date'], format='%Y-%m-%d')\n",
    "    result_df=result_df.sort_values(by=['Ticker','date'])\n",
    "    macro_df = get_macro_features().reset_index()\n",
    "    macro_df['Date'] = pd.to_datetime(macro_df['Date'], format='%d/%m/%Y')\n",
    "    #print(macro_df.head())\n",
    "    result_df['month_year'] = result_df['date'].dt.to_period(\"M\")\n",
    "    macro_df['Month_year'] = macro_df['Date'].dt.to_period(\"M\")\n",
    "    result_df = pd.merge(result_df, macro_df, left_on='month_year', right_on='Month_year', how='left')\n",
    "    result_df.drop(['Date', 'index', 'month_year','Month_year','GDP'],axis=1,inplace=True)\n",
    "    \n",
    "    null_pcts = result_df.isnull().sum()/len(result_df)\n",
    "    \n",
    "    cols_to_drop = null_pcts[null_pcts > 0.2].index.tolist()\n",
    "    result_df.drop(cols_to_drop,axis=1,inplace=True)\n",
    "    \n",
    "    result_df=pd.DataFrame(result_df)\n",
    "    print(\"before dropping nulls: \",len(result_df))\n",
    "    result_df = result_df.dropna()\n",
    "    print(\"after dropping nulls: \", len(result_df))\n",
    "    result_df.to_csv('Advanced_AF_DER_Dataset.csv', index=False)\n",
    "    print(\"dataset written\")\n",
    "    \n",
    "#     result_df = ps.DataFrame(result_df)\n",
    "#     X = result_df.drop(['Ticker', 'date', 'Implosion_Next_Year'], axis=1)\n",
    "#     y = result_df['Implosion_Next_Year']\n",
    "#     scaler = StandardScaler()\n",
    "#     scaler.fit(X)\n",
    "#     X=scaler.transform(X)\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.2, random_state=42)\n",
    "    \n",
    "#     model = xgb.XGBClassifier()\n",
    "#     feat_selector = BorutaPy(model, n_estimators='auto', verbose=2, random_state=1)\n",
    "#     feat_selector.fit(X_train, y_train)\n",
    "    \n",
    "#     print(feat_selector.support_)\n",
    "#     print(feat_selector.ranking_)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "create_dataset('FF_ADVANCED_DER_AF')\n",
    "#boruta_fs()\n",
    "lasso_fs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7337e-6ca2-4212-b9d0-17953b8998d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
