{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b862fd7-75a8-4818-95c2-5375dde4b345",
   "metadata": {},
   "source": [
    "## Building the dataset that will be input into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9cedb4-51ae-42f3-8064-60ce28dd207c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"Building dataset\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "spark.sql(\"USE 2023_04_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63645eaf-dfa5-43d5-ab36-e94c6ced8c79",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:856\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    855\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_items\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpopleft\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 331\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m#create_dataset('FF_ADVANCED_DER_AF')\u001b[39;00m\n\u001b[1;32m    330\u001b[0m df \u001b[38;5;241m=\u001b[39m get_features_all_stocks()\n\u001b[0;32m--> 331\u001b[0m \u001b[43mtrain_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [9], line 309\u001b[0m, in \u001b[0;36mtrain_test\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    303\u001b[0m crossval \u001b[38;5;241m=\u001b[39m CrossValidator(estimator\u001b[38;5;241m=\u001b[39mpipeline,\n\u001b[1;32m    304\u001b[0m                           estimatorParamMaps\u001b[38;5;241m=\u001b[39mparamGrid,\n\u001b[1;32m    305\u001b[0m                           evaluator\u001b[38;5;241m=\u001b[39mevaluator,\n\u001b[1;32m    306\u001b[0m                           numFolds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)  \u001b[38;5;66;03m# Number of folds for cross-validation\u001b[39;00m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;66;03m# Fit the cross-validator\u001b[39;00m\n\u001b[0;32m--> 309\u001b[0m cvModel \u001b[38;5;241m=\u001b[39m \u001b[43mcrossval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;66;03m# Access average metrics for each fold\u001b[39;00m\n\u001b[1;32m    312\u001b[0m avg_metrics \u001b[38;5;241m=\u001b[39m cvModel\u001b[38;5;241m.\u001b[39mavgMetrics\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[1;32m    210\u001b[0m     )\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/ml/tuning.py:847\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    841\u001b[0m train \u001b[38;5;241m=\u001b[39m datasets[i][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mcache()\n\u001b[1;32m    843\u001b[0m tasks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\n\u001b[1;32m    844\u001b[0m     inheritable_thread_target,\n\u001b[1;32m    845\u001b[0m     _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam),\n\u001b[1;32m    846\u001b[0m )\n\u001b[0;32m--> 847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, metric, subModel \u001b[38;5;129;01min\u001b[39;00m pool\u001b[38;5;241m.\u001b[39mimap_unordered(\u001b[38;5;28;01mlambda\u001b[39;00m f: f(), tasks):\n\u001b[1;32m    848\u001b[0m     metrics_all[i][j] \u001b[38;5;241m=\u001b[39m metric\n\u001b[1;32m    849\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m collectSubModelsParam:\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:861\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m--> 861\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cond\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    863\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mpopleft()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import lit,col\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "#from boruta import BorutaPy\n",
    "#from fredapi import Fred\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import csv\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "\n",
    "def get_macro_features():\n",
    "    # fred_key = 'bdfdde3b7a21b7d528011d17996b0b8e'\n",
    "    # fred = Fred(api_key=fred_key)\n",
    "    # cpi = fred.get_series(series_id='CPIAUCSL')\n",
    "    # cpi_change = cpi.pct_change()\n",
    "    # unemp = fred.get_series(series_id='UNRATE')\n",
    "    # gdp = fred.get_series(series_id='GDP')\n",
    "    # gdp_change = gdp.pct_change()\n",
    "    # df = pd.DataFrame({'CPI_change': cpi_change,'Unemployment_Rate': unemp,'GDP_change': gdp_change})\n",
    "    # df.to_csv('macro.csv')\n",
    "    df = pd.read_csv('macro.csv')\n",
    "    return df\n",
    "\n",
    "def get_all_stocks():\n",
    "    query = f\"\"\"SELECT s.ticker_region, sc.fref_listing_exchange FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    ticker_list = [row.ticker_region for row in df.collect()]\n",
    "    return ticker_list\n",
    "\n",
    "\n",
    "\n",
    "def get_non_imp_stocks_query():\n",
    "    df2 = spark.createDataFrame(get_implosion_df('imploded_stocks.csv'))\n",
    "    df2.createOrReplaceTempView(\"imp_table\")\n",
    "    query = f\"\"\"SELECT s.ticker_region, s.fsym_id FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\n",
    "                AND NOT EXISTS (\n",
    "                SELECT 1\n",
    "                FROM imp_table\n",
    "                WHERE s.ticker_region = CONCAT(imp_table.Ticker, '-US') )    \n",
    "                \"\"\"\n",
    "    df = spark.sql(query)\n",
    "    print(\"got non imploded stocks\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_implosion_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df = df[df['Implosion_Start_Date'].notnull()]\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Start_Date'])\n",
    "    return df\n",
    "\n",
    "def get_non_implosion_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df = df[df['Implosion_Start_Date'].isnull()]\n",
    "    return df\n",
    "\n",
    "def get_features_for_imploded_stocks(df, big_string, table):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    # query = \"\"\"SELECT t.Ticker, t.Implosion_Date, t.Implosion_Next_Year, a.date, a.ff_gross_inc, b.date, b.ff_gross_inc, c.date, c.ff_gross_inc\n",
    "    #             FROM temp_table t \n",
    "    #             LEFT JOIN sym_ticker_region s ON s.ticker_region = CONCAT(t.Ticker, '-US')\n",
    "    #             LEFT JOIN FF_BASIC_AF a ON s.fsym_id = a.fsym_id AND YEAR(a.date) = YEAR(t.Implosion_Date)-1\n",
    "    #             LEFT JOIN FF_BASIC_AF b ON s.fsym_id = b.fsym_id AND YEAR(b.date) = YEAR(t.Implosion_Date)-2\n",
    "    #             LEFT JOIN FF_BASIC_AF c ON s.fsym_id = c.fsym_id AND YEAR(c.date) = YEAR(t.Implosion_Date)-3\n",
    "    #             ORDER BY t.Ticker, a.date\n",
    "    # \"\"\"\n",
    "    query = f\"\"\"SELECT t.ticker_region, a.date, {big_string}, t.Implosion_Next_Year FROM temp_table t\n",
    "                    LEFT JOIN sym_ticker_region s ON s.ticker_region = t.ticker_region\n",
    "                    LEFT JOIN {table} a ON a.fsym_id = s.fsym_id AND t.Year = YEAR(a.date)\n",
    "                    LEFT JOIN FF_BASIC_AF b ON b.fsym_id = s.fsym_id AND YEAR(b.date) = t.Year\n",
    "                    ORDER BY t.ticker_region, a.date\n",
    "    \"\"\"\n",
    "    df2 = spark.sql(query)\n",
    "    print(\"imploded query done\")\n",
    "    return df2\n",
    "    \n",
    "    \n",
    "def get_features_for_non_imploded(metric_string, metric_string2,table):\n",
    "    df = spark.createDataFrame(get_non_implosion_df('imploded_stocks3.csv'))\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query = f\"\"\"WITH RankedData AS (\n",
    "    SELECT\n",
    "        t.ticker_region, s.fsym_id,\n",
    "        a.date,\n",
    "        {metric_string},\n",
    "        ROW_NUMBER() OVER (PARTITION BY t.ticker_region ORDER BY a.date DESC) AS row_num\n",
    "        FROM temp_table t\n",
    "        LEFT JOIN sym_ticker_region s ON s.ticker_region = t.ticker_region\n",
    "        LEFT JOIN {table} a ON a.fsym_id = s.fsym_id \n",
    "        WHERE a.date < (\n",
    "            SELECT MAX(date)\n",
    "            FROM {table} a_sub\n",
    "            WHERE a_sub.fsym_id = s.fsym_id ))\n",
    "    SELECT\n",
    "        r.ticker_region, r.date, {metric_string2}\n",
    "        FROM RankedData r\n",
    "        WHERE row_num <= 4\n",
    "        ORDER BY ticker_region, date\"\"\"\n",
    "    new_df = spark.sql(query)\n",
    "    print(\"non imploded query done\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def create_non_imploded_ds(table):\n",
    "    #df_metrics = ps.DataFrame(spark.sql(f\"SELECT * FROM {table} LIMIT 10\")) #get all the metrics\n",
    "    # cols = []\n",
    "    # for c in df_metrics.columns:\n",
    "    #     if df_metrics[c].dtype=='float64':#get all the metrics we can calculate correlations with\n",
    "    #         cols.append(c)\n",
    "    cols = ['ff_debt_entrpr_val', 'ff_tot_debt_tcap_std', 'ff_fix_assets_com_eq', 'ff_debt_eq', 'ff_inven_curr_assets', 'ff_liabs_lease', 'ff_ltd_tcap', 'ff_sales_wkcap',\n",
    "           'ff_bps_gr', 'ff_oper_inc_tcap', 'ff_assets_gr', 'ff_fcf_yld', 'ff_mkt_val_gr', 'ff_earn_yld', 'ff_pbk_tang', 'ff_zscore', 'ff_entrpr_val_sales', 'ff_psales_dil', 'ff_roea', 'ff_dps_gr',\n",
    "           'ff_loan_loss_pct', 'ff_loan_loss_actual_rsrv'] #advanced_der_qf\n",
    "    \n",
    "    metric_string = ', '.join('a.' + item for item in cols)\n",
    "    metric_string2 = ', '.join('r.' + item for item in cols)\n",
    "    df = get_features_for_non_imploded(metric_string, metric_string2, table)\n",
    "    df = df.withColumn(\"Implosion_Next_Year\", lit(0))\n",
    "    return df\n",
    "\n",
    "def create_imploded_df(table):\n",
    "    df = get_implosion_df('imploded_stocks3.csv')\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    df['Implosion_Year'] = df['Implosion_Date'].dt.year-1\n",
    "    df['Implosion_Next_Year'] = 1\n",
    "    \n",
    "    additional_rows_1 = df.copy()\n",
    "    additional_rows_1['Implosion_Year'] = df['Implosion_Year'] - 1\n",
    "    additional_rows_1['Implosion_Next_Year'] = 0\n",
    "    additional_rows_2 = df.copy()\n",
    "    additional_rows_2['Implosion_Year'] = df['Implosion_Year'] - 2\n",
    "    additional_rows_2['Implosion_Next_Year'] = 0\n",
    "    additional_rows_3 = df.copy()\n",
    "    additional_rows_3['Implosion_Year'] = df['Implosion_Year'] - 3\n",
    "    additional_rows_3['Implosion_Next_Year'] = 0\n",
    "    df = pd.concat([df, additional_rows_1, additional_rows_2, additional_rows_3])\n",
    "    df = df.sort_values(by=['ticker_region', 'Implosion_Year'])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df =df.rename({'Implosion_Year' : 'Year'},axis=1)\n",
    "    \n",
    "    # df_metrics = ps.DataFrame(spark.sql(f\"SELECT * FROM {table} LIMIT 10\")) #get all the metrics\n",
    "    # cols = []\n",
    "    # for c in df_metrics.columns:\n",
    "    #     if df_metrics[c].dtype=='float64':#get all the metrics we can calculate correlations with\n",
    "    #         cols.append(c)\n",
    "    \n",
    "    cols = ['ff_debt_entrpr_val', 'ff_tot_debt_tcap_std', 'ff_fix_assets_com_eq', 'ff_debt_eq', 'ff_inven_curr_assets', 'ff_liabs_lease', 'ff_ltd_tcap', 'ff_sales_wkcap',\n",
    "           'ff_bps_gr', 'ff_oper_inc_tcap', 'ff_assets_gr', 'ff_fcf_yld', 'ff_mkt_val_gr', 'ff_earn_yld', 'ff_pbk_tang', 'ff_zscore', 'ff_entrpr_val_sales', 'ff_psales_dil', 'ff_roea', 'ff_dps_gr',\n",
    "           'ff_loan_loss_pct', 'ff_loan_loss_actual_rsrv']\n",
    "    \n",
    "    metric_string = ', '.join('a.' + item for item in cols)\n",
    "    df = get_features_for_imploded_stocks(df, metric_string, table)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "    \n",
    "def create_dataset(table):\n",
    "    # df = get_implosion_df('imploded_stocks.csv')\n",
    "    # df = df.drop(df.columns[0], axis=1)\n",
    "    # df['Implosion_Year'] = df['Implosion_Date'].dt.year\n",
    "    # df['Implosion_Next_Year'] = 1\n",
    "    # get_features_for_imploded_stocks(df)\n",
    "    #print(df.head())\n",
    "    #df=spark.createDataFrame(df)\n",
    "    #df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    imp_df = create_imploded_df(table).toPandas()\n",
    "    non_imp_df =create_non_imploded_ds(table).toPandas()\n",
    "    result_df = pd.concat([non_imp_df,imp_df], ignore_index=True)\n",
    "    #print(result_df.head())\n",
    "    result_df['date'] = pd.to_datetime(result_df['date'], format='%Y-%m-%d')\n",
    "    result_df=result_df.sort_values(by=['ticker_region','date'])\n",
    "    macro_df = get_macro_features().reset_index()\n",
    "    macro_df['Date'] = pd.to_datetime(macro_df['Date'], format='%d/%m/%Y')\n",
    "    #print(macro_df.head())\n",
    "    result_df['month_year'] = result_df['date'].dt.to_period(\"M\")\n",
    "    macro_df['Month_year'] = macro_df['Date'].dt.to_period(\"M\")\n",
    "    result_df = pd.merge(result_df, macro_df, left_on='month_year', right_on='Month_year', how='left')\n",
    "    result_df.drop(['Date', 'index', 'month_year','Month_year','GDP'],axis=1,inplace=True)\n",
    "    \n",
    "    print(result_df.head())\n",
    "    \n",
    "    null_pcts = result_df.isnull().sum()/len(result_df)\n",
    "    print(null_pcts)\n",
    "    \n",
    "    cols_to_drop = null_pcts[null_pcts > 0.1].index.tolist()\n",
    "    result_df.drop(cols_to_drop,axis=1,inplace=True)\n",
    "    print(\"dropped cols: \", cols_to_drop)\n",
    "    \n",
    "    result_df=pd.DataFrame(result_df)\n",
    "    print(\"before dropping nulls: \",len(result_df))\n",
    "    result_df = result_df.dropna()\n",
    "    print(\"after dropping nulls: \", len(result_df))\n",
    "    print(\"number of implosions: \", len(result_df[result_df['Implosion_Next_Year']==1]))\n",
    "    print(\"number of non-implosions: \", len(result_df[result_df['Implosion_Next_Year']==0]))\n",
    "    result_df.to_csv('Advanced_AF_DER_Dataset.csv', index=False)\n",
    "    print(\"dataset written\")\n",
    "    \n",
    "def get_feature_col_names():\n",
    "    csv_file_path = 'features.csv'\n",
    "    data_list = []\n",
    "    with open(csv_file_path, mode='r') as file:\n",
    "    # Create a CSV reader object\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            data_list.append(row)\n",
    "    col_list = data_list[0]\n",
    "    return col_list\n",
    "    \n",
    "    \n",
    "def get_features_all_stocks():\n",
    "    table = \"FF_ADVANCED_DER_AF\"\n",
    "    df = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    spark_df.createOrReplaceTempView(\"temp_table\")\n",
    "    col_names = get_feature_col_names()\n",
    "    col_string = ', '.join('a.' + item for item in col_names)\n",
    "    q=f\"\"\"SELECT t.fsym_id, a.date, {col_string}\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id\n",
    "                WHERE a.date >= \"2000-01-01\"\n",
    "                ORDER BY t.fsym_id, a.date\"\"\"\n",
    "    features_df = spark.sql(q)\n",
    "    feature_cols = [col for col in features_df.columns if col not in ['fsym_id', 'date']]\n",
    "    sequences = []\n",
    "    \n",
    "\n",
    "    grouped_df = features_df.groupBy(\"fsym_id\").agg(\n",
    "        *[F.collect_list(col).alias(col) for col in feature_cols])\n",
    "\n",
    "    #feature_cols = [col for col in grouped_df.columns if col not in ['fsym_id', 'date']]\n",
    "    grouped_df_padded = grouped_df.select(\n",
    "        \"fsym_id\",\n",
    "        *[F.expr(f\"IF(size({col}) < 23, concat({col}, array_repeat(0, 23 - size({col}))), {col})\").alias(col) for col in feature_cols])\n",
    "\n",
    "    \n",
    "    spark_df = spark_df.withColumn('label', F.when(F.isnan('Implosion_Start_Date'), 0).otherwise(1))\n",
    "    joined_df = grouped_df_padded.join(spark_df.select(\"fsym_id\", \"label\"), \"fsym_id\", \"inner\")\n",
    "    joined_df=joined_df.orderBy('fsym_id')\n",
    "\n",
    "    \n",
    "    return joined_df\n",
    "\n",
    "def train_test(df):\n",
    "    df = df.filter(\n",
    "        reduce(lambda acc, column: acc & (F.size(col(column)) == 23), df.columns[1:-1], lit(True))\n",
    "        )\n",
    "    # print(filtered_df.count())\n",
    "\n",
    "    # average_lengths = df.agg(*[(F.avg(F.size(col(column))).alias(f'avg_length_{column}')) for column in df.columns[1:-1]])\n",
    "    \n",
    "    # test = padded_df.select('ff_non_oper_exp').filter(col('fsym_id')=='RTTY5P-R').collect()[0]\n",
    "    # print(test['ff_non_oper_exp'], len(test['ff_non_oper_exp']))\n",
    "\n",
    "    #need to decide whether to only include stocks that started from 2000, or include just from e.g. 2019\n",
    "    #temporary measure - replace with 0\n",
    "    #look into masking\n",
    "    features = df.columns[1:-1]\n",
    "    list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "    for f in features:\n",
    "        df = df.withColumn(f, list_to_vector_udf(f))\n",
    "    #df.show()\n",
    "    vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features_vector\")\n",
    "\n",
    "    lr = LogisticRegression(featuresCol=\"features_vector\", labelCol=\"label\")\n",
    "\n",
    "    # Pipeline\n",
    "    pipeline = Pipeline(stages=[vector_assembler, lr])\n",
    "\n",
    "    # Parameter grid for cross-validation\n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "        .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "        .addGrid(lr.maxIter, [10, 20, 30]) \\\n",
    "        .build()\n",
    "\n",
    "    # Evaluator\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "    # Cross-validator\n",
    "    crossval = CrossValidator(estimator=pipeline,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=evaluator,\n",
    "                              numFolds=5)  # Number of folds for cross-validation\n",
    "\n",
    "    # Fit the cross-validator\n",
    "    cvModel = crossval.fit(df)\n",
    "\n",
    "    # Access average metrics for each fold\n",
    "    avg_metrics = cvModel.avgMetrics\n",
    "\n",
    "    # Print validation accuracy for each fold\n",
    "    for i, acc in enumerate(avg_metrics):\n",
    "        print(f\"Fold {i + 1} - Validation Accuracy: {acc}\")\n",
    "\n",
    "    # Get the best model from cross-validation\n",
    "    best_model = cvModel.bestModel\n",
    "\n",
    "    # Make predictions with the best model\n",
    "    predictions = best_model.transform(df_transformed)\n",
    "\n",
    "    # Filter and display predictions for label 1\n",
    "    predictions.filter(predictions['label'] == 1).select(\"fsym_id\", \"label\", \"prediction\").show()\n",
    "    \n",
    "    \n",
    "    \n",
    "#create_dataset('FF_ADVANCED_DER_AF')\n",
    "df = get_features_all_stocks()\n",
    "train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096020ae-d29d-4286-98b2-dc7ec86009be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=32, input_shape=(23, input_features)))\n",
    "model.add(Dense(units=output_features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7337e-6ca2-4212-b9d0-17953b8998d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT MIN(a.p_date) FROM fp_basic_prices a LEFT JOIN sym_ticker_region s ON s.fsym_id = a.fsym_id WHERE s.ticker_region = 'AACQU-US' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f034c3-4ed9-400e-a07b-6d2c6be018d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
