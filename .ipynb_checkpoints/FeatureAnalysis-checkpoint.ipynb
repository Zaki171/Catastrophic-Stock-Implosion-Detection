{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe610780-dc3d-4ac8-9dd1-914ed92696c0",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6802cbd1-16cf-4e4a-8137-9894bdec78e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop-3.2.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.7-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "2024-01-16 23:46:50,604 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-01-16 23:46:51,816 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2024-01-16 23:46:53,849 WARN spark.ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Database(name='2023_11_01', description='FactSet data snapshot for 2023_11_01', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_01'),\n",
       " Database(name='2023_11_02', description='FactSet data snapshot for 2023_11_02', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_02'),\n",
       " Database(name='2023_11_03', description='FactSet data snapshot for 2023_11_03', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_03'),\n",
       " Database(name='2023_11_14', description='FactSet data snapshot for 2023_11_14', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_14'),\n",
       " Database(name='2023_11_19', description='FactSet data snapshot for 2023_11_19', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_19'),\n",
       " Database(name='2023_11_22', description='FactSet data snapshot for 2023_11_22', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse/2023_11_22'),\n",
       " Database(name='default', description='Default Hive database', locationUri='hdfs://amok.cs.ucl.ac.uk:8020/user/hive/warehouse')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"List available databases and tables\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2886cd22-2c06-4882-b434-e5ed726788c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark.sql(\"USE 2023_11_02\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd18af3-d36d-4088-aa8c-6c410c3322ae",
   "metadata": {},
   "source": [
    "#### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "694d984f-e6b5-42b9-afb9-1c9d98a7fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_all_stocks_df():\n",
    "    query = f\"\"\"SELECT s.ticker_region, s.fsym_id FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_not_null_cols(df, table='FF_ADVANCED_DER_AF'):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query1 = f\"\"\"SELECT t.fsym_id AS fsym_id2, a.*\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id\n",
    "                WHERE a.date > '2000-01-01'\n",
    "                ORDER BY t.fsym_id, a.date\n",
    "            \"\"\"\n",
    "    #we get all the available dates per stock, so these null values are only within the timeframe available\n",
    "    q_df = spark.sql(query1)\n",
    "    q_df = q_df.drop('date', 'adjdate', 'fsym_id2', 'fsym_id')\n",
    "    num_rows = q_df.count()\n",
    "    column_types = q_df.dtypes\n",
    "    good_cols = []\n",
    "    selected_columns = [F.col(c) for c, c_type in zip(q_df.columns, column_types) if c_type[1] == 'double']\n",
    "    q_df = q_df.select(selected_columns)\n",
    "    count_df = q_df.select( [(F.count(F.when(F.isnan(c) | F.col(c).isNull(), c))/num_rows).alias(c) for c in q_df.columns])\n",
    "    count_dict = count_df.first().asDict()\n",
    "    filtered_keys = [key for key, value in count_dict.items() if value <= 0.25]\n",
    "    return filtered_keys\n",
    "#     for c, c_type in zip(q_df.columns, column_types):\n",
    "#         if c_type[1] == 'double':\n",
    "#             null_count = F.sum(F.when(F.isnan(F.col(c)) | F.col(c).isNull(), 1).otherwise(0))\n",
    "#             null_pct = (null_count / num_rows).alias(f\"{c}_null_pct\")\n",
    "#             q_df_agg = q_df.agg(null_pct)\n",
    "#             actual_pct = q_df_agg.collect()[0][0]\n",
    "#             if actual_pct < 0.25:\n",
    "#                 good_cols.append(c)\n",
    "            \n",
    "#     return good_cols\n",
    "\n",
    "\n",
    "def write_features_file(data_list, csv_file_path='features.csv'):\n",
    "    data_list = [data_list]\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row in data_list:\n",
    "            writer.writerow(row)\n",
    "    print(\"Features written: \", data_list[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f66c89-d4c3-485b-93f5-c9fa821c0a82",
   "metadata": {},
   "source": [
    "### Investigating metrics that changed the most before and after implosions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca1fc72f-006a-44b5-83fd-21ba1e382ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns kept:  0.6213592233009708\n",
      "Largest averages of differences between previous year and implosion year:  ff_com_eq_gr          1.659196\n",
      "ff_debt_entrpr_val    2.440135\n",
      "ff_non_oper_exp       2.928936\n",
      "ff_fcf_yld            5.700369\n",
      "ff_earn_yld           6.023490\n",
      "dtype: float64\n",
      "Features with greatest percentage change with year before implosion:  ['ff_com_eq_gr', 'ff_debt_entrpr_val', 'ff_non_oper_exp', 'ff_fcf_yld', 'ff_earn_yld']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns kept:  0.4368932038834951\n",
      "Largest averages of differences in average before and after implosion date:  ff_inc_sund               1.907611\n",
      "ff_ut_non_oper_inc_oth    2.016712\n",
      "ff_non_oper_exp           5.707181\n",
      "ff_fcf_yld                5.911838\n",
      "ff_earn_yld               8.217640\n",
      "dtype: float64\n",
      "Features with greatest percentage change in mean before and after implosion ['ff_inc_sund', 'ff_ut_non_oper_inc_oth', 'ff_non_oper_exp', 'ff_fcf_yld', 'ff_earn_yld']\n",
      "Features written:  ['ff_ut_non_oper_inc_oth', 'ff_non_oper_exp', 'ff_earn_yld', 'ff_debt_entrpr_val', 'ff_inc_sund', 'ff_com_eq_gr', 'ff_fcf_yld']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import when, lit, col\n",
    "# import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "\n",
    "\n",
    "def pct_change_df(df, big_string, table):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, t.Implosion_Start_Date, b.date, {big_string}\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN sym_ticker_region s ON s.fsym_id = t.fsym_id\n",
    "                LEFT JOIN {table} a ON s.fsym_id = a.fsym_id AND  YEAR(a.date) = YEAR(t.Implosion_Start_Date)\n",
    "                LEFT JOIN {table} b ON s.fsym_id = b.fsym_id AND  YEAR(b.date) = YEAR(t.Implosion_Start_Date)-1\n",
    "                ORDER BY t.fsym_id\n",
    "            \"\"\"\n",
    "    df1 = spark.sql(query1)\n",
    "    #print(df1.show())\n",
    "    df1 = df1.toPandas()\n",
    "    df1 = df1.drop(['fsym_id','Implosion_Start_Date','date'], axis=1)\n",
    "    \n",
    "    def remove_outliers(column):\n",
    "        Q1 = column.quantile(0.25)\n",
    "        Q3 = column.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return column[(column >= lower_bound) & (column <= upper_bound)]\n",
    "\n",
    "\n",
    "\n",
    "    df1 = df1.abs()\n",
    "    null_percentage = df1.isnull().sum() / len(df1)\n",
    "    columns_to_keep = null_percentage[null_percentage <= 0.3].index\n",
    "    df_nulls_removed = df1[columns_to_keep]\n",
    "    print(\"Columns kept: \", len(columns_to_keep)/len(df1.columns))\n",
    "    \n",
    "    df_no_outliers = df_nulls_removed.apply(remove_outliers)\n",
    "\n",
    "    \n",
    "    column_means_no_outliers = df_no_outliers.mean()\n",
    "    #column_means_no_outliers = column_means_no_outliers.dropna()\n",
    "    column_means_no_outliers = column_means_no_outliers.sort_values()\n",
    "    feats = column_means_no_outliers.tail(5)\n",
    "\n",
    "    print(\"Largest averages of differences between previous year and implosion year: \",feats)\n",
    "    return feats.index.tolist()\n",
    "    \n",
    "def avg_change_df(df, big_string, table):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, {big_string}\n",
    "                FROM temp_table t  \n",
    "                LEFT JOIN sym_ticker_region s ON s.fsym_id = t.fsym_id\n",
    "                LEFT JOIN {table} a ON s.fsym_id = a.fsym_id AND  YEAR(a.date) > YEAR(t.Implosion_Start_Date)\n",
    "                LEFT JOIN {table} b ON s.fsym_id = b.fsym_id AND  YEAR(b.date) < YEAR(t.Implosion_Start_Date)\n",
    "                GROUP BY t.fsym_id\n",
    "                ORDER BY t.fsym_id\n",
    "            \"\"\"\n",
    "    df1 = spark.sql(query1)\n",
    "    df1 = df1.toPandas()\n",
    "    df1 = df1.drop(['fsym_id'], axis=1)\n",
    "    \n",
    "    def remove_outliers(column):\n",
    "        Q1 = column.quantile(0.25)\n",
    "        Q3 = column.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return column[(column >= lower_bound) & (column <= upper_bound)]\n",
    "\n",
    "\n",
    "    df1 = df1.abs()\n",
    "    null_percentage = df1.isnull().sum() / len(df1)\n",
    "    columns_to_keep = null_percentage[null_percentage <= 0.3].index\n",
    "    df_nulls_removed = df1[columns_to_keep]\n",
    "    print(\"Columns kept: \", len(columns_to_keep)/len(df1.columns))\n",
    "    \n",
    "    df_no_outliers = df_nulls_removed.apply(remove_outliers)\n",
    "    \n",
    "    column_means_no_outliers = df_no_outliers.mean()\n",
    "    #column_means_no_outliers = column_means_no_outliers.dropna()\n",
    "    column_means_no_outliers = column_means_no_outliers.sort_values()\n",
    "    feats = column_means_no_outliers.tail(5)\n",
    "    print(\"Largest averages of differences in average before and after implosion date: \", feats)\n",
    "#     for feature in feats.index:\n",
    "#         before_implosion = df_no_outliers[feature][df_no_outliers.index.isin(df1[df1[feature].notnull() & (df1['date'] < df1['Implosion_Start_Date'])].index)]\n",
    "#         after_implosion = df_no_outliers[feature][df_no_outliers.index.isin(df1[df1[feature].notnull() & (df1['date'] > df1['Implosion_Start_Date'])].index)]\n",
    "        \n",
    "#         _, p_value = ttest_ind(before_implosion, after_implosion)\n",
    "        \n",
    "#         print(f\"T-test p-value for {feature}: {p_value}\")\n",
    "    return feats.index.tolist()\n",
    "\n",
    "def t_test():\n",
    "    pass\n",
    "\n",
    "\n",
    "def get_metric_changes(filename, table):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df = df[df['Implosion_Start_Date'].notnull()]\n",
    "    df['Implosion_Start_Date'] = pd.to_datetime(df['Implosion_Start_Date']).dt.date\n",
    "    df['Implosion_End_Date'] = pd.to_datetime(df['Implosion_End_Date']).dt.date\n",
    "    cols = get_not_null_cols(df, table)\n",
    "    result_string = ', '.join('(a.' + item + '-b.' + item +')/b.'+item + ' AS ' + item for item in cols)\n",
    "    feats1 = pct_change_df(df, result_string, table) #change 1 year before\n",
    "    print(\"Features with greatest percentage change with year before implosion: \", feats1)\n",
    "    \n",
    "    result_string2 = ', '.join('(MEAN(a.' + item + ')-MEAN(b.' + item +'))/MEAN(b.'+item + ') AS ' + item for item in cols)\n",
    "    feats2 = avg_change_df(df, result_string2, table)\n",
    "    print(\"Features with greatest percentage change in mean before and after implosion\", feats2)\n",
    "    \n",
    "    write_features_file( list(set(feats1+feats2)) )\n",
    "\n",
    "\n",
    "get_metric_changes('imploded_stocks_price.csv', 'FF_ADVANCED_DER_AF')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a7274d-d2b0-4016-a2b6-4f7633f51fe7",
   "metadata": {},
   "source": [
    "### Correlations with Market Value Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "88bc63eb-7d98-4de8-b665-808509638bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "[Stage 3912:==================================================> (196 + 4) / 200]1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ff_mkt_val_public', 'ff_xord', 'ff_dil_adj', 'ff_xord_disc', 'ff_dfd_tax_assets_lt']\n",
      "Features written:  ['ff_ut_non_oper_inc_oth', 'ff_non_oper_exp', 'ff_mkt_val_public', 'ff_earn_yld', 'ff_debt_entrpr_val', 'ff_inc_sund', 'ff_xord', 'ff_dil_adj', 'ff_dfd_tax_assets_lt', 'ff_com_eq_gr', 'ff_xord_disc', 'ff_fcf_yld']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n",
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:186: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df[column_name] = series\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from CreateDataset import get_feature_col_names, get_fund_data\n",
    "\n",
    "\n",
    "def corr_query(implosion_df, col_string, table): \n",
    "    df = get_fund_data(implosion_df)\n",
    "    df=df.withColumn('year', F.year('date'))\n",
    "    window_spec = Window.partitionBy('fsym_id', 'year').orderBy(col('date').desc())\n",
    "\n",
    "    df = df.withColumn('row_num', F.row_number().over(window_spec))\n",
    "\n",
    "    df = df.filter(col('row_num') == 1).orderBy('date') #should we compare correlations with market val?\n",
    "    #should we do quarterly?\n",
    "    \n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, t.adj_price, t.Market_Value, t.date, {col_string}\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id AND YEAR(t.date)=YEAR(a.date)\n",
    "                ORDER BY t.fsym_id, t.date\n",
    "            \"\"\"\n",
    " \n",
    "    q_df = spark.sql(query1)\n",
    "    #q_df.show()\n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy('date')\n",
    "    \n",
    "    q_df = q_df.withColumn(\"return_market_val\", (F.col('Market_Value') - F.lag('Market_Value').over(window_spec)) / F.lag('Market_Value').over(window_spec))\n",
    "    q_df = q_df.withColumn(\"return\", (F.col('adj_price') - F.lag('adj_price').over(window_spec)) / F.lag('adj_price').over(window_spec))\n",
    "    \n",
    "    return_columns = [c[2:] for c in col_string.split(\", \")]\n",
    "    mean_corrs = []\n",
    "    corr_vals = []\n",
    "    #I THINK U NEED TO GROUP BY DATE AND THEN CALCULATE CORRELATIONS\n",
    "\n",
    "    for column in return_columns:\n",
    "        return_col_name = f\"return_{column}\"\n",
    "        corr_col_name = f\"corr_with_{column}\"\n",
    "        q_df = q_df.withColumn(return_col_name, (F.col(column) - F.lag(column).over(window_spec)) / F.lag(column).over(window_spec))\n",
    "        q_df = q_df.withColumn(column, F.corr(return_col_name, 'return_market_val').over(window_spec)) #calculating correlations with market value return\n",
    "        q_df = q_df.drop(*[return_col_name])\n",
    "    q_df = q_df.drop(*['return_market_val', 'return'])\n",
    "    q_df = q_df.select(q_df.columns[4:])\n",
    "    mean_corrs = q_df.agg(*[F.mean(F.abs(F.col(column))).alias(column) for column in q_df.columns])\n",
    "    # mean_corrs.show()\n",
    "    \n",
    "    return mean_corrs.toPandas()\n",
    "\n",
    "def corr_analysis(table):\n",
    "    imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "    imp_df_price = imp_df_price.loc[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "    cols = get_not_null_cols(imp_df_price, 'FF_ADVANCED_DER_AF')\n",
    "    result_string = ', '.join('a.' + item for item in cols)\n",
    "    mean_corrs_df = corr_query(spark.createDataFrame(imp_df_price), result_string, 'FF_ADVANCED_DER_AF')\n",
    "    mean_corrs = mean_corrs_df.to_dict(orient='records')\n",
    "    sorted_corrs = dict(sorted(mean_corrs[0].items(), key=lambda item: item[1], reverse=True))\n",
    "    top_records = list(sorted_corrs.items())[:5]\n",
    "    top_10 = []\n",
    "    for r in top_records:\n",
    "        top_10.append(r[0])\n",
    "    print(top_10)\n",
    "    current_feature_list = get_feature_col_names()\n",
    "    new_feature_list = list(set(current_feature_list + top_10))\n",
    "    \n",
    "    write_features_file(new_feature_list)\n",
    "    \n",
    "    \n",
    "corr_analysis('FF_Advanced_Der_AF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3722ccc-0c58-4599-a464-6e153f4e1f13",
   "metadata": {},
   "source": [
    "### Adding the Extra Features From Literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8586949c-01ca-4b25-88c1-1488355015e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "[Stage 3919:===================================================>(198 + 2) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features written:  ['ff_ut_non_oper_inc_oth', 'ff_non_oper_exp', 'ff_earn_yld', 'ff_mkt_val_gr', 'ff_debt_entrpr_val', 'ff_inc_sund', 'ff_xord', 'ff_dil_adj', 'ff_gross_cf_debt', 'ff_dfd_tax_assets_lt', 'ff_capex_assets', 'ff_com_eq_gr', 'ff_xord_disc', 'ff_mkt_val_public', 'ff_fcf_yld']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "imp_df_price['Implosion_Start_Date'] = pd.to_datetime(imp_df_price['Implosion_Start_Date'])\n",
    "imp_df_price['Implosion_End_Date'] = pd.to_datetime(imp_df_price['Implosion_End_Date'])\n",
    "available_feats = get_not_null_cols(imp_df_price)\n",
    "extra_feats = ['ff_capex_assets', 'ff_gross_cf_debt', 'ff_mkt_val_gr']\n",
    "\n",
    "current_feats = get_feature_col_names()\n",
    "final_feats = list(set(current_feats + extra_feats))\n",
    "write_features_file(final_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca6a8a0-1c51-42e5-8a36-18044f9e9bc4",
   "metadata": {},
   "source": [
    "### Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203b078-0417-4933-afe8-4442a98809ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(all_feats=False, imploded_only=False):\n",
    "    df = get_tabular_dataset(all_feats=all_feats, imploded_only=imploded_only)\n",
    "    df = forward_fill(df)\n",
    "    print(\"Number of rows: \", df.count())\n",
    "    print(\"Number of positives: \", df.filter(F.col('label')==1).count())\n",
    "    df=df.fillna(0.0)\n",
    "    print(\"Number of rows after dropping nulls: \", df.count())\n",
    "    print(\"Number of positives after dropping nulls: \", df.filter(F.col('label')==1).count())\n",
    "    return df\n",
    "\n",
    "\n",
    "def forward_fill(df):\n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy('date')\n",
    "    feature_cols = df.columns[2:-1]\n",
    "    for c in feature_cols:\n",
    "        df = df.withColumn(\n",
    "            c, F.last(c, ignorenulls=True).over(window_spec)\n",
    "        )\n",
    "    return df.orderBy('fsym_id','date')\n",
    "\n",
    "df = get_df()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0e0a7-413f-4d65-9fec-31092302bb13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Boruta import BorutaPy\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "def boruta_fs(train_df, model_name): #HOW DOES BORUTA ACC WORK?\n",
    "    train_df = train_df.toPandas()\n",
    "    X_train = train_df.drop(['fsym_id', 'date', 'label'], axis=1)\n",
    "    y_train = train_df['label']\n",
    "    \n",
    "    if model_name == 'rf':\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        model = GradientBoostingClassifier\n",
    "    feat_selector = BorutaPy(model, n_estimators='auto', verbose=2, random_state=1)\n",
    "    feat_selector.fit(X_train, y_train)\n",
    "    features = X_train.columns.tolist()\n",
    "    print(\"Number of features: \", len(features) )\n",
    "    feature_ranks = list(zip(features, feat_selector.ranking_, feat_selector.support_))\n",
    "    selected_features = []\n",
    "    for feat in feature_ranks:\n",
    "        print(f\"Feature: {feat[0]}, Rank: {feat[1]}, Keep: {feat[2]}\")\n",
    "        if feat[1] <= 5:\n",
    "            selected_features.append(feat[0])\n",
    "    print(\"Selected features: \", selected_features)\n",
    "    return selected_features\n",
    "\n",
    "rf_feats = boruta_fs(df, 'rf')\n",
    "gbt_feats = boruta_fs(df, 'gbt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f1dd31-27ce-42b8-ac91-41d2ebf9876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_features = get_feature_col_names()\n",
    "# for f in boruta_features:\n",
    "#     if f in current_features:\n",
    "#         print(f)\n",
    "# final_features = list(set(boruta_features + current_features))\n",
    "# write_features_file(final_features) #in the feature selection pipeline, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552c21d5-a37f-4211-a2dc-a32a14a41cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def correlation_matrix(df):\n",
    "    df =df.toPandas()\n",
    "    print(\"Converted to Pandas\")\n",
    "    corr_df = df.drop(['date','fsym_id'], axis=1)\n",
    "    corr_mat = corr_df.corr()\n",
    "    mask = np.triu(np.ones_like(corr_mat))\n",
    "    plt.figure(figsize=(50, 40))\n",
    "    sns.heatmap(corr_mat, annot=True, cmap='coolwarm', fmt=\".2f\", mask=mask)\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('corr_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Variable pairs with absolute correlation above 0.7:\")\n",
    "    for i in range(len(corr_mat.columns)):\n",
    "        for j in range(i+1, len(corr_mat.columns)):\n",
    "            if abs(corr_mat.iloc[i, j]) >= 0.7:\n",
    "                print(f\"{corr_mat.columns[i]} - {corr_mat.columns[j]}: {corr_mat.iloc[i, j]}\")\n",
    "                \n",
    "# correlation_matrix(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4ea6a0-81ff-4b2b-9dd0-f6fff61d7fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('ff_div_yld_secs', 'ff_earn_yld', 'ff_roa_ptx', 'ff_net_inc_basic_aft_xord', 'ff_net_inc_dil', 'ff_oper_inc_aft_unusual', \n",
    "                        'ff_net_inc_dil_aft_xord', 'ff_net_inc_dil_bef_unusual', 'ff_ebit_bef_unusual', 'ff_eps_dil_gr', 'GDP', 'ff_bk_oper_inc_tot')\n",
    "feats = df.columns[2:-1]\n",
    "# write_features_file(feats)\n",
    "feats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbb6e21-44f3-4b26-b63b-a45b4ba8056a",
   "metadata": {},
   "source": [
    "### Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1385f503-98bc-4b81-938c-27e8225f53b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_dates(imp_df_price):\n",
    "    price_data = get_fund_data(spark.createDataFrame(imp_df_price))\n",
    "    #cols = get_not_null_cols(imp_df_price, 'FF_ADVANCED_DER_AF')\n",
    "    #result_string = ', '.join('a.' + item for item in cols)\n",
    "    \n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy(col('p_date'))\n",
    "\n",
    "    price_data = price_data.withColumn('row_num', F.row_number().over(window_spec))\n",
    "    price_data.show()\n",
    "\n",
    "    price_data = price_data.filter(col('row_num') == 1).orderBy(col('p_date').desc())\n",
    "    price_data.show()\n",
    "    \n",
    "    start_dates = price_data.groupBy('year').count().orderBy('year')\n",
    "    years = [row['year'] for row in start_dates.collect()]\n",
    "    counts = [row['count'] for row in start_dates.collect()]\n",
    "    plt.bar(years, counts)\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Start Dates Count per Year')\n",
    "    plt.show()\n",
    "    #start_dates.show(25)\n",
    "    \n",
    "def null_vals(imp_df_price, table):\n",
    "    price_data = get_fund_data(spark.createDataFrame(imp_df_price))\n",
    "    cols = get_not_null_cols(imp_df_price, table)\n",
    "    col_string = ', '.join('a.' + item for item in cols)\n",
    "    price_data.createOrReplaceTempView('temp_table')\n",
    "    null_counts = []\n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.fsym_id, t.split_adj_price, t.Market_Value, t.p_date, {col_string}\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id AND YEAR(t.p_date)=YEAR(a.date)\n",
    "                ORDER BY t.fsym_id, t.p_date\n",
    "            \"\"\"\n",
    "    full_df = spark.sql(query1)\n",
    "    for column in cols:\n",
    "        null_count = full_df.select(column).filter(col(column).isNull()).count()\n",
    "        null_counts.append((column, null_count))\n",
    "    null_counts_df = pd.DataFrame(null_counts, columns=['Column', 'Null Count'])\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(null_counts_df['Column'], null_counts_df['Null Count'])\n",
    "    plt.xlabel('Column')\n",
    "    plt.ylabel('Null Count')\n",
    "    plt.title('Null Counts for Each Column')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # null_counts = price_data.groupBy('year').agg(F.sum(col('p_price').isNull().cast('int')).alias('null_count'))\n",
    "    # null_counts.show()\n",
    "    \n",
    "imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "imp_df_price_imploded = imp_df_price.loc[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "start_dates(imp_df_price)\n",
    "start_dates(imp_df_price_imploded)\n",
    "\n",
    "#null_vals(imp_df_price, 'FF_ADVANCED_DER_AF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345755a7-1bf1-442d-9418-576cb9688733",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "imp_df_test = imp_df_price[imp_df_price['fsym_id']=='H7CTYF-R']\n",
    "df = get_fund_data(spark.createDataFrame(imp_df_test))\n",
    "df.show(1000)\n",
    "imp_df_imp = imp_df_price[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "print(len(imp_df_imp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e730ca87-195a-49a3-87e3-60f22f57b015",
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_df_imp = imp_df_price[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "print(len(imp_df_imp))\n",
    "print(len(imp_df_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19776f2-22bb-4ab6-a748-cb9928010b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cols():\n",
    "    df_metrics = ps.DataFrame(spark.sql(\"SELECT * FROM FF_BASIC_AF LIMIT 10\")) #get all the metrics\n",
    "    cols = []\n",
    "    for c in df_metrics.columns:\n",
    "        if df_metrics[c].dtype=='float64':#get all the metrics we can calculate correlations with\n",
    "            cols.append(c)\n",
    "    return cols\n",
    "\n",
    "#%change average of each feature plotted for pharmacy industry\n",
    "def industry_analysis():\n",
    "    stock_df = get_all_stocks_df()\n",
    "    #stock_df = pd.read_csv('imploded_stocks.csv')\n",
    "    #stock_df = spark.createDataFrame(stock_df)\n",
    "    cols = ['ff_gross_inc', 'ff_sales', 'FF_OPER_EXP_TOT', 'FF_CASH_ST']\n",
    "    col_string = ', '.join('a.' + item for item in cols)\n",
    "    stock_df.createOrReplaceTempView(\"temp_table\")\n",
    "    q = f\"\"\"SELECT e.factset_industry_desc, t.ticker_region, a.date, {col_string} FROM temp_table t\n",
    "    LEFT JOIN FF_BASIC_AF a ON a.fsym_id = t.fsym_id\n",
    "    LEFT JOIN sym_coverage sc ON sc.fsym_id = t.fsym_id\n",
    "    LEFT JOIN ff_sec_entity_hist c on c.fsym_id=sc.fsym_security_id\n",
    "    LEFT JOIN sym_entity_sector d on d.factset_entity_id=c.factset_entity_id\n",
    "    LEFT JOIN factset_industry_map e on e.factset_industry_code=d.industry_code\n",
    "    WHERE a.date >= \"2009-01-01\" AND e.factset_industry_desc=\"Regional Banks\"\n",
    "    ORDER BY t.ticker_region,a.date\"\"\"\n",
    "    ind_df = spark.sql(q)\n",
    "    #print(ind_df.show(10))\n",
    "    ind_df =ind_df.toPandas()\n",
    "    ind_df['date'] = pd.to_datetime(ind_df['date'])\n",
    "    new_cols = []\n",
    "    for column in cols:\n",
    "        ind_df[f'{column}_percentage_change'] = ind_df.groupby('ticker_region')[column].pct_change() * 100\n",
    "        ind_df[f'{column}_percentage_change'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        ind_df.drop(column, axis=1, inplace=True)\n",
    "        new_cols.append(f'{column}_percentage_change')\n",
    "    ind_df['year'] = ind_df['date'].dt.year\n",
    "    avg_pct_change = ind_df.groupby(['year'])[new_cols].mean().reset_index()\n",
    "    print(avg_pct_change.head(20))\n",
    "    num_rows = (len(new_cols) + 1) // 2  # Adjust the number of rows as needed\n",
    "    num_cols = 2\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 5 * num_rows))\n",
    "    for i,column in enumerate(new_cols):\n",
    "        row = i//num_cols\n",
    "        col = i % num_cols \n",
    "        axes[row,col].plot(avg_pct_change['year'], avg_pct_change[column])\n",
    "        axes[row, col].set_title(f'Avg {column} Percentage Change Over Time')\n",
    "        axes[row, col].set_xlabel('Year')\n",
    "        axes[row, col].set_ylabel(f'Avg {column} Percentage Change')\n",
    "        axes[row, col].grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#industry_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700de0e-2f78-48aa-8c16-e12e167d67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#YOU'VE DONE WORST CHANGES NOW FIND OUT WHICH ONES DECREASE CONSISTENTLY\n",
    "#ALSO FIGURE OUT MEANS BEFORE PERIOD AND AFTER PERIOD USING QUARTERLY AND COMPARE DIFF\n",
    "#FINALLY WITH A HUGE LIST USE BORUTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b54c9-5ddc-4459-afff-5247cc6b7b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_not_null_cols(df, table='FF_ADVANCED_DER_AF'):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query1 = f\"\"\"SELECT t.fsym_id, a.*\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id\n",
    "                ORDER BY t.fsym_id, a.date\n",
    "            \"\"\"\n",
    "    #we get all the available dates per stock, so these null values are only within the timeframe available\n",
    "    q_df = spark.sql(query1)\n",
    "    column_types = q_df.dtypes\n",
    "    null_pcts = []\n",
    "    for c, dtype in zip(q_df.columns, column_types):\n",
    "        if dtype[1] == 'double':\n",
    "            null_count = q_df.filter(F.col(c).isNull()).count()\n",
    "            null_pcts.append(null_count/q_df.count())\n",
    "\n",
    "\n",
    "    columns_to_drop = [col_name for col_name, null_pct, dtype in zip(q_df.columns, null_pcts, column_types) if null_pct > 0.2 or dtype[1]!='double']\n",
    "\n",
    "    q_df = q_df.drop(*columns_to_drop)\n",
    "\n",
    "    cols = q_df.columns\n",
    "    print(cols)\n",
    "\n",
    "    return cols\n",
    "    \n",
    "df = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "df = df.loc[df['Implosion_Start_Date'].notnull()]\n",
    "get_not_null_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d7b62c-becd-45ee-bcb2-8c8c59e8e0b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4997ed0d-ad2e-4ec5-9b4b-7818b3a857f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
