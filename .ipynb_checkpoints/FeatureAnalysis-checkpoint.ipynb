{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98d51d0-085d-4172-85cd-dc0f19412ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6802cbd1-16cf-4e4a-8137-9894bdec78e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='2022_10_22', catalog='spark_catalog', description='FactSet data version for the day', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse'),\n",
       " Database(name='2023_04_01', catalog='spark_catalog', description='FactSet data version for the day', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse'),\n",
       " Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"List available databases and tables\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2886cd22-2c06-4882-b434-e5ed726788c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "spark.sql(\"USE 2023_04_01\")\n",
    "    # Assuming that 'ticker' is a valid Python variable\n",
    "\n",
    "# query = f\"\"\"SELECT ticker_region FROM sym_ticker_region WHERE ticker_region LIKE \"%-US\" \"\"\"\n",
    "# df = spark.sql(query)\n",
    "# df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "# ticker_list = df.collect()\n",
    "# print(len(ticker_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca1fc72f-006a-44b5-83fd-21ba1e382ebc",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[MISSING_AGGREGATION] The non-aggregating expression \"fsym_id\" is based on columns which are not participating in the GROUP BY clause.\nAdd the columns or the expression to the GROUP BY, aggregate the expression, or use \"any_value(fsym_id)\" if you do not care which of the values within a group is returned.;\nSort [Ticker#8358 ASC NULLS FIRST], true\n+- Aggregate [Ticker#8358], [Ticker#8358, fsym_id#18, date#19, adjdate#20, currency#21, ff_accr_exp_cf#22]\n   +- Join LeftOuter, ((fsym_id#2017 = fsym_id#18) AND (date#19 < Implosion_Date#8359))\n      :- Join LeftOuter, ((Ticker#8358 = substring(ticker_region#2018, 1, (length(ticker_region#2018) - 3))) AND (substring(ticker_region#2018, -2, 3) = US))\n      :  :- SubqueryAlias t\n      :  :  +- SubqueryAlias temp_table\n      :  :     +- View (`temp_table`, [Unnamed: 0#8357L,Ticker#8358,Implosion_Date#8359])\n      :  :        +- LogicalRDD [Unnamed: 0#8357L, Ticker#8358, Implosion_Date#8359], false\n      :  +- SubqueryAlias s\n      :     +- SubqueryAlias spark_catalog.2023_04_01.sym_ticker_region\n      :        +- Relation spark_catalog.2023_04_01.sym_ticker_region[fsym_id#2017,ticker_region#2018] parquet\n      +- SubqueryAlias a\n         +- SubqueryAlias spark_catalog.2023_04_01.ff_advanced_af\n            +- Relation spark_catalog.2023_04_01.ff_advanced_af[fsym_id#18,date#19,adjdate#20,currency#21,ff_accr_exp_cf#22,ff_actg_chg_ps#23,ff_amort_cf#24,ff_amort_dfd_chrg#25,ff_amort_intang#26,ff_assets_intl#27,ff_assets_oth_tang#28,ff_assets_sep_accts#29,ff_aud_fees#30,ff_bdebt#31,ff_bonds_below_invgr#32,ff_cap_lease#33,ff_com_eq_apic#34,ff_com_eq_for_exch#35,ff_com_eq_gl#36,ff_com_eq_hedg_gl#37,ff_com_eq_oth_compr_adj_oth#38,ff_com_eq_par#39,ff_com_eq_unearn_comp#40,ff_com_shs_auth#41,... 474 more fields] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 197\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m top10s,bottom10s\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# intersection1 = sorted(list(set(top10s[0]).intersection(top10s[1])))\u001b[39;00m\n\u001b[1;32m    185\u001b[0m     \u001b[38;5;66;03m# print(intersection1)\u001b[39;00m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;66;03m# intersection1 = sorted(list(set(top10s[0]).intersection(top10s[2])))\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# intersection1 = sorted(list(set(top10s[2]).intersection(top10s[3])))\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# print(intersection1)\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m tops, bottoms \u001b[38;5;241m=\u001b[39m \u001b[43mget_metric_changes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimploded_tickers_dates.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [17], line 179\u001b[0m, in \u001b[0;36mget_metric_changes\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    175\u001b[0m bottom10s \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m#df['Implosion_Prev4Years'] = df['Implosion_Date'] - pd.DateOffset(years=y)\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m#print(result_string)\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     new_df \u001b[38;5;241m=\u001b[39m \u001b[43mavg_change_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult_string2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg_string\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     top10, bottom10 \u001b[38;5;241m=\u001b[39m get_top_bottom_ten(new_df)\n\u001b[1;32m    181\u001b[0m     top10s\u001b[38;5;241m.\u001b[39mappend(top10)\n",
      "Cell \u001b[0;32mIn [17], line 52\u001b[0m, in \u001b[0;36mavg_change_df\u001b[0;34m(df, big_string, big_string2, avg_string)\u001b[0m\n\u001b[1;32m     36\u001b[0m query1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124m            SELECT t.Ticker, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbig_string\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124m            FROM temp_table t  \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124m            ORDER BY t.Ticker\u001b[39m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     44\u001b[0m query2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124m            SELECT \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_string\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124m            FROM temp_table t  \u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124m            ORDER BY t.Ticker\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124m        \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 52\u001b[0m df1 \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m df2 \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(query2)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(df1\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m))\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/session.py:1440\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1439\u001b[0m     litArgs \u001b[38;5;241m=\u001b[39m {k: _to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m {})\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m-> 1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1442\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [MISSING_AGGREGATION] The non-aggregating expression \"fsym_id\" is based on columns which are not participating in the GROUP BY clause.\nAdd the columns or the expression to the GROUP BY, aggregate the expression, or use \"any_value(fsym_id)\" if you do not care which of the values within a group is returned.;\nSort [Ticker#8358 ASC NULLS FIRST], true\n+- Aggregate [Ticker#8358], [Ticker#8358, fsym_id#18, date#19, adjdate#20, currency#21, ff_accr_exp_cf#22]\n   +- Join LeftOuter, ((fsym_id#2017 = fsym_id#18) AND (date#19 < Implosion_Date#8359))\n      :- Join LeftOuter, ((Ticker#8358 = substring(ticker_region#2018, 1, (length(ticker_region#2018) - 3))) AND (substring(ticker_region#2018, -2, 3) = US))\n      :  :- SubqueryAlias t\n      :  :  +- SubqueryAlias temp_table\n      :  :     +- View (`temp_table`, [Unnamed: 0#8357L,Ticker#8358,Implosion_Date#8359])\n      :  :        +- LogicalRDD [Unnamed: 0#8357L, Ticker#8358, Implosion_Date#8359], false\n      :  +- SubqueryAlias s\n      :     +- SubqueryAlias spark_catalog.2023_04_01.sym_ticker_region\n      :        +- Relation spark_catalog.2023_04_01.sym_ticker_region[fsym_id#2017,ticker_region#2018] parquet\n      +- SubqueryAlias a\n         +- SubqueryAlias spark_catalog.2023_04_01.ff_advanced_af\n            +- Relation spark_catalog.2023_04_01.ff_advanced_af[fsym_id#18,date#19,adjdate#20,currency#21,ff_accr_exp_cf#22,ff_actg_chg_ps#23,ff_amort_cf#24,ff_amort_dfd_chrg#25,ff_amort_intang#26,ff_assets_intl#27,ff_assets_oth_tang#28,ff_assets_sep_accts#29,ff_aud_fees#30,ff_bdebt#31,ff_bonds_below_invgr#32,ff_cap_lease#33,ff_com_eq_apic#34,ff_com_eq_for_exch#35,ff_com_eq_gl#36,ff_com_eq_hedg_gl#37,ff_com_eq_oth_compr_adj_oth#38,ff_com_eq_par#39,ff_com_eq_unearn_comp#40,ff_com_shs_auth#41,... 474 more fields] parquet\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import when\n",
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "\n",
    "\n",
    "#fund_df = fund_df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "\n",
    "def query(ticker):\n",
    "    query = f\"\"\"SELECT d.ticker_region, a.date\n",
    "                FROM FF_ADVANCED_AF a \n",
    "                LEFT JOIN sym_ticker_region d ON d.fsym_id = a.fsym_id \n",
    "                WHERE d.ticker_region = \"{ticker}-US\"\n",
    "                ORDER BY a.date\n",
    "                \"\"\"\n",
    "\n",
    "    fund_df = spark.sql(query)\n",
    "    fund_df = fund_df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    \n",
    "    return fund_df\n",
    "\n",
    "def get_top_bottom_ten(df):\n",
    "    df = df.sort_values(by='Value')\n",
    "    df=df.dropna()\n",
    "    top10 = df.head(10)\n",
    "    down10 = df.tail(10)\n",
    "    print(top10,down10)\n",
    "    return top10['Metric'].tolist(), down10['Metric'].tolist()\n",
    "\n",
    "def avg_change_df(df, big_string, big_string2, avg_string):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    query1 = f\"\"\"\n",
    "                SELECT t.Ticker, {big_string}\n",
    "                FROM temp_table t  \n",
    "                LEFT JOIN sym_ticker_region s ON t.Ticker = SUBSTRING(s.ticker_region, 1, LENGTH(s.ticker_region)-3) AND SUBSTRING(s.ticker_region, -2, 3) = 'US'\n",
    "                LEFT JOIN FF_ADVANCED_AF a ON s.fsym_id = a.fsym_id AND a.date < t.Implosion_Date\n",
    "                GROUP BY t.Ticker\n",
    "                ORDER BY t.Ticker\n",
    "            \"\"\"\n",
    "    query2 = f\"\"\"\n",
    "                SELECT {avg_string}\n",
    "                FROM temp_table t  \n",
    "                LEFT JOIN sym_ticker_region s ON t.Ticker = SUBSTRING(s.ticker_region, 1, LENGTH(s.ticker_region)-3) AND SUBSTRING(s.ticker_region, -2, 3) = 'US'\n",
    "                LEFT JOIN FF_ADVANCED_AF a ON s.fsym_id = a.fsym_id AND a.date > t.Implosion_Date\n",
    "                GROUP BY t.Ticker\n",
    "                ORDER BY t.Ticker\n",
    "            \"\"\"\n",
    "    df1 = spark.sql(query1)\n",
    "    df2 = spark.sql(query2)\n",
    "    print(df1.show(5))\n",
    "    \n",
    "    df1 = df1.toPandas()\n",
    "    df2 = df2.toPandas()\n",
    "    \n",
    "    non_string_columns = df1.select_dtypes(exclude=['object']).columns\n",
    "    df1 = df1[non_string_columns]\n",
    "    df2 = df2[non_string_columns]\n",
    "    \n",
    "    null_threshold = 200\n",
    "    columns_to_drop = df1.columns[df1.isnull().sum() > null_threshold]\n",
    "    df1 = df1.drop(columns=columns_to_drop)\n",
    "    df2 = df2.drop(columns=columns_to_drop)\n",
    "    # print(\"NULLS:\")\n",
    "    # print(df1.isnull().sum())\n",
    "    # print(df2.isnull().sum())\n",
    "    \n",
    "    percentage_change_df = ((df1 - df2) / df2) * 100\n",
    "    #print(percentage_change_df)\n",
    "    #print(\"LENGTH: \",len(percentage_change_df))\n",
    "    \n",
    "    \n",
    "    metric_dict = {}\n",
    "    for column in percentage_change_df.columns:\n",
    "        percentage_change_df[column] = percentage_change_df[column].replace([np.inf, -np.inf], np.nan)\n",
    "        new_col = percentage_change_df[column].dropna()\n",
    "        mean_val = new_col.mean()\n",
    "        stddev_val = new_col.std()\n",
    "        z_score_threshold = 3.0\n",
    "        new_col = new_col[(new_col >= mean_val - z_score_threshold * stddev_val) &\n",
    "        (new_col <= mean_val + z_score_threshold * stddev_val)]\n",
    "        #if new_col.std() < 1000:\n",
    "        metric_dict[column] = new_col.mean()\n",
    "    #print(metric_dict)\n",
    "    metric_df = pd.DataFrame(list(metric_dict.items()), columns=['Metric', 'Value'])\n",
    "    #metric_df.to_csv('ChangesBeforeImplosionA4yrs.csv', index=False)\n",
    "    return metric_df\n",
    "\n",
    "def pct_change_df(df, big_string, big_string2):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    query1 = f\"\"\"\n",
    "                SELECT {big_string}\n",
    "                FROM temp_table t  \n",
    "                LEFT JOIN sym_ticker_region s ON t.Ticker = SUBSTRING(s.ticker_region, 1, LENGTH(s.ticker_region)-3) AND SUBSTRING(s.ticker_region, -2, 3) = 'US'\n",
    "                LEFT JOIN FF_ADVANCED_AF a ON s.fsym_id = a.fsym_id AND YEAR(a.date) = YEAR(t.Implosion_Date)\n",
    "                ORDER BY t.Ticker\n",
    "            \"\"\"\n",
    "    query2 = f\"\"\"\n",
    "                SELECT {big_string2}\n",
    "                FROM temp_table t  \n",
    "                LEFT JOIN sym_ticker_region s ON t.Ticker = SUBSTRING(s.ticker_region, 1, LENGTH(s.ticker_region)-3) AND SUBSTRING(s.ticker_region, -2, 3) = 'US'\n",
    "                LEFT JOIN FF_ADVANCED_AF b ON s.fsym_id = b.fsym_id AND YEAR(b.date) = YEAR(t.Implosion_Prev4Years)\n",
    "                ORDER BY t.Ticker\n",
    "            \"\"\"\n",
    "    df1 = spark.sql(query1)\n",
    "    df2 = spark.sql(query2)\n",
    "    \n",
    "    df1 = df1.toPandas()\n",
    "    df2 = df2.toPandas()\n",
    "    \n",
    "    non_string_columns = df1.select_dtypes(exclude=['object']).columns\n",
    "    df1 = df1[non_string_columns]\n",
    "    df2 = df2[non_string_columns]\n",
    "    \n",
    "    null_threshold = 200\n",
    "    columns_to_drop = df1.columns[df1.isnull().sum() > null_threshold]\n",
    "    df1 = df1.drop(columns=columns_to_drop)\n",
    "    df2 = df2.drop(columns=columns_to_drop)\n",
    "    # print(\"NULLS:\")\n",
    "    # print(df1.isnull().sum())\n",
    "    # print(df2.isnull().sum())\n",
    "    \n",
    "    percentage_change_df = ((df1 - df2) / df2) * 100\n",
    "    #print(percentage_change_df)\n",
    "    #print(\"LENGTH: \",len(percentage_change_df))\n",
    "    \n",
    "    \n",
    "    metric_dict = {}\n",
    "    for column in percentage_change_df.columns:\n",
    "        percentage_change_df[column] = percentage_change_df[column].replace([np.inf, -np.inf], np.nan)\n",
    "        new_col = percentage_change_df[column].dropna()\n",
    "        mean_val = new_col.mean()\n",
    "        stddev_val = new_col.std()\n",
    "        z_score_threshold = 3.0\n",
    "        new_col = new_col[(new_col >= mean_val - z_score_threshold * stddev_val) &\n",
    "        (new_col <= mean_val + z_score_threshold * stddev_val)]\n",
    "        #if new_col.std() < 1000:\n",
    "        metric_dict[column] = new_col.mean()\n",
    "    #print(metric_dict)\n",
    "    metric_df = pd.DataFrame(list(metric_dict.items()), columns=['Metric', 'Value'])\n",
    "    #metric_df.to_csv('ChangesBeforeImplosionA4yrs.csv', index=False)\n",
    "    return metric_df\n",
    "    \n",
    "    # df['pct_change'] = (df[metric_curr] - df[metric_prev])/df[metric_prev]\n",
    "    # df['pct_change'] = df['pct_change'].replace([np.inf, -np.inf], np.nan) \n",
    "    # df=df.dropna(axis=0)\n",
    "    # mean_val = df['pct_change'].mean()\n",
    "    # stddev_val = df['pct_change'].std()\n",
    "    # z_score_threshold = 3.0\n",
    "    # df = df[\n",
    "    # (df['pct_change'] >= mean_val - z_score_threshold * stddev_val) &\n",
    "    # (df['pct_change'] <= mean_val + z_score_threshold * stddev_val)]\n",
    "    # new_mean = df['pct_change'].mean()\n",
    "    # return new_mean\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_metric_changes(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Date']).dt.date\n",
    "    df_metrics = spark.sql(\"SELECT * FROM FF_ADVANCED_AF LIMIT 10\")\n",
    "    df_metrics = df_metrics.columns[:5]\n",
    "    result_string = ', '.join('a.' + item for item in df_metrics)\n",
    "    result_string2 = ', '.join('b.' + item for item in df_metrics)\n",
    "    result_list = result_string.split(',')\n",
    "    avg_string =  [f'AVG({element})' for element in result_list]\n",
    "    avg_string = ', '.join(avg_string)\n",
    "    top10s = []\n",
    "    bottom10s = []\n",
    "    for y in range(1,2):\n",
    "        #df['Implosion_Prev4Years'] = df['Implosion_Date'] - pd.DateOffset(years=y)\n",
    "    #print(result_string)\n",
    "        new_df = avg_change_df(df, result_string, result_string2, avg_string)\n",
    "        top10, bottom10 = get_top_bottom_ten(new_df)\n",
    "        top10s.append(top10)\n",
    "        bottom10s.append(bottom10)\n",
    "    return top10s,bottom10s\n",
    "    # intersection1 = sorted(list(set(top10s[0]).intersection(top10s[1])))\n",
    "    # print(intersection1)\n",
    "    # intersection1 = sorted(list(set(top10s[0]).intersection(top10s[2])))\n",
    "    # print(intersection1)\n",
    "    # intersection1 = sorted(list(set(top10s[0]).intersection(top10s[3])))\n",
    "    # print(intersection1)\n",
    "    # intersection1 = sorted(list(set(top10s[1]).intersection(top10s[2])))\n",
    "    # print(intersection1)\n",
    "    # intersection1 = sorted(list(set(top10s[1]).intersection(top10s[3])))\n",
    "    # print(intersection1)\n",
    "    # intersection1 = sorted(list(set(top10s[2]).intersection(top10s[3])))\n",
    "    # print(intersection1)\n",
    "\n",
    "tops, bottoms = get_metric_changes('imploded_tickers_dates.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4454efe2-9115-402a-bc4c-c795839e6e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['avg(ff_amort_dfd_chrg)',\n",
       "  'avg(ff_amort_intang)',\n",
       "  'avg(ff_amort_cf)',\n",
       "  'avg(ff_assets_intl)']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88bc63eb-7d98-4de8-b665-808509638bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "MBGCF\n",
      "query done\n",
      "filtering done\n",
      "corr done\n",
      "query done\n",
      "filtering done\n",
      "corr done\n",
      "ECK\n",
      "query done\n",
      "filtering done\n",
      "corr done\n",
      "TAOP\n",
      "query done\n",
      "filtering done\n",
      "corr done\n",
      "query done\n",
      "filtering done\n",
      "corr done\n",
      "GIPL\n",
      "query done\n",
      "filtering done\n",
      "corr done\n",
      "query done\n",
      "filtering done\n",
      "corr done\n",
      "RTME\n",
      "query done\n",
      "filtering done\n",
      "corr done\n",
      "query done\n",
      "filtering done\n",
      "corr done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: `to_list` loads all data into the driver's memory. It should only be used if the resulting list is expected to be small.\n",
      "  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 62\u001b[0m\n\u001b[1;32m     56\u001b[0m     metric_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[38;5;28mlist\u001b[39m(avg_dict\u001b[38;5;241m.\u001b[39mitems()), columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetric\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     57\u001b[0m     metric_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAvg_CorrelationsADVANCEDAF.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 62\u001b[0m \u001b[43mcorr_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimploded_tickers_dates.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [21], line 43\u001b[0m, in \u001b[0;36mcorr_analysis\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     41\u001b[0m corr_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m df_metrics:\n\u001b[0;32m---> 43\u001b[0m     met_values \u001b[38;5;241m=\u001b[39m [corr_mat[metric] \u001b[38;5;28;01mfor\u001b[39;00m corr_mat \u001b[38;5;129;01min\u001b[39;00m corr_matrices \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(corr_mat[metric])]\n\u001b[1;32m     44\u001b[0m     met_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(met_values)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m met_count \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Cell \u001b[0;32mIn [21], line 43\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m corr_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m df_metrics:\n\u001b[0;32m---> 43\u001b[0m     met_values \u001b[38;5;241m=\u001b[39m [\u001b[43mcorr_mat\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m corr_mat \u001b[38;5;129;01min\u001b[39;00m corr_matrices \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pd\u001b[38;5;241m.\u001b[39misna(corr_mat[metric])]\n\u001b[1;32m     44\u001b[0m     met_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(met_values)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m met_count \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/pandas/series.py:7273\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   7266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28mtype\u001b[39m(n) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mint\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mstart, key\u001b[38;5;241m.\u001b[39mstop])) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   7267\u001b[0m         \u001b[38;5;28mtype\u001b[39m(key) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mint\u001b[39m\n\u001b[1;32m   7268\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39mdata_type, (IntegerType, LongType))\n\u001b[1;32m   7269\u001b[0m     ):\n\u001b[1;32m   7270\u001b[0m         \u001b[38;5;66;03m# Seems like pandas Series always uses int as positional search when slicing\u001b[39;00m\n\u001b[1;32m   7271\u001b[0m         \u001b[38;5;66;03m# with ints, searches based on index values when the value is int.\u001b[39;00m\n\u001b[1;32m   7272\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miloc[key]\n\u001b[0;32m-> 7273\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   7274\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SparkPandasIndexingError:\n\u001b[1;32m   7275\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   7276\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey length (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m) exceeds index depth (\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   7277\u001b[0m             \u001b[38;5;28mlen\u001b[39m(key), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal\u001b[38;5;241m.\u001b[39mindex_level\n\u001b[1;32m   7278\u001b[0m         )\n\u001b[1;32m   7279\u001b[0m     )\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/pandas/indexing.py:562\u001b[0m, in \u001b[0;36mLocIndexerLike.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    559\u001b[0m     psdf_or_psser \u001b[38;5;241m=\u001b[39m psdf\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m remaining_index \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 562\u001b[0m     pdf_or_pser \u001b[38;5;241m=\u001b[39m \u001b[43mpsdf_or_psser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    563\u001b[0m     length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(pdf_or_pser)\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/pandas/series.py:1698\u001b[0m, in \u001b[0;36mSeries._to_pandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_pandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries:\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1696\u001b[0m \u001b[38;5;124;03m    Same as `to_pandas()`, without issuing the advice log for internal usage.\u001b[39;00m\n\u001b[1;32m   1697\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_to_internal_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/pandas/series.py:7298\u001b[0m, in \u001b[0;36mSeries._to_internal_pandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   7292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_to_internal_pandas\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries:\n\u001b[1;32m   7293\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   7294\u001b[0m \u001b[38;5;124;03m    Return a pandas Series directly from _internal to avoid overhead of copy.\u001b[39;00m\n\u001b[1;32m   7295\u001b[0m \n\u001b[1;32m   7296\u001b[0m \u001b[38;5;124;03m    This method is for internal use only.\u001b[39;00m\n\u001b[1;32m   7297\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 7298\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_psdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_internal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pandas_frame\u001b[49m[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname]\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/pandas/utils.py:588\u001b[0m, in \u001b[0;36mlazy_property.<locals>.wrapped_lazy_property\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    585\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[1;32m    586\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_lazy_property\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name):\n\u001b[0;32m--> 588\u001b[0m         \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name, \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr_name)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/pandas/internal.py:1056\u001b[0m, in \u001b[0;36mInternalFrame.to_pandas_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[38;5;124;03m\"\"\"Return as pandas DataFrame.\"\"\"\u001b[39;00m\n\u001b[1;32m   1055\u001b[0m sdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_internal_spark_frame\n\u001b[0;32m-> 1056\u001b[0m pdf \u001b[38;5;241m=\u001b[39m \u001b[43msdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pdf) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sdf\u001b[38;5;241m.\u001b[39mschema) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1058\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pdf\u001b[38;5;241m.\u001b[39mastype(\n\u001b[1;32m   1059\u001b[0m         {field\u001b[38;5;241m.\u001b[39mname: spark_type_to_pandas_dtype(field\u001b[38;5;241m.\u001b[39mdataType) \u001b[38;5;28;01mfor\u001b[39;00m field \u001b[38;5;129;01min\u001b[39;00m sdf\u001b[38;5;241m.\u001b[39mschema}\n\u001b[1;32m   1060\u001b[0m     )\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/pandas/conversion.py:208\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    209\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    211\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:1216\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m \n\u001b[1;32m   1198\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1216\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def consistent_changes(ticker, big_string):\n",
    "    start_date = pd.to_datetime(\"2009-01-01\")\n",
    "    query1 = f\"\"\"\n",
    "                SELECT b.FF_PRICE_CLOSE_FP, {big_string}\n",
    "                FROM FF_ADVANCED_AF a\n",
    "                LEFT JOIN sym_ticker_region d ON d.fsym_id = a.fsym_id\n",
    "                LEFT JOIN FF_BASIC_QF b ON b.date=a.date AND d.fsym_id=b.fsym_id\n",
    "                WHERE d.ticker_region = \"{ticker}-US\" \n",
    "                AND a.date >= \"{start_date}\"\n",
    "                ORDER BY a.date\n",
    "            \"\"\"\n",
    "    q_df = spark.sql(query1)\n",
    "    print(\"query done\")\n",
    "    q_df = ps.DataFrame(q_df)\n",
    "    non_string_columns = q_df.select_dtypes(exclude=['object']).columns\n",
    "    q_df = q_df[non_string_columns]\n",
    "    print(\"filtering done\")\n",
    "    # null_threshold = 10\n",
    "    # columns_to_drop = q_df.columns[q_df.isnull().sum() > null_threshold]\n",
    "    # q_df = q_df.drop(columns=columns_to_drop)\n",
    "    correlations = q_df.corr()['FF_PRICE_CLOSE_FP']\n",
    "    print(\"corr done\")\n",
    "    return correlations\n",
    "\n",
    "def corr_analysis(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Date']).dt.date\n",
    "    df_metrics = spark.sql(\"SELECT * FROM FF_ADVANCED_AF LIMIT 10\")\n",
    "    df_metrics = df_metrics.columns[:50]\n",
    "    result_string = ', '.join('a.' + item for item in df_metrics)\n",
    "    result_string2 = ', '.join('b.' + item for item in df_metrics)\n",
    "    print(len(df_metrics))\n",
    "    corr_matrices = []\n",
    "    for t in df['Ticker'].unique().tolist():\n",
    "        print(t)\n",
    "        mat = consistent_changes(t, result_string)\n",
    "        if not(mat.empty):\n",
    "            corr_matrices.append(consistent_changes(t, result_string))\n",
    "    avg_dict = {}\n",
    "    df_metrics = corr_matrices[0].index.tolist()\n",
    "    corr_count = 0\n",
    "    for metric in df_metrics:\n",
    "        met_values = [corr_mat[metric] for corr_mat in corr_matrices if not pd.isna(corr_mat[metric])]\n",
    "        met_count = len(met_values)\n",
    "\n",
    "        if met_count != 0:\n",
    "            avg_dict[metric] = np.mean(met_values)\n",
    "            corr_count += 1\n",
    "\n",
    "        if corr_count % 25 == 0:\n",
    "            metric_df = pd.DataFrame(list(avg_dict.items()), columns=['Metric', 'Value'])\n",
    "            metric_df.to_csv(\"Avg_CorrelationsADVANCEDAF.csv\")\n",
    "        \n",
    "        \n",
    "    print(avg_dict)\n",
    "    metric_df = pd.DataFrame(list(avg_dict.items()), columns=['Metric', 'Value'])\n",
    "    metric_df.to_csv(\"Avg_CorrelationsADVANCEDAF.csv\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "corr_analysis('imploded_tickers_dates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed286246-6e05-4ed4-8b91-6f2a14abe39b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700de0e-2f78-48aa-8c16-e12e167d67ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_bottom_five(df):\n",
    "    df = df.sort_values(by='Value')\n",
    "    df=df.dropna()\n",
    "    top5 = df.head(10)\n",
    "    down5 = df.tail(10)\n",
    "    print(top5)\n",
    "    print(down5)\n",
    "    print(len(df))\n",
    "\n",
    "\n",
    "df = pd.read_csv('Avg_Correlations.csv', index_col=None)\n",
    "get_top_bottom_five(df)\n",
    "#YOU'VE DONE WORST CHANGES NOW FIND OUT WHICH ONES DECREASE CONSISTENTLY\n",
    "#ALSO FIGURE OUT MEANS BEFORE PERIOD AND AFTER PERIOD USING QUARTERLY AND COMPARE DIFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007cd3b-3e7f-4dfa-87a8-3c329fa9519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c66b85-a467-4538-91ee-a400c24c56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b54c9-5ddc-4459-afff-5247cc6b7b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
