{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b862fd7-75a8-4818-95c2-5375dde4b345",
   "metadata": {},
   "source": [
    "## Building the dataset that will be input into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda6c879-2ecc-475a-9f90-99bccea780bd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop-3.2.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.7-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "2024-01-07 00:01:19,487 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-01-07 00:01:22,533 WARN spark.ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"Building dataset\").config(\"hive.metastore.uris\", \"thrift://amok:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "spark.sql(\"USE 2023_11_02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54391b05-47b8-4d4b-b943-eda194771996",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import lit,col\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "#from boruta import BorutaPy\n",
    "#from fredapi import Fred\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import csv\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from pyspark.mllib.evaluation import BinaryClassificationMetrics\n",
    "from CreateDataset import get_features_all_stocks_seq, get_full_series_stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba39e2b3-07f5-4fd9-adf5-735d881bf565",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImplosion_Start_Date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImplosion_Start_Date\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImplosion_End_Date\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImplosion_End_Date\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mget_features_all_stocks_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_feats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#get stocks that have data in the ff_advanced_der_af for all years, not just prices (or not even)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#for boruta, maybe treat each column separately 22*10 features?\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#df.show()\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mcount())\n",
      "File \u001b[0;32m~/Stock-Implosion-Prediction-FYP/CreateDataset.py:200\u001b[0m, in \u001b[0;36mget_features_all_stocks_seq\u001b[0;34m(df, all_feats)\u001b[0m\n\u001b[1;32m    198\u001b[0m spark_df\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtemp_table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m all_feats:\n\u001b[0;32m--> 200\u001b[0m     col_names \u001b[38;5;241m=\u001b[39m \u001b[43mget_not_null_cols\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    202\u001b[0m     col_names \u001b[38;5;241m=\u001b[39m get_feature_col_names()\n",
      "File \u001b[0;32m~/Stock-Implosion-Prediction-FYP/CreateDataset.py:135\u001b[0m, in \u001b[0;36mget_not_null_cols\u001b[0;34m(df, table)\u001b[0m\n\u001b[1;32m    131\u001b[0m column_types \u001b[38;5;241m=\u001b[39m q_df\u001b[38;5;241m.\u001b[39mdtypes\n\u001b[1;32m    133\u001b[0m null_pcts \u001b[38;5;241m=\u001b[39m [F\u001b[38;5;241m.\u001b[39msum(F\u001b[38;5;241m.\u001b[39mcol(col_name)\u001b[38;5;241m.\u001b[39misNull()\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m/\u001b[39m F\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m col_name \u001b[38;5;129;01min\u001b[39;00m q_df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m--> 135\u001b[0m columns_to_drop \u001b[38;5;241m=\u001b[39m [col_name \u001b[38;5;28;01mfor\u001b[39;00m col_name, null_pct \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(q_df\u001b[38;5;241m.\u001b[39mcolumns, null_pcts) \u001b[38;5;28;01mif\u001b[39;00m null_pct \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.2\u001b[39m]\n\u001b[1;32m    137\u001b[0m q_df \u001b[38;5;241m=\u001b[39m q_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;241m*\u001b[39mcolumns_to_drop)\n\u001b[1;32m    139\u001b[0m cols \u001b[38;5;241m=\u001b[39m q_df\u001b[38;5;241m.\u001b[39mcolumns\n",
      "File \u001b[0;32m~/Stock-Implosion-Prediction-FYP/CreateDataset.py:135\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    131\u001b[0m column_types \u001b[38;5;241m=\u001b[39m q_df\u001b[38;5;241m.\u001b[39mdtypes\n\u001b[1;32m    133\u001b[0m null_pcts \u001b[38;5;241m=\u001b[39m [F\u001b[38;5;241m.\u001b[39msum(F\u001b[38;5;241m.\u001b[39mcol(col_name)\u001b[38;5;241m.\u001b[39misNull()\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m/\u001b[39m F\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m col_name \u001b[38;5;129;01min\u001b[39;00m q_df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[0;32m--> 135\u001b[0m columns_to_drop \u001b[38;5;241m=\u001b[39m [col_name \u001b[38;5;28;01mfor\u001b[39;00m col_name, null_pct \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(q_df\u001b[38;5;241m.\u001b[39mcolumns, null_pcts) \u001b[38;5;28;01mif\u001b[39;00m null_pct \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.2\u001b[39m]\n\u001b[1;32m    137\u001b[0m q_df \u001b[38;5;241m=\u001b[39m q_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;241m*\u001b[39mcolumns_to_drop)\n\u001b[1;32m    139\u001b[0m cols \u001b[38;5;241m=\u001b[39m q_df\u001b[38;5;241m.\u001b[39mcolumns\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/column.py:904\u001b[0m, in \u001b[0;36mColumn.__nonzero__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__nonzero__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 904\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert column into bool: please use \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mand\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    905\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m~\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnot\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m when building DataFrame boolean expressions.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot convert column into bool: please use '&' for 'and', '|' for 'or', '~' for 'not' when building DataFrame boolean expressions."
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "#full_series_stocks = get_full_series_stocks(df) #this gets the stocks that have data since 2001\n",
    "#filtered_df = df[df['fsym_id'].isin(full_series_stocks)]\n",
    "df['Implosion_Start_Date'] = pd.to_datetime(df['Implosion_Start_Date'])\n",
    "df['Implosion_End_Date'] = pd.to_datetime(df['Implosion_End_Date'])\n",
    "df = get_features_all_stocks_seq(df, all_feats=False) #get stocks that have data in the ff_advanced_der_af for all years, not just prices (or not even)\n",
    "#for boruta, maybe treat each column separately 22*10 features?\n",
    "#df.show()\n",
    "print(df.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30d429c-7d3d-4325-bc80-2471074b13ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df.filter(df['label'] == 1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7c04c-6dc4-4ac2-a544-5e3c187cfaaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63645eaf-dfa5-43d5-ab36-e94c6ced8c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def train_test_split(df):\n",
    "    train, test = df.randomSplit([0.7,0.3], 22)\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def confusion_matrix_pandas(df):\n",
    "    df = df.toPandas()\n",
    "    cm = confusion_matrix(df['label'], df['prediction'])\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def model_testing(df):\n",
    "    df = df.filter(reduce(lambda acc, column: acc & (F.size(col(column)) == 22), df.columns[1:-1], lit(True)))\n",
    "\n",
    "    #need to decide whether to only include stocks that started from 2000, or include just from e.g. 2019\n",
    "    #temporary measure - replace with 0\n",
    "    #try imputer?\n",
    "    #look into masking\n",
    "    \n",
    "    print(\"Number of records: \", df.count())\n",
    "    features = df.columns[1:-1]\n",
    "    list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "    for f in features:\n",
    "        df = df.withColumn(f, list_to_vector_udf(f))\n",
    "    df.show(2)\n",
    "    train_df, test_df = train_test_split(df)\n",
    "    \n",
    "    vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features_vector\")\n",
    "    train_df = vector_assembler.transform(train_df)\n",
    "    test_df = vector_assembler.transform(test_df)\n",
    "    \n",
    "    lr = LogisticRegression(featuresCol=\"features_vector\", labelCol=\"label\")\n",
    "    rf = RandomForestClassifier(featuresCol='features_vector', labelCol='label')\n",
    "    gbt = GBTClassifier(featuresCol = 'features_vector', labelCol='label')\n",
    "    models = [rf]\n",
    "    model_names = ['random forest']\n",
    "    \n",
    "    for model, model_name in zip(models, model_names):\n",
    "        if model_name == 'random forest':\n",
    "            paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(model.numTrees, [100]) \\\n",
    "                .addGrid(model.maxDepth, [15]) \\\n",
    "                .addGrid(model.minInstancesPerNode, [5]) \\\n",
    "                .build()\n",
    "        elif model_name == 'logistic regression':\n",
    "            paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(lr.regParam, [0.1]) \\\n",
    "                .addGrid(lr.elasticNetParam, [0.5]) \\\n",
    "                .addGrid(lr.maxIter, [100]) \\\n",
    "                .build()\n",
    "        else:\n",
    "            paramGrid = ParamGridBuilder() \\\n",
    "                .addGrid(gbt.maxDepth, [10]) \\\n",
    "                .addGrid(gbt.maxBins, [32]) \\\n",
    "                .addGrid(gbt.maxIter, [20]) \\\n",
    "                .build()\n",
    "\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "        crossval = CrossValidator(estimator=model,\n",
    "                                  estimatorParamMaps=paramGrid,\n",
    "                                  evaluator=evaluator,\n",
    "                                  numFolds=5, parallelism=12)\n",
    "\n",
    "        cvModel = crossval.fit(train_df)\n",
    "\n",
    "        avg_metrics = cvModel.avgMetrics\n",
    "        print(model_name.upper())\n",
    "\n",
    "        for i, acc in enumerate(avg_metrics):\n",
    "            print(f\"Fold {i + 1} - Validation Accuracy: {acc}\")\n",
    "\n",
    "        best_model = cvModel.bestModel\n",
    "\n",
    "        predictions = best_model.transform(test_df)\n",
    "\n",
    "        confusion_matrix_pandas(predictions.select('label', 'prediction'))\n",
    "        \n",
    "def basic_test(df):\n",
    "    df = df.filter(reduce(lambda acc, column: acc & (F.size(col(column)) == 22), df.columns[1:-1], lit(True)))\n",
    "    print(\"Number of records: \", df.count())\n",
    "    features = df.columns[1:-1]\n",
    "    list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "    for f in features:\n",
    "        df = df.withColumn(f, list_to_vector_udf(f))\n",
    "    df.show(2)\n",
    "    train_df, test_df = train_test_split(df)\n",
    "    \n",
    "    vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features_vector\")\n",
    "    train_df = vector_assembler.transform(train_df)\n",
    "    test_df = vector_assembler.transform(test_df)\n",
    "    \n",
    "    lr = LogisticRegression(featuresCol=\"features_vector\", labelCol=\"label\")\n",
    "    rf = RandomForestClassifier(featuresCol='features_vector', labelCol='label')\n",
    "    gbt = GBTClassifier(featuresCol = 'features_vector', labelCol='label')\n",
    "    models = [lr, rf, gbt]\n",
    "    model_names = ['logistic regression']\n",
    "    \n",
    "    for model, model_name in zip(models, model_names):\n",
    "        if model_name == 'boosted trees':\n",
    "            paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(gbt.maxDepth, [10])\n",
    "             .addGrid(gbt.maxIter, [50])\n",
    "             .build())\n",
    "        elif model_name == 'random forest':\n",
    "            paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [5]) #5,10,15\n",
    "             .addGrid(rf.numTrees, [20]) #20,50,100\n",
    "             .build())\n",
    "        else:\n",
    "            paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.maxIter, [10])  # Number of iterations #10,50,100\n",
    "             .addGrid(lr.regParam, [0.01])  # Regularization parameter #0.01,0.1,0.5\n",
    "             .addGrid(lr.elasticNetParam, [0.0])  # Elastic net parameter (0 for L2, 1 for L1) 0.0,0.5,1.0\n",
    "             .build())\n",
    "    \n",
    "\n",
    "\n",
    "        evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "        \n",
    "        crossval = CrossValidator(estimator = model, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\n",
    "        \n",
    "        model = crossval.fit(train_df)\n",
    "        \n",
    "        best_model = model.bestModel\n",
    "\n",
    "        print(model_name.upper())\n",
    "\n",
    "        predictions = best_model.transform(test_df)\n",
    "\n",
    "        confusion_matrix_pandas(predictions.select('label', 'prediction'))\n",
    "        \n",
    "        recall = evaluator.evaluate(predictions)\n",
    "        print(f\"Recall: {recall}\")\n",
    "    \n",
    "\n",
    "\n",
    "#basic_test(df)\n",
    "#test_pandas(df)\n",
    "#model_testing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb392cd-c08f-4884-bf9d-80ef508a15ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics \n",
    "    \n",
    "def model_training(df, classifier):\n",
    "    df = df.filter(reduce(lambda acc, column: acc & (F.size(col(column)) == 22), df.columns[1:-1], lit(True)))\n",
    "    print(\"Number of records: \", df.count())\n",
    "    \n",
    "    features = df.columns[1:-1]\n",
    "\n",
    "    list_to_vector_udf = F.udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "    for f in features:\n",
    "        df = df.withColumn(f, list_to_vector_udf(f))\n",
    " \n",
    "    train_df, test_df = train_test_split(df)\n",
    "\n",
    "    vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features_vector\")\n",
    "    train_df = vector_assembler.transform(train_df)\n",
    "    test_df = vector_assembler.transform(test_df)\n",
    "\n",
    "    if classifier == 'LogisticRegression':\n",
    "        param_space = {\n",
    "            'regParam': hp.uniform('regParam', 0.01, 1.0),\n",
    "            'elasticNetParam': hp.uniform('elasticNetParam', 0.0, 1.0)\n",
    "        }\n",
    "        classifier_instance = LogisticRegression(featuresCol=\"features_vector\", labelCol=\"label\")\n",
    "    elif classifier == 'RandomForest':\n",
    "        param_space = {\n",
    "            'numTrees': hp.choice('numTrees', [10, 20, 30]),\n",
    "            'maxDepth': hp.choice('maxDepth', [5, 10, 15])\n",
    "        }\n",
    "        classifier_instance = RandomForestClassifier(featuresCol='features_vector', labelCol='label', numTrees=10)\n",
    "    elif classifier == 'GBT':\n",
    "        param_space = {\n",
    "            'maxDepth': hp.choice('maxDepth', [5, 10, 15]),\n",
    "            'maxIter': hp.choice('maxIter', [10, 20, 30])\n",
    "        }\n",
    "        classifier_instance = GBTClassifier(featuresCol='features_vector', labelCol='label')\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported classifier\")\n",
    "\n",
    "    def objective(params):\n",
    "        classifier_instance.setParams(**params)\n",
    "        #evaluator = BinaryClassificationEvaluator(metricName='areaUnderPR')\n",
    "        evaluator = MulticlassClassificationEvaluator(predictionCol='prediction', metricName='recallByLabel')\n",
    "        crossval = CrossValidator(estimator=classifier_instance,\n",
    "                                  estimatorParamMaps=ParamGridBuilder().build(),\n",
    "                                  evaluator=evaluator,\n",
    "                                  numFolds=3)\n",
    "        \n",
    "        cv_model = crossval.fit(train_df)\n",
    "        predictions = cv_model.transform(train_df)\n",
    "        val_metric = evaluator.evaluate(predictions)\n",
    "        return val_metric\n",
    "\n",
    "    # Find the best hyperparameters\n",
    "    best_params = fmin(fn=objective, space=param_space, algo=tpe.suggest, max_evals=1)\n",
    "    print(\"Best hyperparameters: \", best_params)\n",
    "\n",
    "    # Train the model with the best hyperparameters\n",
    "    classifier_instance.setParams(**best_params)\n",
    "    best_model = classifier_instance.fit(train_df)\n",
    "    feature_importances = best_model.featureImportances\n",
    "    print(\"Feature Importances:\")\n",
    "    for i in range(len(feature_importances)):\n",
    "        print(\"Feature {}: {}\".format(i, feature_importances[i]))\n",
    "        \n",
    "    predictions = best_model.transform(test_df)\n",
    "\n",
    "    confusion_matrix_pandas(predictions.select('label', 'prediction'))\n",
    "    \n",
    "    \n",
    "    \n",
    "model_training(df, 'LogisticRegression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8882a899-98b5-4b95-ba1c-5de2a137a0aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b910d24-3885-4d00-857b-90ca055539c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
    "\n",
    "def train_tree(train_df):\n",
    "    rf = RandomForestClassifier(featuresCol='features_vector', labelCol='label', maxDepth=maxDepth, numTrees=numTrees)\n",
    "    model = rf.fit(train_df)\n",
    "    preds = model.transform()\n",
    "    \n",
    "\n",
    "            \n",
    "    \n",
    "    \n",
    "\n",
    "def train_with_hyperopt(params, df):\n",
    "    numTrees = int(params['numTrees'])\n",
    "    maxDepth = int(params['maxDepth'])\n",
    "\n",
    "    model, f1_score = train_rf(numTrees, maxDepth, df)\n",
    "    loss = - f1_score\n",
    "    return {'loss': loss, 'status': STATUS_OK}\n",
    "\n",
    "def prepare_df():\n",
    "    df = df.filter(reduce(lambda acc, column: acc & (F.size(col(column)) == 22), df.columns[1:-1], lit(True)))\n",
    "    print(\"Number of records: \", df.count())\n",
    "    features = df.columns[1:-1]\n",
    "    list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "    for f in features:\n",
    "        df = df.withColumn(f, list_to_vector_udf(f))\n",
    "    df.show(2)\n",
    "    train_df, test_df = train_test_split(df)\n",
    "    \n",
    "    vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features_vector\")\n",
    "    train_df = vector_assembler.transform(train_df)\n",
    "    test_df = vector_assembler.transform(test_df)\n",
    "    return train_df, test_df\n",
    "\n",
    "# train_df, test_df = prepare_df(df)\n",
    "# train_with_hyperopt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f034c3-4ed9-400e-a07b-6d2c6be018d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(filtered_df.filter(F.col('label')==1).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b9373-fceb-4cf1-9c66-aab50d165c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.filter(F.col('label')==1).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1698c017-00e4-4208-8305-518dd1746e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c12702-06f6-442b-9787-3d03ef354eec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
