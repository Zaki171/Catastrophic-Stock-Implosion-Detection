{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98d51d0-085d-4172-85cd-dc0f19412ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6802cbd1-16cf-4e4a-8137-9894bdec78e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='2022_10_22', catalog='spark_catalog', description='FactSet data version for the day', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse'),\n",
       " Database(name='2023_04_01', catalog='spark_catalog', description='FactSet data version for the day', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse'),\n",
       " Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"List available databases and tables\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2886cd22-2c06-4882-b434-e5ed726788c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "spark.sql(\"USE 2023_04_01\")\n",
    "    # Assuming that 'ticker' is a valid Python variable\n",
    "\n",
    "# query = f\"\"\"SELECT ticker_region FROM sym_ticker_region WHERE ticker_region LIKE \"%-US\" \"\"\"\n",
    "# df = spark.sql(query)\n",
    "# df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "# ticker_list = df.collect()\n",
    "# print(len(ticker_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ca1fc72f-006a-44b5-83fd-21ba1e382ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ff_amort_cf  ff_assets_sep_accts\n",
      "0     0.011000                  NaN\n",
      "1          NaN                  NaN\n",
      "2          NaN                  NaN\n",
      "3     7.807261                  NaN\n",
      "4          NaN                  NaN\n",
      "   ff_amort_cf  ff_assets_sep_accts\n",
      "0        0.042                  NaN\n",
      "1          NaN                  NaN\n",
      "2          NaN                  NaN\n",
      "3          NaN                  NaN\n",
      "4          NaN                  NaN\n",
      "True\n",
      "     ff_amort_cf  ff_assets_sep_accts\n",
      "0     -73.809524                  NaN\n",
      "1            NaN                  NaN\n",
      "2            NaN                  NaN\n",
      "3            NaN                  NaN\n",
      "4            NaN                  NaN\n",
      "..           ...                  ...\n",
      "473   261.323869                  NaN\n",
      "474          NaN                  NaN\n",
      "475          NaN                  NaN\n",
      "476          NaN                  NaN\n",
      "477          NaN                  NaN\n",
      "\n",
      "[478 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import when\n",
    "import pyspark.pandas as ps\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#fund_df = fund_df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "\n",
    "def query(ticker,date):\n",
    "    query = f\"\"\"SELECT d.ticker_region, a.date, FF_PRICE_CLOSE_FP\n",
    "                FROM FF_BASIC_AF a \n",
    "                LEFT JOIN sym_ticker_region d ON d.fsym_id = a.fsym_id \n",
    "                WHERE d.ticker_region = \"{ticker}-US\" AND a.date = \"{date}\"\n",
    "                \"\"\"\n",
    "\n",
    "    fund_df = spark.sql(query)\n",
    "    fund_df = fund_df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    \n",
    "    return fund_df\n",
    "\n",
    "def pull_prev_data(df, big_string, big_string2):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    query1 = f\"\"\"\n",
    "                SELECT {big_string}\n",
    "                FROM temp_table t  \n",
    "                LEFT JOIN sym_ticker_region s ON t.Ticker = SUBSTRING(s.ticker_region, 1, LENGTH(s.ticker_region)-3) AND SUBSTRING(s.ticker_region, -2, 3) = 'US'\n",
    "                LEFT JOIN FF_ADVANCED_QF a ON s.fsym_id = a.fsym_id AND YEAR(a.date) = YEAR(t.Implosion_Date) AND QUARTER(a.date) = QUARTER(t.Implosion_Date)\n",
    "                ORDER BY t.Ticker\n",
    "            \"\"\"\n",
    "    query2 = f\"\"\"\n",
    "                SELECT {big_string2}\n",
    "                FROM temp_table t  \n",
    "                LEFT JOIN sym_ticker_region s ON t.Ticker = SUBSTRING(s.ticker_region, 1, LENGTH(s.ticker_region)-3) AND SUBSTRING(s.ticker_region, -2, 3) = 'US'\n",
    "                LEFT JOIN FF_ADVANCED_QF b ON s.fsym_id = b.fsym_id AND YEAR(b.date) = YEAR(t.Implosion_Prev4Years) AND QUARTER(b.date) = QUARTER(t.Implosion_Prev4Years)\n",
    "                ORDER BY t.Ticker\n",
    "            \"\"\"\n",
    "    df1 = spark.sql(query1)\n",
    "    df2 = spark.sql(query2)\n",
    "    df1 = df1.toPandas()\n",
    "    df2 = df2.toPandas()\n",
    "    print(df1.head(5))\n",
    "    print(df2.head(5))\n",
    "    print(len(df1)==len(df2))\n",
    "    percentage_change_df = ((df1 - df2) / df2) * 100\n",
    "    print(percentage_change_df)\n",
    "    \n",
    "    # df['pct_change'] = (df[metric_curr] - df[metric_prev])/df[metric_prev]\n",
    "    # df['pct_change'] = df['pct_change'].replace([np.inf, -np.inf], np.nan) \n",
    "    # df=df.dropna(axis=0)\n",
    "    # mean_val = df['pct_change'].mean()\n",
    "    # stddev_val = df['pct_change'].std()\n",
    "    # z_score_threshold = 3.0\n",
    "    # df = df[\n",
    "    # (df['pct_change'] >= mean_val - z_score_threshold * stddev_val) &\n",
    "    # (df['pct_change'] <= mean_val + z_score_threshold * stddev_val)]\n",
    "    # new_mean = df['pct_change'].mean()\n",
    "    # return new_mean\n",
    "\n",
    "\n",
    "def create_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Date']).dt.date\n",
    "    df['Implosion_Year'] = df['Implosion_Date']\n",
    "    df['Implosion_Year'] =  pd.to_datetime(df['Implosion_Year']).dt.year\n",
    "    df['Implosion_Prev4Years'] = df['Implosion_Date'] - pd.DateOffset(years=1)\n",
    "    df['Implosion_Prev4Years'] =  pd.to_datetime(df['Implosion_Prev4Years']).dt.year\n",
    "    df = df.sort_values(by='Ticker')\n",
    "    df_metrics = spark.sql(\"SELECT * FROM FF_BASIC_AF LIMIT 10\")\n",
    "    df_metrics = df_metrics.columns\n",
    "    #df_metrics.remove('ff_actg_standard')\n",
    "    df_metrics.remove('ff_eps_rpt_date')\n",
    "    df_metrics.remove('ff_source_is_date')\n",
    "    df_metrics.remove('ff_source_bs_date')\n",
    "    df_metrics.remove('ff_source_cf_date')\n",
    "    df_metrics.remove('ff_source_doc')\n",
    "    df_metrics.remove('ff_report_freq_code')\n",
    "    df_metrics.remove('ff_fiscal_date')\n",
    "    df_metrics.remove('ff_actg_standard')\n",
    "    df_metrics = df_metrics[6:100]\n",
    "    metric_dict = {}\n",
    "    for m in df_metrics:\n",
    "        print(m)\n",
    "        avg = pull_prev_data(df, m)\n",
    "        metric_dict[m] = avg\n",
    "    # df=pull_prev_data(df, 'FF_PRICE_CLOSE_FP')\n",
    "    # return df\n",
    "    metric_df = pd.DataFrame(list(metric_dict.items()), columns=['Metric', 'Value'])\n",
    "    metric_df.to_csv('ChangesBeforeImplosion1yr.csv', index=False)\n",
    "    return metric_df\n",
    "\n",
    "def pull2(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Date']).dt.date\n",
    "    df['Implosion_Prev4Years'] = df['Implosion_Date'] - pd.DateOffset(years=4)\n",
    "    df_metrics = spark.sql(\"SELECT * FROM FF_ADVANCED_QF LIMIT 10\")\n",
    "    df_metrics = df_metrics.columns\n",
    "    df_metrics = df_metrics[5:7]\n",
    "    result_string = ', '.join('a.' + item for item in df_metrics)\n",
    "    result_string2 = ', '.join('b.' + item for item in df_metrics)\n",
    "    #print(result_string)\n",
    "    pull_prev_data(df, result_string, result_string2)\n",
    "\n",
    "df = pull2('imploded_tickers_dates.csv')\n",
    "# df.show(100)\n",
    "# rdf=query('AAPL','2012-01-01')\n",
    "# rdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc63eb-7d98-4de8-b665-808509638bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a700de0e-2f78-48aa-8c16-e12e167d67ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Metric       Value\n",
      "8   ff_loan_loss_prov -152.202681\n",
      "65     ff_for_exch_cf   -3.384404\n",
      "55       ff_wkcap_chg   -1.504482\n",
      "66     ff_chg_cash_cf   -1.243237\n",
      "15      ff_eq_aff_inc   -0.797521\n",
      "93      ff_rsrv_noneq   -0.750000\n",
      "74  ff_price_close_fp   -0.669528\n",
      "58    ff_loan_decr_cf   -0.631032\n",
      "4     ff_oper_exp_oth   -0.599675\n",
      "51      ff_dfd_tax_cf   -0.526569\n",
      "                   Metric     Value\n",
      "19           ff_cash_only  1.198095\n",
      "41                ff_debt  1.310782\n",
      "7          ff_int_exp_tot  1.440537\n",
      "73         ff_com_shs_out  1.494861\n",
      "92             ff_ppe_net  1.512333\n",
      "31             ff_debt_st  1.572959\n",
      "59  ff_sale_assets_bus_cf  2.036180\n",
      "56          ff_acq_bus_cf  2.350508\n",
      "26   ff_assets_oth_intang  6.881336\n",
      "10       ff_ptx_xord_chrg  6.944790\n"
     ]
    }
   ],
   "source": [
    "def get_top_bottom_five(df):\n",
    "    df = df.sort_values(by='Value')\n",
    "    df=df.dropna()\n",
    "    top5 = df.head(10)\n",
    "    down5 = df.tail(10)\n",
    "    print(top5)\n",
    "    print(down5)\n",
    "\n",
    "\n",
    "df = pd.read_csv('ChangesBeforeImplosion1yr.csv')\n",
    "get_top_bottom_five(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1007cd3b-3e7f-4dfa-87a8-3c329fa9519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c66b85-a467-4538-91ee-a400c24c56ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768b54c9-5ddc-4459-afff-5247cc6b7b7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
