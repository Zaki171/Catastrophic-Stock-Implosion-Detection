{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98d51d0-085d-4172-85cd-dc0f19412ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6802cbd1-16cf-4e4a-8137-9894bdec78e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='2022_10_22', catalog='spark_catalog', description='FactSet data version for the day', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse'),\n",
       " Database(name='2023_04_01', catalog='spark_catalog', description='FactSet data version for the day', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse'),\n",
       " Database(name='default', catalog='spark_catalog', description='Default Hive database', locationUri='hdfs://bialobog.cs.ucl.ac.uk:8020/user/hive/warehouse')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"List available databases and tables\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2886cd22-2c06-4882-b434-e5ed726788c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "spark.sql(\"USE 2023_04_01\")\n",
    "    # Assuming that 'ticker' is a valid Python variable\n",
    "\n",
    "# query = f\"\"\"SELECT ticker_region FROM sym_ticker_region WHERE ticker_region LIKE \"%-US\" \"\"\"\n",
    "# df = spark.sql(query)\n",
    "# df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "# ticker_list = df.collect()\n",
    "# print(len(ticker_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b450c0c-854a-40c1-bfa9-b9c5f545b953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Ticker Implosion_Date Implosion_Date - 1yrs Implosion_Date - 4yrs     prev4  \\\n",
      "0  EMCMF     2022-09-04            2021-12-31            2018-12-31  0.507639   \n",
      "1  MGAWF     2022-10-30            2021-12-31            2018-12-31  0.000000   \n",
      "2   SWNM     2022-04-10            2021-12-31            2018-12-31  0.000000   \n",
      "3  MBGCF     2016-10-02            2015-12-31            2012-12-31  0.000000   \n",
      "4  MBGCF     2018-05-06            2017-12-31            2014-12-31  0.000000   \n",
      "\n",
      "      prev1  \n",
      "0  0.490836  \n",
      "1  0.000000  \n",
      "2  0.000000  \n",
      "3  0.000000  \n",
      "4  0.000000  \n",
      "  Ticker Implosion_Date Implosion_Date - 1yrs Implosion_Date - 4yrs     prev4  \\\n",
      "0  EMCMF     2022-09-04            2021-12-31            2018-12-31  0.507639   \n",
      "1  MGAWF     2022-10-30            2021-12-31            2018-12-31  0.090330   \n",
      "2   SWNM     2022-04-10            2021-12-31            2018-12-31  0.000000   \n",
      "3  MBGCF     2016-10-02            2015-12-31            2012-12-31  0.000000   \n",
      "4  MBGCF     2018-05-06            2017-12-31            2014-12-31  0.000000   \n",
      "\n",
      "      prev1  \n",
      "0  0.490836  \n",
      "1  0.061774  \n",
      "2  0.000000  \n",
      "3  0.000000  \n",
      "4  0.000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [66], line 75\u001b[0m\n\u001b[1;32m     70\u001b[0m     df\u001b[38;5;241m=\u001b[39mpull_prev_data(df)\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m---> 75\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_df\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimploded_tickers_dates.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m100\u001b[39m)\n",
      "Cell \u001b[0;32mIn [66], line 70\u001b[0m, in \u001b[0;36mcreate_df\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     68\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImplosion_Date - 4yrs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m  pd\u001b[38;5;241m.\u001b[39mto_datetime(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImplosion_Date - 4yrs\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mdate\n\u001b[1;32m     69\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImplosion_Date - 4yrs\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m  df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImplosion_Date - 4yrs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mreplace(month\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m12\u001b[39m, day\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m31\u001b[39m))\n\u001b[0;32m---> 70\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mpull_prev_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "Cell \u001b[0;32mIn [66], line 30\u001b[0m, in \u001b[0;36mpull_prev_data\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     27\u001b[0m date_value \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImplosion_Date - 4yrs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     28\u001b[0m dv2 \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImplosion_Date - 1yrs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 30\u001b[0m result_df \u001b[38;5;241m=\u001b[39m \u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate_value\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m r2 \u001b[38;5;241m=\u001b[39m query(ticker_value, dv2)\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result_df\u001b[38;5;241m.\u001b[39mempty:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/pandas/conversion.py:208\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    205\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;66;03m# Below is toPandas without Arrow optimization.\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    209\u001b[0m column_counter \u001b[38;5;241m=\u001b[39m Counter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    211\u001b[0m corrected_dtypes: List[Optional[Type]] \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:1216\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m \n\u001b[1;32m   1198\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1216\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "\n",
    "#fund_df = fund_df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "\n",
    "def query(ticker,date):\n",
    "    query = f\"\"\"SELECT d.ticker_region, a.date, FF_PRICE_CLOSE_FP\n",
    "                FROM FF_BASIC_AF a \n",
    "                LEFT JOIN sym_ticker_region d ON d.fsym_id = a.fsym_id \n",
    "                WHERE d.ticker_region = \"{ticker}-US\" AND a.date = \"{date}\" \n",
    "                \"\"\"\n",
    "\n",
    "    fund_df = spark.sql(query)\n",
    "    fund_df = fund_df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    \n",
    "    return fund_df\n",
    "\n",
    "def pull_prev_data(df):\n",
    "    df['prev4'] = 0\n",
    "    df['prev1']=0\n",
    "\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        ticker_value = row['Ticker']\n",
    "        date_value = row['Implosion_Date - 4yrs']\n",
    "        dv2 = row['Implosion_Date - 1yrs']\n",
    "\n",
    "        result_df = query(ticker_value, date_value).toPandas()\n",
    "        r2 = query(ticker_value, dv2).toPandas()\n",
    "\n",
    "        if not result_df.empty:\n",
    "            fourth_price = result_df['FF_PRICE_CLOSE_FP'].iloc[0]\n",
    "\n",
    "            # Update 'prev' column based on conditions\n",
    "            condition = (\n",
    "                (df['Ticker'].values == result_df['ticker_region'].values) &\n",
    "                (df['Implosion_Date - 4yrs'].values == result_df['date'].values)\n",
    "            )\n",
    "            df.loc[condition, 'prev4'] = fourth_price\n",
    "            \n",
    "        if not r2.empty:\n",
    "            first_price = r2['FF_PRICE_CLOSE_FP'].iloc[0]\n",
    "\n",
    "            # Update 'prev' column based on conditions\n",
    "            condition = (\n",
    "                (df['Ticker'].values == r2['ticker_region'].values) &\n",
    "                (df['Implosion_Date - 1yrs'].values == r2['date'].values)\n",
    "            )\n",
    "            df.loc[condition, 'prev1'] = first_price\n",
    "            print(df.head())\n",
    "    df = spark.createDataFrame(df)\n",
    "    df.show()\n",
    "    df=df.orderBy('Ticker','Implosion_Date')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def create_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=None, usecols=['Ticker', 'Implosion_Date'])\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Date']).dt.date\n",
    "    df['Implosion_Date - 1yrs'] = df['Implosion_Date'] - pd.DateOffset(years=1)\n",
    "    df['Implosion_Date - 1yrs'] =  pd.to_datetime(df['Implosion_Date - 1yrs']).dt.date\n",
    "    df['Implosion_Date - 1yrs'] =  df['Implosion_Date - 1yrs'].apply(lambda x: x.replace(month=12, day=31))\n",
    "    df['Implosion_Date - 4yrs'] = df['Implosion_Date'] - pd.DateOffset(years=4)\n",
    "    df['Implosion_Date - 4yrs'] =  pd.to_datetime(df['Implosion_Date - 4yrs']).dt.date\n",
    "    df['Implosion_Date - 4yrs'] =  df['Implosion_Date - 4yrs'].apply(lambda x: x.replace(month=12, day=31))\n",
    "    df=pull_prev_data(df)\n",
    "    return df\n",
    "    \n",
    "    \n",
    "\n",
    "df = create_df('imploded_tickers_dates.csv')\n",
    "df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1fc72f-006a-44b5-83fd-21ba1e382ebc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc63eb-7d98-4de8-b665-808509638bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a700de0e-2f78-48aa-8c16-e12e167d67ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
