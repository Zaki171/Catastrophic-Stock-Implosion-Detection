{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b862fd7-75a8-4818-95c2-5375dde4b345",
   "metadata": {},
   "source": [
    "## Building the dataset that will be input into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9cedb4-51ae-42f3-8064-60ce28dd207c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"Building dataset\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "spark.sql(\"USE 2023_04_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63645eaf-dfa5-43d5-ab36-e94c6ced8c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------------------+--------------------+----------------------+--------------------+-----------------------+--------------------------+--------------------+--------------------+--------------------------+--------------------+-------------------------+-----------------------+--------------------+-----+\n",
      "| fsym_id|     ff_non_oper_exp|ff_net_inc_bef_xord_gr|      ff_oper_inc_gr|ff_ut_non_oper_inc_oth|         ff_earn_yld|ff_net_inc_dil_aft_xord|ff_net_inc_basic_beft_xord|        ff_assets_gr|          ff_fcf_yld|ff_net_inc_dil_bef_unusual|            ff_wkcap|ff_net_inc_basic_aft_xord|ff_oper_inc_aft_unusual|      ff_net_inc_dil|label|\n",
      "+--------+--------------------+----------------------+--------------------+----------------------+--------------------+-----------------------+--------------------------+--------------------+--------------------+--------------------------+--------------------+-------------------------+-----------------------+--------------------+-----+\n",
      "|B00FG1-R|[0.0,0.0,-1.1,-0....|  [-1406.7796610169...|[-1406.7796610169...|  [0.0,0.0,0.0,0.0,...|[4.37799,10.2158,...|   [-0.59,-8.89,29.9...|      [-0.59,-8.89,29.9...|[27.3337012353016...|[-95.4447,-11.974...|      [-0.59,-8.89,30.7...|[-4.682,-0.499,21...|     [-0.59,-8.89,29.9...|   [-0.59,-8.89,15.9...|[-0.59,-8.89,29.9...|    0|\n",
      "|B01DPB-R|[-140.0,-60.8,-12...|  [8.65021770682148...|[6.11214456550625...|  [-36.7,-30.8,-43....|[2.24332,3.17269,...|   [374.3,484.0,611....|      [374.3,484.0,611....|[13.2700334847025...|[2.9145,3.49776,9...|      [446.61,505.0,668...|[453.1,759.2,1437...|     [374.3,484.0,611....|   [695.3,744.6,917....|[374.3,484.0,611....|    0|\n",
      "+--------+--------------------+----------------------+--------------------+----------------------+--------------------+-----------------------+--------------------------+--------------------+--------------------+--------------------------+--------------------+-------------------------+-----------------------+--------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "Fold 1 - Validation Accuracy: 0.7968007667163937\n",
      "Fold 2 - Validation Accuracy: 0.7975906992496407\n",
      "Fold 3 - Validation Accuracy: 0.784914133019812\n",
      "Fold 4 - Validation Accuracy: 0.7871176121101903\n",
      "Fold 5 - Validation Accuracy: 0.793853793225699\n",
      "Fold 6 - Validation Accuracy: 0.7930582380724224\n",
      "Fold 7 - Validation Accuracy: 0.5\n",
      "Fold 8 - Validation Accuracy: 0.5\n",
      "+--------+-----+----------+\n",
      "| fsym_id|label|prediction|\n",
      "+--------+-----+----------+\n",
      "|B00FG1-R|    0|       0.0|\n",
      "|B01DPB-R|    0|       0.0|\n",
      "|B01HWF-R|    0|       0.0|\n",
      "|B04CB3-R|    0|       0.0|\n",
      "|B04HL7-R|    0|       0.0|\n",
      "|B04XY5-R|    0|       0.0|\n",
      "|B0592Z-R|    0|       0.0|\n",
      "|B05BF8-R|    0|       0.0|\n",
      "|B06303-R|    0|       0.0|\n",
      "|B06GLB-R|    0|       0.0|\n",
      "|B06HD3-R|    0|       0.0|\n",
      "|B06Z9K-R|    0|       0.0|\n",
      "|B084Z2-R|    0|       0.0|\n",
      "|B08855-R|    0|       0.0|\n",
      "|B0CXCV-R|    1|       0.0|\n",
      "|B0KV90-R|    0|       0.0|\n",
      "|B0P5YF-R|    0|       0.0|\n",
      "|B0R9V8-R|    0|       0.0|\n",
      "|B0TXKG-R|    0|       0.0|\n",
      "|B0V6BZ-R|    0|       0.0|\n",
      "|B0VZ2Z-R|    0|       0.0|\n",
      "|B0XB1M-R|    0|       0.0|\n",
      "|B0ZCK4-R|    0|       0.0|\n",
      "|B12BZP-R|    0|       0.0|\n",
      "|B12HXG-R|    0|       0.0|\n",
      "|B14762-R|    0|       0.0|\n",
      "|B14DJ7-R|    0|       0.0|\n",
      "|B14GN9-R|    0|       0.0|\n",
      "|B15GK7-R|    0|       1.0|\n",
      "|B18QS9-R|    0|       0.0|\n",
      "|B18RVB-R|    0|       0.0|\n",
      "|B191CL-R|    0|       0.0|\n",
      "|B19F2Y-R|    0|       0.0|\n",
      "|B19ST9-R|    0|       0.0|\n",
      "|B1C27H-R|    0|       0.0|\n",
      "|B1CVR7-R|    0|       0.0|\n",
      "|B1H55G-R|    0|       0.0|\n",
      "|B1PVB4-R|    0|       0.0|\n",
      "|B1R19Z-R|    0|       0.0|\n",
      "|B1VPDJ-R|    0|       0.0|\n",
      "|B1YGL0-R|    0|       0.0|\n",
      "|B1ZWDG-R|    0|       0.0|\n",
      "|B21HN4-R|    0|       0.0|\n",
      "|B21JK0-R|    0|       0.0|\n",
      "|B21YHK-R|    0|       0.0|\n",
      "|B241MN-R|    0|       0.0|\n",
      "|B28MQ9-R|    0|       0.0|\n",
      "|B28NSB-R|    0|       0.0|\n",
      "|B29NP8-R|    0|       0.0|\n",
      "|B29NTD-R|    0|       0.0|\n",
      "|B29X4H-R|    0|       0.0|\n",
      "|B2DCHG-R|    0|       0.0|\n",
      "|B2DYZC-R|    0|       0.0|\n",
      "|B2F25G-R|    0|       0.0|\n",
      "|B2F8KN-R|    0|       0.0|\n",
      "|B2FCRR-R|    0|       0.0|\n",
      "|B2FJ50-R|    0|       0.0|\n",
      "|B2FTR9-R|    0|       0.0|\n",
      "|B2FWWC-R|    0|       0.0|\n",
      "|B2G10C-R|    0|       0.0|\n",
      "|B2G7PV-R|    0|       0.0|\n",
      "|B2G826-R|    0|       0.0|\n",
      "|B2HTF1-R|    0|       0.0|\n",
      "|B2HYP5-R|    0|       0.0|\n",
      "|B2J17N-R|    0|       0.0|\n",
      "|B2J8Z4-R|    0|       0.0|\n",
      "|B2JT9Z-R|    1|       0.0|\n",
      "|B2L9FN-R|    0|       0.0|\n",
      "|B2M67L-R|    0|       0.0|\n",
      "|B2QVH9-R|    0|       0.0|\n",
      "|B2SX3Y-R|    0|       0.0|\n",
      "|B2YZLH-R|    0|       0.0|\n",
      "|B314TM-R|    0|       0.0|\n",
      "|B31C6T-R|    0|       0.0|\n",
      "|B38P5P-R|    0|       0.0|\n",
      "|B3C712-R|    0|       0.0|\n",
      "|B3DLD3-R|    0|       0.0|\n",
      "|B3DMG4-R|    0|       0.0|\n",
      "|B3DNJ5-R|    1|       0.0|\n",
      "|B3DVXC-R|    0|       0.0|\n",
      "|B3DWZD-R|    1|       0.0|\n",
      "|B3G4BK-R|    0|       0.0|\n",
      "|B3GKSL-R|    0|       0.0|\n",
      "|B3HL2X-R|    1|       0.0|\n",
      "|B3K1X9-R|    0|       0.0|\n",
      "|B3KGPP-R|    0|       0.0|\n",
      "|B3KSTH-R|    0|       0.0|\n",
      "|B3L86F-R|    0|       0.0|\n",
      "|B3L98G-R|    0|       0.0|\n",
      "|B3L9DL-R|    0|       0.0|\n",
      "|B3LPNG-R|    0|       0.0|\n",
      "|B3MVC1-R|    0|       0.0|\n",
      "|B3MZ3P-R|    0|       0.0|\n",
      "|B3W9WB-R|    0|       0.0|\n",
      "|B3XHBN-R|    0|       0.0|\n",
      "|B3XV10-R|    0|       0.0|\n",
      "|B40L7K-R|    0|       0.0|\n",
      "|B46L9T-R|    0|       0.0|\n",
      "|B4HF53-R|    0|       0.0|\n",
      "|B4JZXB-R|    0|       0.0|\n",
      "+--------+-----+----------+\n",
      "only showing top 100 rows\n",
      "\n",
      "True Positives: 101\n",
      "True Negatives: 9883\n",
      "False Positives: 25\n",
      "False Negatives: 553\n",
      "Precision: 0.8015873015873016\n",
      "Recall: 0.154434250764526\n",
      "F1 Score: 0.25897435897435894\n"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import lit,col\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "#from boruta import BorutaPy\n",
    "#from fredapi import Fred\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import csv\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "\n",
    "\n",
    "def get_macro_features():\n",
    "    # fred_key = 'bdfdde3b7a21b7d528011d17996b0b8e'\n",
    "    # fred = Fred(api_key=fred_key)\n",
    "    # cpi = fred.get_series(series_id='CPIAUCSL')\n",
    "    # cpi_change = cpi.pct_change()\n",
    "    # unemp = fred.get_series(series_id='UNRATE')\n",
    "    # gdp = fred.get_series(series_id='GDP')\n",
    "    # gdp_change = gdp.pct_change()\n",
    "    # df = pd.DataFrame({'CPI_change': cpi_change,'Unemployment_Rate': unemp,'GDP_change': gdp_change})\n",
    "    # df.to_csv('macro.csv')\n",
    "    df = pd.read_csv('macro.csv')\n",
    "    return df\n",
    "\n",
    "def get_all_stocks():\n",
    "    query = f\"\"\"SELECT s.ticker_region, sc.fref_listing_exchange FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\"\"\"\n",
    "    df = spark.sql(query)\n",
    "    df = df.withColumn(\"ticker_region\", regexp_replace(\"ticker_region\", \"-US$\", \"\"))\n",
    "    ticker_list = [row.ticker_region for row in df.collect()]\n",
    "    return ticker_list\n",
    "\n",
    "\n",
    "\n",
    "def get_non_imp_stocks_query():\n",
    "    df2 = spark.createDataFrame(get_implosion_df('imploded_stocks.csv'))\n",
    "    df2.createOrReplaceTempView(\"imp_table\")\n",
    "    query = f\"\"\"SELECT s.ticker_region, s.fsym_id FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\n",
    "                AND NOT EXISTS (\n",
    "                SELECT 1\n",
    "                FROM imp_table\n",
    "                WHERE s.ticker_region = CONCAT(imp_table.Ticker, '-US') )    \n",
    "                \"\"\"\n",
    "    df = spark.sql(query)\n",
    "    print(\"got non imploded stocks\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_implosion_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df = df[df['Implosion_Start_Date'].notnull()]\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Start_Date'])\n",
    "    return df\n",
    "\n",
    "def get_non_implosion_df(filename):\n",
    "    df = pd.read_csv(filename, index_col=False)\n",
    "    df = df[df['Implosion_Start_Date'].isnull()]\n",
    "    return df\n",
    "\n",
    "def get_features_for_imploded_stocks(df, big_string, table):\n",
    "    df=spark.createDataFrame(df)\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    # query = \"\"\"SELECT t.Ticker, t.Implosion_Date, t.Implosion_Next_Year, a.date, a.ff_gross_inc, b.date, b.ff_gross_inc, c.date, c.ff_gross_inc\n",
    "    #             FROM temp_table t \n",
    "    #             LEFT JOIN sym_ticker_region s ON s.ticker_region = CONCAT(t.Ticker, '-US')\n",
    "    #             LEFT JOIN FF_BASIC_AF a ON s.fsym_id = a.fsym_id AND YEAR(a.date) = YEAR(t.Implosion_Date)-1\n",
    "    #             LEFT JOIN FF_BASIC_AF b ON s.fsym_id = b.fsym_id AND YEAR(b.date) = YEAR(t.Implosion_Date)-2\n",
    "    #             LEFT JOIN FF_BASIC_AF c ON s.fsym_id = c.fsym_id AND YEAR(c.date) = YEAR(t.Implosion_Date)-3\n",
    "    #             ORDER BY t.Ticker, a.date\n",
    "    # \"\"\"\n",
    "    query = f\"\"\"SELECT t.ticker_region, a.date, {big_string}, t.Implosion_Next_Year FROM temp_table t\n",
    "                    LEFT JOIN sym_ticker_region s ON s.ticker_region = t.ticker_region\n",
    "                    LEFT JOIN {table} a ON a.fsym_id = s.fsym_id AND t.Year = YEAR(a.date)\n",
    "                    LEFT JOIN FF_BASIC_AF b ON b.fsym_id = s.fsym_id AND YEAR(b.date) = t.Year\n",
    "                    ORDER BY t.ticker_region, a.date\n",
    "    \"\"\"\n",
    "    df2 = spark.sql(query)\n",
    "    print(\"imploded query done\")\n",
    "    return df2\n",
    "    \n",
    "    \n",
    "def get_features_for_non_imploded(metric_string, metric_string2,table):\n",
    "    df = spark.createDataFrame(get_non_implosion_df('imploded_stocks3.csv'))\n",
    "    df.createOrReplaceTempView(\"temp_table\")\n",
    "    query = f\"\"\"WITH RankedData AS (\n",
    "    SELECT\n",
    "        t.ticker_region, s.fsym_id,\n",
    "        a.date,\n",
    "        {metric_string},\n",
    "        ROW_NUMBER() OVER (PARTITION BY t.ticker_region ORDER BY a.date DESC) AS row_num\n",
    "        FROM temp_table t\n",
    "        LEFT JOIN sym_ticker_region s ON s.ticker_region = t.ticker_region\n",
    "        LEFT JOIN {table} a ON a.fsym_id = s.fsym_id \n",
    "        WHERE a.date < (\n",
    "            SELECT MAX(date)\n",
    "            FROM {table} a_sub\n",
    "            WHERE a_sub.fsym_id = s.fsym_id ))\n",
    "    SELECT\n",
    "        r.ticker_region, r.date, {metric_string2}\n",
    "        FROM RankedData r\n",
    "        WHERE row_num <= 4\n",
    "        ORDER BY ticker_region, date\"\"\"\n",
    "    new_df = spark.sql(query)\n",
    "    print(\"non imploded query done\")\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def create_non_imploded_ds(table):\n",
    "    #df_metrics = ps.DataFrame(spark.sql(f\"SELECT * FROM {table} LIMIT 10\")) #get all the metrics\n",
    "    # cols = []\n",
    "    # for c in df_metrics.columns:\n",
    "    #     if df_metrics[c].dtype=='float64':#get all the metrics we can calculate correlations with\n",
    "    #         cols.append(c)\n",
    "    cols = ['ff_debt_entrpr_val', 'ff_tot_debt_tcap_std', 'ff_fix_assets_com_eq', 'ff_debt_eq', 'ff_inven_curr_assets', 'ff_liabs_lease', 'ff_ltd_tcap', 'ff_sales_wkcap',\n",
    "           'ff_bps_gr', 'ff_oper_inc_tcap', 'ff_assets_gr', 'ff_fcf_yld', 'ff_mkt_val_gr', 'ff_earn_yld', 'ff_pbk_tang', 'ff_zscore', 'ff_entrpr_val_sales', 'ff_psales_dil', 'ff_roea', 'ff_dps_gr',\n",
    "           'ff_loan_loss_pct', 'ff_loan_loss_actual_rsrv'] #advanced_der_qf\n",
    "    \n",
    "    metric_string = ', '.join('a.' + item for item in cols)\n",
    "    metric_string2 = ', '.join('r.' + item for item in cols)\n",
    "    df = get_features_for_non_imploded(metric_string, metric_string2, table)\n",
    "    df = df.withColumn(\"Implosion_Next_Year\", lit(0))\n",
    "    return df\n",
    "\n",
    "def create_imploded_df(table):\n",
    "    df = get_implosion_df('imploded_stocks3.csv')\n",
    "    df = df.drop(df.columns[0], axis=1)\n",
    "    df['Implosion_Year'] = df['Implosion_Date'].dt.year-1\n",
    "    df['Implosion_Next_Year'] = 1\n",
    "    \n",
    "    additional_rows_1 = df.copy()\n",
    "    additional_rows_1['Implosion_Year'] = df['Implosion_Year'] - 1\n",
    "    additional_rows_1['Implosion_Next_Year'] = 0\n",
    "    additional_rows_2 = df.copy()\n",
    "    additional_rows_2['Implosion_Year'] = df['Implosion_Year'] - 2\n",
    "    additional_rows_2['Implosion_Next_Year'] = 0\n",
    "    additional_rows_3 = df.copy()\n",
    "    additional_rows_3['Implosion_Year'] = df['Implosion_Year'] - 3\n",
    "    additional_rows_3['Implosion_Next_Year'] = 0\n",
    "    df = pd.concat([df, additional_rows_1, additional_rows_2, additional_rows_3])\n",
    "    df = df.sort_values(by=['ticker_region', 'Implosion_Year'])\n",
    "    df = df.reset_index(drop=True)\n",
    "    df =df.rename({'Implosion_Year' : 'Year'},axis=1)\n",
    "    \n",
    "    # df_metrics = ps.DataFrame(spark.sql(f\"SELECT * FROM {table} LIMIT 10\")) #get all the metrics\n",
    "    # cols = []\n",
    "    # for c in df_metrics.columns:\n",
    "    #     if df_metrics[c].dtype=='float64':#get all the metrics we can calculate correlations with\n",
    "    #         cols.append(c)\n",
    "    \n",
    "    cols = ['ff_debt_entrpr_val', 'ff_tot_debt_tcap_std', 'ff_fix_assets_com_eq', 'ff_debt_eq', 'ff_inven_curr_assets', 'ff_liabs_lease', 'ff_ltd_tcap', 'ff_sales_wkcap',\n",
    "           'ff_bps_gr', 'ff_oper_inc_tcap', 'ff_assets_gr', 'ff_fcf_yld', 'ff_mkt_val_gr', 'ff_earn_yld', 'ff_pbk_tang', 'ff_zscore', 'ff_entrpr_val_sales', 'ff_psales_dil', 'ff_roea', 'ff_dps_gr',\n",
    "           'ff_loan_loss_pct', 'ff_loan_loss_actual_rsrv']\n",
    "    \n",
    "    metric_string = ', '.join('a.' + item for item in cols)\n",
    "    df = get_features_for_imploded_stocks(df, metric_string, table)\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "    \n",
    "def create_dataset(table):\n",
    "    # df = get_implosion_df('imploded_stocks.csv')\n",
    "    # df = df.drop(df.columns[0], axis=1)\n",
    "    # df['Implosion_Year'] = df['Implosion_Date'].dt.year\n",
    "    # df['Implosion_Next_Year'] = 1\n",
    "    # get_features_for_imploded_stocks(df)\n",
    "    #print(df.head())\n",
    "    #df=spark.createDataFrame(df)\n",
    "    #df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    imp_df = create_imploded_df(table).toPandas()\n",
    "    non_imp_df =create_non_imploded_ds(table).toPandas()\n",
    "    result_df = pd.concat([non_imp_df,imp_df], ignore_index=True)\n",
    "    #print(result_df.head())\n",
    "    result_df['date'] = pd.to_datetime(result_df['date'], format='%Y-%m-%d')\n",
    "    result_df=result_df.sort_values(by=['ticker_region','date'])\n",
    "    macro_df = get_macro_features().reset_index()\n",
    "    macro_df['Date'] = pd.to_datetime(macro_df['Date'], format='%d/%m/%Y')\n",
    "    #print(macro_df.head())\n",
    "    result_df['month_year'] = result_df['date'].dt.to_period(\"M\")\n",
    "    macro_df['Month_year'] = macro_df['Date'].dt.to_period(\"M\")\n",
    "    result_df = pd.merge(result_df, macro_df, left_on='month_year', right_on='Month_year', how='left')\n",
    "    result_df.drop(['Date', 'index', 'month_year','Month_year','GDP'],axis=1,inplace=True)\n",
    "    \n",
    "    print(result_df.head())\n",
    "    \n",
    "    null_pcts = result_df.isnull().sum()/len(result_df)\n",
    "    print(null_pcts)\n",
    "    \n",
    "    cols_to_drop = null_pcts[null_pcts > 0.1].index.tolist()\n",
    "    result_df.drop(cols_to_drop,axis=1,inplace=True)\n",
    "    print(\"dropped cols: \", cols_to_drop)\n",
    "    \n",
    "    result_df=pd.DataFrame(result_df)\n",
    "    print(\"before dropping nulls: \",len(result_df))\n",
    "    result_df = result_df.dropna()\n",
    "    print(\"after dropping nulls: \", len(result_df))\n",
    "    print(\"number of implosions: \", len(result_df[result_df['Implosion_Next_Year']==1]))\n",
    "    print(\"number of non-implosions: \", len(result_df[result_df['Implosion_Next_Year']==0]))\n",
    "    result_df.to_csv('Advanced_AF_DER_Dataset.csv', index=False)\n",
    "    print(\"dataset written\")\n",
    "    \n",
    "def get_feature_col_names():\n",
    "    csv_file_path = 'features.csv'\n",
    "    data_list = []\n",
    "    with open(csv_file_path, mode='r') as file:\n",
    "    # Create a CSV reader object\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            data_list.append(row)\n",
    "    col_list = data_list[0]\n",
    "    return col_list\n",
    "    \n",
    "    \n",
    "def get_features_all_stocks():\n",
    "    table = \"FF_ADVANCED_DER_AF\"\n",
    "    df = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "    spark_df = spark.createDataFrame(df)\n",
    "    spark_df.createOrReplaceTempView(\"temp_table\")\n",
    "    col_names = get_feature_col_names()\n",
    "    col_string = ', '.join('a.' + item for item in col_names)\n",
    "    q=f\"\"\"SELECT t.fsym_id, a.date, {col_string}\n",
    "                FROM temp_table t\n",
    "                LEFT JOIN {table} a ON t.fsym_id = a.fsym_id\n",
    "                WHERE a.date >= \"2000-01-01\"\n",
    "                ORDER BY t.fsym_id, a.date\"\"\"\n",
    "    features_df = spark.sql(q)\n",
    "    feature_cols = [col for col in features_df.columns if col not in ['fsym_id', 'date']]\n",
    "    sequences = []\n",
    "    \n",
    "\n",
    "    grouped_df = features_df.groupBy(\"fsym_id\").agg(\n",
    "        *[F.collect_list(col).alias(col) for col in feature_cols])\n",
    "\n",
    "    #feature_cols = [col for col in grouped_df.columns if col not in ['fsym_id', 'date']]\n",
    "    grouped_df_padded = grouped_df.select(\n",
    "        \"fsym_id\",\n",
    "        *[F.expr(f\"IF(size({col}) < 23, concat({col}, array_repeat(0, 23 - size({col}))), {col})\").alias(col) for col in feature_cols])\n",
    "\n",
    "    \n",
    "    spark_df = spark_df.withColumn('label', F.when(F.isnan('Implosion_Start_Date'), 0).otherwise(1))\n",
    "    joined_df = grouped_df_padded.join(spark_df.select(\"fsym_id\", \"label\"), \"fsym_id\", \"inner\")\n",
    "    joined_df=joined_df.orderBy('fsym_id')\n",
    "\n",
    "    \n",
    "    return joined_df\n",
    "\n",
    "def train_test(df):\n",
    "    df = df.filter(\n",
    "        reduce(lambda acc, column: acc & (F.size(col(column)) == 23), df.columns[1:-1], lit(True))\n",
    "        )\n",
    "    # print(filtered_df.count())\n",
    "\n",
    "    # average_lengths = df.agg(*[(F.avg(F.size(col(column))).alias(f'avg_length_{column}')) for column in df.columns[1:-1]])\n",
    "    \n",
    "    # test = padded_df.select('ff_non_oper_exp').filter(col('fsym_id')=='RTTY5P-R').collect()[0]\n",
    "    # print(test['ff_non_oper_exp'], len(test['ff_non_oper_exp']))\n",
    "\n",
    "    #need to decide whether to only include stocks that started from 2000, or include just from e.g. 2019\n",
    "    #temporary measure - replace with 0\n",
    "    #try imputer?\n",
    "    #look into masking\n",
    "    features = df.columns[1:-1]\n",
    "    list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "    for f in features:\n",
    "        df = df.withColumn(f, list_to_vector_udf(f))\n",
    "    df.show(2)\n",
    "    vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features_vector\")\n",
    "    df_assembled = vector_assembler.transform(df)\n",
    "    \n",
    "    lr = LogisticRegression(featuresCol=\"features_vector\", labelCol=\"label\")\n",
    "\n",
    "    # pipeline = Pipeline(stages=[vector_assembler, lr])\n",
    "    \n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam, [0.01, 0.1]) \\\n",
    "        .addGrid(lr.elasticNetParam, [0.0, 0.5]) \\\n",
    "        .addGrid(lr.maxIter, [10, 20]) \\\n",
    "        .build()\n",
    "\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "    crossval = CrossValidator(estimator=lr,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=evaluator,\n",
    "                              numFolds=5) \n",
    "\n",
    "    cvModel = crossval.fit(df_assembled)\n",
    "\n",
    "    avg_metrics = cvModel.avgMetrics\n",
    "\n",
    "    for i, acc in enumerate(avg_metrics):\n",
    "        print(f\"Fold {i + 1} - Validation Accuracy: {acc}\")\n",
    "\n",
    "    best_model = cvModel.bestModel\n",
    "\n",
    "    predictions = best_model.transform(df_assembled)\n",
    "    predictions.select('fsym_id', 'label', 'prediction').show(100)\n",
    "    tp = predictions.filter((predictions.label == 1) & (predictions.prediction == 1)).count()\n",
    "\n",
    "    # Calculate True Negatives (TN)\n",
    "    tn = predictions.filter((predictions.label == 0) & (predictions.prediction == 0)).count()\n",
    "\n",
    "    # Calculate False Positives (FP)\n",
    "    fp = predictions.filter((predictions.label == 0) & (predictions.prediction == 1)).count()\n",
    "\n",
    "    # Calculate False Negatives (FN)\n",
    "    fn = predictions.filter((predictions.label == 1) & (predictions.prediction == 0)).count()\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"True Positives: {tp}\")\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "\n",
    "    # You can also calculate precision, recall, and F1-score if needed\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "\n",
    "#     # Filter and display predictions for label 1\n",
    "#     predictions.filter(predictions['label'] == 1).select(\"fsym_id\", \"label\", \"prediction\").show()\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def test_pandas(df1):\n",
    "    df1 = df1.toPandas()\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "    df = df1[df1.loc[:, ~df1.columns.isin(exclude_columns)].apply(lambda row: all(len(cell) == 23 for cell in row), axis=1)]\n",
    "    print(len(df))\n",
    "    X = df.drop(exclude_columns, axis=1)\n",
    "    y = df['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    lr = LogisticRegression()\n",
    "\n",
    "    param_grid = {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.01, 0.1, 1.0, 10.0],\n",
    "        'max_iter': [100, 200, 300]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    predictions = best_model.predict(X_test)\n",
    "\n",
    "    auc = roc_auc_score(y_test, predictions)\n",
    "\n",
    "    print(f\"Area under the ROC curve (AUC): {auc}\")\n",
    "    print(\"Best model hyperparameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    #extra_df= df[df.apply(lambda row: any(len(cell) != 23 for cell in row), axis=1)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#create_dataset('FF_ADVANCED_DER_AF')\n",
    "df = get_features_all_stocks()\n",
    "train_test(df)\n",
    "#test_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096020ae-d29d-4286-98b2-dc7ec86009be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(units=32, input_shape=(23, input_features)))\n",
    "model.add(Dense(units=output_features))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7337e-6ca2-4212-b9d0-17953b8998d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT MIN(a.p_date) FROM fp_basic_prices a LEFT JOIN sym_ticker_region s ON s.fsym_id = a.fsym_id WHERE s.ticker_region = 'AACQU-US' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f034c3-4ed9-400e-a07b-6d2c6be018d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
