{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b862fd7-75a8-4818-95c2-5375dde4b345",
   "metadata": {},
   "source": [
    "## Building the dataset that will be input into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9cedb4-51ae-42f3-8064-60ce28dd207c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"Building dataset\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "spark.sql(\"USE 2023_04_01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63645eaf-dfa5-43d5-ab36-e94c6ced8c79",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Column is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 151\u001b[0m\n\u001b[1;32m    149\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimploded_stocks_price.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    150\u001b[0m full_series_stocks \u001b[38;5;241m=\u001b[39m get_full_series_stocks(df)\n\u001b[0;32m--> 151\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfsym_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_series_stocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m filtered_df \u001b[38;5;241m=\u001b[39m get_features_all_stocks_seq(filtered_df)\n\u001b[1;32m    153\u001b[0m train_test(filtered_df)\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/pandas/core/generic.py:5454\u001b[0m, in \u001b[0;36mNDFrame.filter\u001b[0;34m(self, items, like, regex, axis)\u001b[0m\n\u001b[1;32m   5452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m items \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5453\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 5454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreindex(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{name: [r \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m items \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m labels]})\n\u001b[1;32m   5455\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m like:\n\u001b[1;32m   5457\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(x) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m bool_t:\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/column.py:710\u001b[0m, in \u001b[0;36mColumn.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 710\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumn is not iterable\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Column is not iterable"
     ]
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import lit,col\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "#from boruta import BorutaPy\n",
    "#from fredapi import Fred\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import csv\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from CreateDataset import get_features_all_stocks_seq, get_full_series_stocks\n",
    "\n",
    "\n",
    "def get_macro_features():\n",
    "    # fred_key = 'bdfdde3b7a21b7d528011d17996b0b8e'\n",
    "    # fred = Fred(api_key=fred_key)\n",
    "    # cpi = fred.get_series(series_id='CPIAUCSL')\n",
    "    # cpi_change = cpi.pct_change()\n",
    "    # unemp = fred.get_series(series_id='UNRATE')\n",
    "    # gdp = fred.get_series(series_id='GDP')\n",
    "    # gdp_change = gdp.pct_change()\n",
    "    # df = pd.DataFrame({'CPI_change': cpi_change,'Unemployment_Rate': unemp,'GDP_change': gdp_change})\n",
    "    # df.to_csv('macro.csv')\n",
    "    df = pd.read_csv('macro.csv')\n",
    "    return df\n",
    "\n",
    "\n",
    "def train_test(df):\n",
    "    print(\"Number of records: \", len(df))\n",
    "    df = df.filter(reduce(lambda acc, column: acc & (F.size(col(column)) == 23), df.columns[1:-1], lit(True)))\n",
    "    # print(filtered_df.count())\n",
    "\n",
    "    # average_lengths = df.agg(*[(F.avg(F.size(col(column))).alias(f'avg_length_{column}')) for column in df.columns[1:-1]])\n",
    "    \n",
    "    # test = padded_df.select('ff_non_oper_exp').filter(col('fsym_id')=='RTTY5P-R').collect()[0]\n",
    "    # print(test['ff_non_oper_exp'], len(test['ff_non_oper_exp']))\n",
    "\n",
    "    #need to decide whether to only include stocks that started from 2000, or include just from e.g. 2019\n",
    "    #temporary measure - replace with 0\n",
    "    #try imputer?\n",
    "    #look into masking\n",
    "    features = df.columns[1:-1]\n",
    "    list_to_vector_udf = udf(lambda l: Vectors.dense(l), VectorUDT())\n",
    "    for f in features:\n",
    "        df = df.withColumn(f, list_to_vector_udf(f))\n",
    "    df.show(2)\n",
    "    vector_assembler = VectorAssembler(inputCols=features, outputCol=\"features_vector\")\n",
    "    df_assembled = vector_assembler.transform(df)\n",
    "    \n",
    "    lr = LogisticRegression(featuresCol=\"features_vector\", labelCol=\"label\")\n",
    "\n",
    "    # pipeline = Pipeline(stages=[vector_assembler, lr])\n",
    "    \n",
    "    paramGrid = ParamGridBuilder() \\\n",
    "        .addGrid(lr.regParam, [0.01, 0.1]) \\\n",
    "        .addGrid(lr.elasticNetParam, [0.0, 0.5]) \\\n",
    "        .addGrid(lr.maxIter, [10, 20]) \\\n",
    "        .build()\n",
    "\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "    crossval = CrossValidator(estimator=lr,\n",
    "                              estimatorParamMaps=paramGrid,\n",
    "                              evaluator=evaluator,\n",
    "                              numFolds=5) \n",
    "\n",
    "    cvModel = crossval.fit(df_assembled)\n",
    "\n",
    "    avg_metrics = cvModel.avgMetrics\n",
    "\n",
    "    for i, acc in enumerate(avg_metrics):\n",
    "        print(f\"Fold {i + 1} - Validation Accuracy: {acc}\")\n",
    "\n",
    "    best_model = cvModel.bestModel\n",
    "\n",
    "    predictions = best_model.transform(df_assembled)\n",
    "    predictions.select('fsym_id', 'label', 'prediction').show(100)\n",
    "    tp = predictions.filter((predictions.label == 1) & (predictions.prediction == 1)).count()\n",
    "    tn = predictions.filter((predictions.label == 0) & (predictions.prediction == 0)).count()\n",
    "    fp = predictions.filter((predictions.label == 0) & (predictions.prediction == 1)).count()\n",
    "    fn = predictions.filter((predictions.label == 1) & (predictions.prediction == 0)).count()\n",
    "    \n",
    "    print(f\"True Positives: {tp}\")\n",
    "    print(f\"True Negatives: {tn}\")\n",
    "    print(f\"False Positives: {fp}\")\n",
    "    print(f\"False Negatives: {fn}\")\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1 Score: {f1_score}\")\n",
    "\n",
    "#     # Filter and display predictions for label 1\n",
    "#     predictions.filter(predictions['label'] == 1).select(\"fsym_id\", \"label\", \"prediction\").show()\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def test_pandas(df1):\n",
    "    df1 = df1.toPandas()\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "    df = df1[df1.loc[:, ~df1.columns.isin(exclude_columns)].apply(lambda row: all(len(cell) == 23 for cell in row), axis=1)]\n",
    "    print(\"Number of records: \", len(df))\n",
    "    X = df.drop(exclude_columns, axis=1)\n",
    "    y = df['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    lr = LogisticRegression()\n",
    "\n",
    "    param_grid = {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.01, 0.1, 1.0, 10.0],\n",
    "        'max_iter': [100, 200, 300]\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(lr, param_grid, cv=5, scoring='roc_auc')\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    predictions = best_model.predict(X_test)\n",
    "\n",
    "    auc = roc_auc_score(y_test, predictions)\n",
    "\n",
    "    print(f\"Area under the ROC curve (AUC): {auc}\")\n",
    "    print(\"Best model hyperparameters:\")\n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    #extra_df= df[df.apply(lambda row: any(len(cell) != 23 for cell in row), axis=1)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#create_dataset('FF_ADVANCED_DER_AF')\n",
    "df = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "full_series_stocks = get_full_series_stocks(df)\n",
    "filtered_df = df[df['fsym_id'].isin(full_series_stocks)]\n",
    "filtered_df = get_features_all_stocks_seq(filtered_df)\n",
    "train_test(filtered_df)\n",
    "#test_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d7337e-6ca2-4212-b9d0-17953b8998d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"SELECT MIN(a.p_date) FROM fp_basic_prices a LEFT JOIN sym_ticker_region s ON s.fsym_id = a.fsym_id WHERE s.ticker_region = 'AACQU-US' \").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f034c3-4ed9-400e-a07b-6d2c6be018d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
