{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd144b3e-7d0d-4cf5-9c93-9bc5d02ad9bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop-3.2.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.7-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "2024-01-21 00:43:38,906 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-01-21 00:43:41,744 WARN spark.ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"when_approach\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "spark.sql(\"USE 2023_11_02\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69308bac-14f7-4812-84d7-bcb53404d755",
   "metadata": {},
   "source": [
    "## WHEN Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08c6105a-8637-426f-b539-f5a2f7a0a5f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,col\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "#from boruta import BorutaPy\n",
    "#from fredapi import Fred\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import csv\n",
    "from pyspark.sql import functions as F\n",
    "from functools import reduce\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "# from pyspark.ml.regression import LinearRegression\n",
    "# from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "# from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "from CreateDataset import get_tabular_dataset, get_feature_col_names, get_not_null_cols\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e750d07a-ef46-4855-b9fe-aa2c6bfe4b79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def split_train_test(df, split_date):\n",
    "    split_date = pd.to_datetime(split_date)\n",
    "    train_df = df.filter(col('date')<split_date)\n",
    "    test_df = df.filter(col('date')>=split_date)\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def forward_fill(df):\n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy('date')\n",
    "    feature_cols = df.columns[2:-1]\n",
    "    for c in feature_cols:\n",
    "        df = df.withColumn(\n",
    "            c, F.last(c, ignorenulls=True).over(window_spec)\n",
    "        )\n",
    "    return df.orderBy('fsym_id','date')\n",
    "\n",
    "\n",
    "def write_features_file(data_list, csv_file_path='features.csv'):\n",
    "    data_list = [data_list]\n",
    "    with open(csv_file_path, mode='w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row in data_list:\n",
    "            writer.writerow(row)\n",
    "    print(\"Features written: \", data_list[0])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f517213d-ec24-4599-aff8-7dff42b26765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_df(all_feats=False, imploded_only=False):\n",
    "    df = get_tabular_dataset(all_feats=all_feats, imploded_only=imploded_only)\n",
    "    df = forward_fill(df)\n",
    "    print(\"Number of rows: \", df.count())\n",
    "    print(\"Number of positives: \", df.filter(F.col('label')==1).count())\n",
    "    df=df.fillna(0.0)\n",
    "    print(\"Number of rows after dropping nulls: \", df.count())\n",
    "    print(\"Number of positives after dropping nulls: \", df.filter(F.col('label')==1).count())\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c44fee8-ddcf-4bb2-909d-5510bf9b7241",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "iteritems is deprecated and will be removed in a future version. Use .items instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ff_assets_gr', 'ff_net_inc_per_emp', 'ff_debt_entrpr_val', 'ff_fcf_yld', 'ff_sga_oth', 'ff_gross_cf_debt', 'ff_dil_adj', 'ff_shs_float', 'ff_xord', 'ff_inc_sund', 'ff_net_inc_basic_beft_xord', 'ff_non_oper_exp', 'ff_cf_ps_gr', 'ff_emp_gr', 'ff_net_inc_bef_xord_gr', 'ff_com_eq_gr', 'ff_mkt_val_gr', 'ff_zscore', 'ff_dfd_tax_assets_lt', 'ff_ut_non_oper_inc_oth', 'ff_mkt_val_public', 'ff_xord_disc', 'ff_bps_gr', 'ff_ut_operation_exp', 'ff_sales_fix_assets', 'CPI', 'ff_bk_non_oper_inc', 'ff_capex_assets']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+-------------------+------------------+----------+----------+-----------------+----------+---------------+-------+-----------+--------------------------+-------------------+------------------+-----------------+----------------------+-----------------+-------------+---------+--------------------+----------------------+-----------------+------------+-----------------+-------------------+-------------------+------------+------------------+-----------------+\n",
      "| fsym_id|      date|     ff_assets_gr| ff_net_inc_per_emp|ff_debt_entrpr_val|ff_fcf_yld|ff_sga_oth| ff_gross_cf_debt|ff_dil_adj|   ff_shs_float|ff_xord|ff_inc_sund|ff_net_inc_basic_beft_xord|    ff_non_oper_exp|       ff_cf_ps_gr|        ff_emp_gr|ff_net_inc_bef_xord_gr|     ff_com_eq_gr|ff_mkt_val_gr|ff_zscore|ff_dfd_tax_assets_lt|ff_ut_non_oper_inc_oth|ff_mkt_val_public|ff_xord_disc|        ff_bps_gr|ff_ut_operation_exp|ff_sales_fix_assets|         CPI|ff_bk_non_oper_inc|  ff_capex_assets|\n",
      "+--------+----------+-----------------+-------------------+------------------+----------+----------+-----------------+----------+---------------+-------+-----------+--------------------------+-------------------+------------------+-----------------+----------------------+-----------------+-------------+---------+--------------------+----------------------+-----------------+------------+-----------------+-------------------+-------------------+------------+------------------+-----------------+\n",
      "|B00FG1-R|2012-12-31|             null|               null|              null|      null|      0.59|             null|       0.0|           null|    0.0|        0.0|                     -0.59|                0.0|              null|             null|                  null|             null|         null|     null|                 0.0|                  null|             null|         0.0|             null|                0.0|                0.0| -1.21082E-4|             -0.59| 78.2739737657597|\n",
      "|B00FG1-R|2013-12-31| 27.3337012353016|               null|              null|      null|     7.858|             null|       0.0|           null|    0.0|        0.0|                     -8.89|                0.0|              null|             null|     -1406.77966101695| 56.2754966887417|         null|     null|                 0.0|                  null|             null|         0.0| 56.2737770595563|                0.0|                0.0| 0.002644169|             -8.89| 38.8518469129217|\n",
      "|B00FG1-R|2014-12-31| 1213.34511268169|               null|          0.432116|  -95.4447|     6.666| 3.62235294117647|       0.0| 14.62325559744|    0.0|        0.0|                    29.964|               -1.1|              null|             null|                  null|-507.814354002102|         null|  1.15752|                 0.0|                   0.0| 312.206507005344|         0.0|-492.849756559955|             22.364|  0.359627598152425|-0.003084609|            14.392| 11.9849930575122|\n",
      "|B00FG1-R|2015-12-31| 7.34888349058638|               null|            0.4599|  -11.9743|    13.059| 14.5585231015534|       0.0|  14.7848629761|    0.0|        0.0|                    73.848| -0.829999999999998|  883.087919032337|             null|      146.455746896276|-54.3522898256756|      4.30427|     null|                 0.0|                   0.0| 315.804673169496|         0.0|-48.0518554671434|             25.255|   0.87369424206093|-0.001075553|            75.952|0.483800029321214|\n",
      "|B00FG1-R|2016-12-31| 76.9038689814662|               null|          0.425666|  -22.8867|     13.44| 17.8101193860148|       0.0| 21.88557444594|    0.0|        0.0|                     80.96|  -3.52200000000001|  12.2864859020326|             null|      9.63059256851911| 81.9351016561196|      3.40202|  1.64783|                 0.0|                   0.0| 398.317454916108|         0.0| 85.1141023124192|             44.162|  0.296733531435523| 0.002524522|            85.144| 2.17970138879606|\n",
      "|B00FG1-R|2017-12-31|-1.41419272051649|   2.34312820512821|          0.346485|  -4.98055|    15.751| 26.5156443321981|       0.0|34.416235934208|    0.0|        0.0|                    91.382| -0.532999999999987|  29.7047334404834|             null|       12.873023715415| 55.8523642001073|      15.5789|  1.97356|                 0.0|                   0.0| 721.020142821658|         0.0| 56.0309541286687|             66.483|  0.349934699583753| 0.002106889|           115.385| 10.7710663683818|\n",
      "|B00FG1-R|2018-12-31|  29.666192122568|  0.920121951219512|          0.395634|  -18.7801|    18.475| 20.1105262845228|       0.0|22.052102494977|    0.0|        0.0|                     75.45|             -2.896| -12.2888973454115|  110.25641025641|     -17.4345057013416|             null|      3.83772|  1.52302|                 0.0|                   0.0| 443.247260149038|         0.0|             null|              88.39|   0.29922470145094|  6.84894E-4|           103.733| 18.3714590742121|\n",
      "|B00FG1-R|2019-12-31| 1.74088438055823|   1.09008695652174|          0.410164| -0.559477|    20.673| 19.6100505670088|       0.0|25.060584527495|    0.0|        0.0|                   100.288|  -3.05200000000001| -14.3631120254975| 12.1951219512195|      32.9198144466534| 343.051690699047|      38.0276|  1.82694|                 0.0|                   0.0| 507.476836681774|         0.0| 223.385786122636|            118.614|  0.348310925643052| 0.003153571|               0.0| 3.26268599653444|\n",
      "|B00FG1-R|2020-12-31| -4.0544623752058|   1.62013186813187|          0.573539|   15.0117|    17.366| 28.0527714002317|       0.0|23.996368176612|    0.0|        0.0|                   147.432|   6.00800000000001|  17.5352245204647|-1.08695652173913|      47.0086151882578| 59.1284984250545|      -54.644|  1.57815|                 0.0|                   0.0|    219.566768816|         0.0| 58.5293317724408|             99.852|  0.365529962894531| 0.004685349|               0.0| 1.31840540216292|\n",
      "|B00FG1-R|2021-12-31|-3.45508338046515|   1.72232584269663|          0.479072|   14.6534|    18.735|  31.857507260531|       0.0|24.168267326764|    0.0|      2.795|                   153.287| -0.193000000000012|0.0248188939876068| -2.1978021978022|      3.97132237234794| 48.7265050802251|      23.9127|  1.86135|                 0.0|                 2.795| 273.101420792433|         0.0|    48.2278920064|            103.438|  0.359795700687542|  0.00780737|               0.0|0.956621402268065|\n",
      "|B01DPB-R|2001-12-31|  16.344051186523|             0.0484|         0.0488336|   3.49776|     835.4| 98.7631578947368|       0.0|  302.954677788|    0.0|      -30.8|                     484.0|              -60.8|   6.7608885020849| 8.08473843493299|      29.3080416777986| 30.6125897760879|     -8.66636|  9.69598|                43.8|                 -30.8| 15087.1429538424|         0.0| 32.0351352760257|              575.2|   2.49479406615682|  -5.6338E-4|             728.0| 5.81970066132962|\n",
      "|B01DPB-R|2002-12-31|   27.46954403063| 0.0556181818181818|         0.0419551|   9.56695|     993.2| 227.055630936228|       0.0|  303.337440984|    0.0|      -43.2|                     611.8|             -124.8|  53.1113380013017|             10.0|      26.4049586776859| 50.2005434079441|     -37.7576|  7.25408|                53.9|                 -43.2|  9357.9600543564|         0.0| 49.4895038356305|              682.6|   2.68801858612678| 0.001652893|             964.0| 4.13685762656327|\n",
      "|B01DPB-R|2003-12-31| 26.6779531429196| 0.0354583333333333|         0.0518372|   0.44542|    1198.9| 71.9919856585469|       0.0|  307.025718584|    0.0|      -33.3|                     425.5|             -539.8| -61.8559613870213| 9.09090909090909|     -30.4511278195489| 16.8619174778189|      96.6442|  8.54648|                 0.9|                 -33.3| 18482.9482587568|       -95.2| 15.9667116972571|              774.0|   2.57899874494492| 0.002702703|            1024.8| 5.42119330919124|\n",
      "|B01DPB-R|2004-12-31| 15.0198310053458|            0.04775|         0.0305096|   3.44323|    1191.0| 152.503033980583|       0.0|  318.301561832|    0.0|      -37.2|                     573.0|             -203.0|  171.576782054103|              0.0|      34.6650998824912| 37.9169277263848|      24.3109|  11.3929|                36.2|                 -37.2| 22949.5426080872|       -49.0| 32.8762100406184|              779.4|   2.36934499465173|         0.0|            1080.8| 4.01986506746627|\n",
      "|B01DPB-R|2005-12-31| 17.6799100449775| 0.0341230769230769|         0.0187992|   2.57323|    1344.7| 162.451253481894|       0.0|  331.069481466|    0.0|       38.6|                     443.6|             -204.0| -15.4480745627036| 8.33333333333333|     -22.5828970331588| 25.8918788915315|      -6.5128|  10.4988|                79.3|                  38.6| 21436.7489249235|       -29.7|  20.934403877681|              683.8|   1.94149168853893|         0.0|             631.8| 4.93836990795299|\n",
      "|B01HWF-R|2001-12-31| 55.1791059504622|-0.0846993865030675|              null|      null|    15.874|-3417.62114537445|       0.0|           null|    0.0|       0.49|                    -14.21|               0.49|  70.2828811540748|             null|      35.2195945945946| 189.748157745566|         null|     null|                 0.0|                  0.49|             null|         0.0| 84.4022303573138|             32.928|   5.30754942284729|  -5.6338E-4|           -14.296| 3.61200610479905|\n",
      "|B01HWF-R|2002-12-31| 35.5689333559437|-0.0235051546391753|         0.0012791|  0.436023|    20.524| 840.659340659341|       0.0|    6.927463815|    0.0|      0.398|                   -11.573|-0.0410000000000084|              null| 19.0184049079755|      66.9708822251195| 9.75377993907382|         null|  5.98403|                 0.0|                 0.398|     90.057029595|         0.0| 13.4152186938286|             73.441|    9.8628841607565| 0.001652893|            -4.519| 2.73781975107887|\n",
      "|B01HWF-R|2003-12-31| 52.8113077740947|-0.0364631901840491|        5.65612E-4|  -5.31887|    37.084|          -5300.0|       0.0|    4.954369125|    0.0|      0.576|                   -12.149|  0.575999999999993| -438.519975607204| 68.0412371134021|     -160.679824561404| 39.8334648977617|       77.092|  6.40421|                 0.0|                 0.576|   98.44331451375|         0.0| 20.6885378695828|            211.923|   14.8109465071592| 0.002702703|           -12.463| 7.03863627061761|\n",
      "|B01HWF-R|2004-12-31| 284.995702533459|-0.0117417840375587|          0.110449|   1.32405|    71.128|0.639515600949068|       0.0|    12.00540327|    0.0|      1.124|                     -5.19|   1.12399999999998|              null| 30.6748466257669|      57.9204172625557| 206.901700841316|      319.417|  5.76224|                 0.0|                 1.124|     828.37282563|         0.0| 154.099633182268|            424.921|   19.7254346785771|         0.0|            -6.126| 2.32124253183935|\n",
      "|B01HWF-R|2005-12-31|-13.3818276529245|-0.0401903225806452|          0.161971|  -9.29705|   116.218|-11.7010723227361|       0.0|    10.62432724|    0.0|      4.728|                   -25.103|   4.72799999999996| -122.407315732113| 45.5399061032864|     -398.160735705718|-46.7466119193981|     -60.1957|  3.47132|                 0.0|                 4.728|    299.074811806|         0.0|-45.4183276083244|            695.784|   9.53627314897202|         0.0|           -29.646| 13.7275898782804|\n",
      "+--------+----------+-----------------+-------------------+------------------+----------+----------+-----------------+----------+---------------+-------+-----------+--------------------------+-------------------+------------------+-----------------+----------------------+-----------------+-------------+---------+--------------------+----------------------+-----------------+------------+-----------------+-------------------+-------------------+------------+------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114553\n",
      "114553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  110777\n",
      "Number of positives:  579\n",
      "Number of rows after dropping nulls:  110777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 650:====================================================>(197 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positives after dropping nulls:  579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e32173bf-00ff-4a98-afb6-27204dcc2ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Boruta import BorutaPy\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def boruta_fs(train_df, test_df, model_name): #HOW DOES BORUTA ACC WORK?\n",
    "    train_df = train_df.toPandas()\n",
    "    test_df = test_df.toPandas()\n",
    "    X_train = train_df.drop(['fsym_id', 'date', 'label'], axis=1)\n",
    "    y_train = train_df['label']\n",
    "    X_test = test_df.drop(['fsym_id', 'date', 'label'], axis=1)\n",
    "    y_test = test_df['label']\n",
    "    \n",
    "    if model_name == 'rf':\n",
    "        model = RandomForestClassifier()\n",
    "    else:\n",
    "        model = GradientBoostingClassifier\n",
    "    feat_selector = BorutaPy(model, n_estimators='auto', verbose=2, random_state=1)\n",
    "    feat_selector.fit(X_train, y_train)\n",
    "    features = X_train.columns.tolist()\n",
    "    print(\"Number of features: \", len(features) )\n",
    "    feature_ranks = list(zip(features, feat_selector.ranking_, feat_selector.support_))\n",
    "    selected_features = []\n",
    "    for feat in feature_ranks:\n",
    "        print(f\"Feature: {feat[0]}, Rank: {feat[1]}, Keep: {feat[2]}\")\n",
    "        if feat[1] <= 5:\n",
    "            selected_features.append(feat[0])\n",
    "    print(\"Selected features: \", selected_features)\n",
    "    return selected_features\n",
    "\n",
    "\n",
    "# train_df, test_df = get_df(all_feats=True)\n",
    "# boruta_features = boruta_fs(train_df, test_df, 'rf')\n",
    "# current_features = get_feature_col_names()\n",
    "# for f in boruta_features:\n",
    "#     if f in current_features:\n",
    "#         print(f)\n",
    "# final_features = list(set(boruta_features + current_features))\n",
    "# write_features_file(final_features) #in the feature selection pipeline, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7029e19c-0f91-44a2-9235-eb105d12f241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def correlation_matrix(df):\n",
    "    df =df.toPandas()\n",
    "    print(\"Converted to Pandas\")\n",
    "    corr_df = df.drop(['date','fsym_id'], axis=1)\n",
    "    corr_mat = corr_df.corr()\n",
    "    mask = np.triu(np.ones_like(corr_mat))\n",
    "    plt.figure(figsize=(50, 40))\n",
    "    sns.heatmap(corr_mat, annot=True, cmap='coolwarm', fmt=\".2f\", mask=mask)\n",
    "    plt.title('Correlation Matrix Heatmap')\n",
    "    plt.tight_layout()\n",
    "    # plt.savefig('corr_matrix.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"Variable pairs with absolute correlation above 0.7:\")\n",
    "    for i in range(len(corr_mat.columns)):\n",
    "        for j in range(i+1, len(corr_mat.columns)):\n",
    "            if abs(corr_mat.iloc[i, j]) >= 0.7:\n",
    "                print(f\"{corr_mat.columns[i]} - {corr_mat.columns[j]}: {corr_mat.iloc[i, j]}\")\n",
    "                \n",
    "# correlation_matrix(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "848cd54e-1b88-40ad-a7b1-c2e68b008be4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_df \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_df\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_div_yld_secs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_earn_yld\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_roa_ptx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_net_inc_basic_aft_xord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_net_inc_dil\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_oper_inc_aft_unusual\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      2\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_net_inc_dil_aft_xord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_net_inc_dil_bef_unusual\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_ebit_bef_unusual\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_eps_dil_gr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGDP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_bk_oper_inc_tot\u001b[39m\u001b[38;5;124m'\u001b[39m )\n\u001b[1;32m      3\u001b[0m test_df \u001b[38;5;241m=\u001b[39m test_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_div_yld_secs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_earn_yld\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_roa_ptx\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_net_inc_basic_aft_xord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_net_inc_dil\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_oper_inc_aft_unusual\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      4\u001b[0m                         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_net_inc_dil_aft_xord\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_net_inc_dil_bef_unusual\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_ebit_bef_unusual\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_eps_dil_gr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGDP\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mff_bk_oper_inc_tot\u001b[39m\u001b[38;5;124m'\u001b[39m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# train_df = train_df.drop('ff_div_yld_secs', 'ff_earn_yld', 'ff_roa_ptx', 'ff_net_inc_basic_aft_xord', 'ff_net_inc_dil', 'ff_oper_inc_aft_unusual', \n",
    "#                         'ff_net_inc_dil_aft_xord', 'ff_net_inc_dil_bef_unusual', 'ff_ebit_bef_unusual', 'ff_eps_dil_gr', 'GDP', 'ff_bk_oper_inc_tot' )\n",
    "# test_df = test_df.drop('ff_div_yld_secs', 'ff_earn_yld', 'ff_roa_ptx', 'ff_net_inc_basic_aft_xord', 'ff_net_inc_dil', 'ff_oper_inc_aft_unusual', \n",
    "#                         'ff_net_inc_dil_aft_xord', 'ff_net_inc_dil_bef_unusual', 'ff_ebit_bef_unusual', 'ff_eps_dil_gr', 'GDP', 'ff_bk_oper_inc_tot' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21aec020-f103-4438-8592-c5cf06826d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.drop('ff_div_yld_secs', 'ff_earn_yld', 'ff_roa_ptx', 'ff_net_inc_basic_aft_xord', 'ff_net_inc_dil', 'ff_oper_inc_aft_unusual', \n",
    "                        'ff_net_inc_dil_aft_xord', 'ff_net_inc_dil_bef_unusual', 'ff_ebit_bef_unusual', 'ff_eps_dil_gr', 'GDP', 'ff_bk_oper_inc_tot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "134eb184-057f-4541-b038-04d7b86bf9a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ff_assets_gr',\n",
       " 'ff_net_inc_per_emp',\n",
       " 'ff_debt_entrpr_val',\n",
       " 'ff_fcf_yld',\n",
       " 'ff_sga_oth',\n",
       " 'ff_gross_cf_debt',\n",
       " 'ff_dil_adj',\n",
       " 'ff_shs_float',\n",
       " 'ff_xord',\n",
       " 'ff_inc_sund',\n",
       " 'ff_net_inc_basic_beft_xord',\n",
       " 'ff_non_oper_exp',\n",
       " 'ff_cf_ps_gr',\n",
       " 'ff_emp_gr',\n",
       " 'ff_net_inc_bef_xord_gr',\n",
       " 'ff_com_eq_gr',\n",
       " 'ff_mkt_val_gr',\n",
       " 'ff_zscore',\n",
       " 'ff_dfd_tax_assets_lt',\n",
       " 'ff_ut_non_oper_inc_oth',\n",
       " 'ff_mkt_val_public',\n",
       " 'ff_xord_disc',\n",
       " 'ff_bps_gr',\n",
       " 'ff_ut_operation_exp',\n",
       " 'ff_sales_fix_assets',\n",
       " 'CPI',\n",
       " 'ff_bk_non_oper_inc',\n",
       " 'ff_capex_assets']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats = df.columns[2:-1]\n",
    "# write_features_file(feats)\n",
    "feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b57d4b-57d7-4969-8ec1-5827dcbbd6c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to Pandas\n",
      " 60%|██████    | 3/5 [1:33:00<1:04:45, 1942.98s/trial, best loss: -0.18703048596261856]"
     ]
    }
   ],
   "source": [
    "# from pyspark.ml.feature import VectorAssembler\n",
    "# from pyspark.ml import Pipeline\n",
    "# from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.ml.regression import LinearRegression\n",
    "# from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier\n",
    "# from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "# from pyspark.ml.tuning import CrossValidator, ParamGridBuilder, CrossValidatorModel\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from hyperopt import fmin, tpe, hp\n",
    "from sklearn import tree\n",
    "import shap\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def confusion_matrix_pandas(acts, preds):\n",
    "    cm = confusion_matrix(acts, preds) #correct order\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "    plt.title(f'Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "def feature_importances(model, features):\n",
    "    feature_importances = model.feature_importances_\n",
    "\n",
    "    print(\"Feature Importances:\")\n",
    "    for feature, importance in zip(features, feature_importances):\n",
    "        print(f\"{feature}: {importance}\")\n",
    "\n",
    "    sorted_idx = np.argsort(feature_importances)[::-1]\n",
    "    sorted_features = [features[i] for i in sorted_idx]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(feature_importances)), feature_importances[sorted_idx], align=\"center\")\n",
    "    plt.xticks(range(len(feature_importances)), sorted_features, rotation=45, ha=\"right\")\n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.ylabel(\"Feature Importance\")\n",
    "    plt.title(\"Feature Importances\")\n",
    "    plt.show()\n",
    "\n",
    "        \n",
    "def model_testing(df, classifier):\n",
    "    df = df.toPandas()\n",
    "    print(\"Converted to Pandas\")\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.set_index('date', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    X_train = df.drop(exclude_columns, axis=1)\n",
    "    y_train = df['label']\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    if classifier == 'LogisticRegression':\n",
    "        param_space = {\n",
    "            'C': hp.uniform('C', 0.01, 1.0) }\n",
    "        classifier_instance = LogisticRegression(class_weight = class_weight_dict, solver='sag')\n",
    "        scaler = StandardScaler()\n",
    "        feats = X_train.columns\n",
    "        X_train[feats] = scaler.fit_transform(X_train[feats])\n",
    "        \n",
    "    elif classifier == 'RandomForest':\n",
    "        param_space = { \n",
    "            'n_estimators': hp.quniform('n_estimators', 100, 500, 1),\n",
    "            'max_depth': hp.quniform('max_depth', 5, 20, 1)\n",
    "        }\n",
    "        classifier_instance = RandomForestClassifier(class_weight = class_weight_dict)\n",
    "    elif classifier == 'GBT':\n",
    "        param_space = { 'n_estimators':hp.uniform('n_estimators',100,500),\n",
    "           'max_depth':hp.quniform('max_depth',5,20,1),\n",
    "           'min_samples_leaf':hp.quniform('min_samples_leaf',1,5,1),\n",
    "           'min_samples_split':hp.quniform('min_samples_split',2,6,1)}\n",
    "        classifier_instance = GradientBoostingClassifier()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported classifier\")\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    def set_params(classifier, params):\n",
    "        if classifier == 'RandomForest':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            # params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            # params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        elif classifier == 'GBT':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        else:\n",
    "            return params\n",
    "        \n",
    "    def objective(params):\n",
    "        params = set_params(classifier, params)\n",
    "        classifier_instance.set_params(**params)\n",
    "        scores = cross_val_score(classifier_instance, X_train, y_train, cv=tscv, scoring='f1')\n",
    "        score = -scores.mean()\n",
    "        return score\n",
    "    \n",
    "    \n",
    "#     initial_model = classifier_instance\n",
    "#     initial_model = initial_model.fit(X_train, y_train)\n",
    "#     initial_preds = pd.DataFrame()\n",
    "#     print(\"INITIAL: \")\n",
    "#     initial_preds['prediction'] = initial_model.predict(X_test)\n",
    "#     initial_preds['label'] = y_test\n",
    "#     confusion_matrix_pandas(initial_preds)\n",
    "    \n",
    "    best_params = fmin(fn=objective, space=param_space, algo=tpe.suggest, max_evals=5)\n",
    "    \n",
    "    best_params = set_params(classifier, best_params)\n",
    "    classifier_instance.set_params(**best_params)\n",
    "    i = 0\n",
    "    final_recall = None\n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        x_train, x_test = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "        Y_train, Y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        \n",
    "        classifier_instance.fit(x_train, Y_train)\n",
    "        \n",
    "        preds = classifier_instance.predict(x_test)\n",
    "        print(f\"Classification Report: \")\n",
    "        print(classification_report(Y_test, preds))\n",
    "        # cm = confusion_matrix(Y_test, preds, labels=classifier_instance.classes_)\n",
    "        # plt.figure(figsize=(8, 6))\n",
    "        # sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
    "        # plt.xlabel(\"Predicted\")\n",
    "        # plt.ylabel(\"Actual\")\n",
    "        # plt.title(\"Confusion Matrix\")\n",
    "        # plt.show()\n",
    "        final_recall = recall_score(Y_test, preds, pos_label=1)\n",
    "        # return recall_minority_class\n",
    "    # return final_recall\n",
    "        \n",
    "    if classifier != 'LogisticRegression':\n",
    "        feature_importances(classifier_instance, X_train.columns.tolist())\n",
    "        \n",
    "    return classifier_instance, X_train.columns.tolist(), X_train\n",
    "#     shapley(classifier_instance, X_train.columns.tolist(), X_train)\n",
    "        \n",
    "        \n",
    "\n",
    "model_testing(df, 'GBT')\n",
    "# basic_test(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5233a-92be-4a5f-b716-d2d925f8b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "        \n",
    "def model_testing(df, classifier):\n",
    "    print(\"Converted to Pandas\")\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df.set_index('date', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    X_train = df.drop(exclude_columns, axis=1)\n",
    "    y_train = df['label']\n",
    "    \n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    if classifier == 'LogisticRegression':\n",
    "        param_space = {\n",
    "            'C': hp.uniform('C', 0.01, 1.0) }\n",
    "        classifier_instance = LogisticRegression(class_weight = class_weight_dict, solver='sag')\n",
    "        scaler = StandardScaler()\n",
    "        feats = X_train.columns\n",
    "        X_train[feats] = scaler.fit_transform(X_train[feats])\n",
    "        \n",
    "    elif classifier == 'RandomForest':\n",
    "        param_space = { \n",
    "            'n_estimators': hp.quniform('n_estimators', 100, 500, 1),\n",
    "            'max_depth': hp.quniform('max_depth', 5, 20, 1)\n",
    "        }\n",
    "        classifier_instance = RandomForestClassifier(class_weight = class_weight_dict)\n",
    "    elif classifier == 'GBT':\n",
    "        param_space = { 'n_estimators':hp.uniform('n_estimators',100,500),\n",
    "           'max_depth':hp.quniform('max_depth',5,20,1),\n",
    "           'min_samples_leaf':hp.quniform('min_samples_leaf',1,5,1),\n",
    "           'min_samples_split':hp.quniform('min_samples_split',2,6,1)}\n",
    "        classifier_instance = GradientBoostingClassifier()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported classifier\")\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    def set_params(classifier, params):\n",
    "        if classifier == 'RandomForest':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            # params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            # params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        elif classifier == 'GBT':\n",
    "            params['max_depth'] = int(params['max_depth'])\n",
    "            params['n_estimators'] = int(params['n_estimators'])\n",
    "            params['min_samples_leaf'] = int(params['min_samples_leaf'])\n",
    "            params['min_samples_split'] = int(params['min_samples_split'])\n",
    "            return params\n",
    "        else:\n",
    "            return params\n",
    "        \n",
    "    def objective(params):\n",
    "        params = set_params(classifier, params)\n",
    "        classifier_instance.set_params(**params)\n",
    "        scores = cross_val_score(classifier_instance, X_train, y_train, cv=tscv, scoring='f1')\n",
    "        score = -scores.mean()\n",
    "        return score\n",
    "    \n",
    "    \n",
    "#     initial_model = classifier_instance\n",
    "#     initial_model = initial_model.fit(X_train, y_train)\n",
    "#     initial_preds = pd.DataFrame()\n",
    "#     print(\"INITIAL: \")\n",
    "#     initial_preds['prediction'] = initial_model.predict(X_test)\n",
    "#     initial_preds['label'] = y_test\n",
    "#     confusion_matrix_pandas(initial_preds)\n",
    "    \n",
    "    best_params = fmin(fn=objective, space=param_space, algo=tpe.suggest, max_evals=5)\n",
    "    \n",
    "    best_params = set_params(classifier, best_params)\n",
    "    classifier_instance.set_params(**best_params)\n",
    "    i = 0\n",
    "    final_recall = None\n",
    "    for train_index, test_index in tscv.split(X_train):\n",
    "        x_train, x_test = X_train.iloc[train_index, :], X_train.iloc[test_index, :]\n",
    "        Y_train, Y_test = y_train.iloc[train_index], y_train.iloc[test_index]\n",
    "        \n",
    "        classifier_instance.fit(x_train, Y_train)\n",
    "        \n",
    "        preds = classifier_instance.predict(x_test)\n",
    "        print(f\"Classification Report: \")\n",
    "        print(classification_report(Y_test, preds))\n",
    "        # cm = confusion_matrix(Y_test, preds, labels=classifier_instance.classes_)\n",
    "        # plt.figure(figsize=(8, 6))\n",
    "        # sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
    "        # plt.xlabel(\"Predicted\")\n",
    "        # plt.ylabel(\"Actual\")\n",
    "        # plt.title(\"Confusion Matrix\")\n",
    "        # plt.show()\n",
    "        final_recall = recall_score(Y_test, preds, pos_label=1)\n",
    "        # return recall_minority_class\n",
    "    # return final_recall\n",
    "        \n",
    "    if classifier != 'LogisticRegression':\n",
    "        feature_importances(classifier_instance, X_train.columns.tolist())\n",
    "        \n",
    "    return classifier_instance, X_train.columns.tolist(), X_train\n",
    "#     shapley(classifier_instance, X_train.columns.tolist(), X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3d3ef2b-4836-4841-ba91-52867fa18db8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def shapley(model, features, X_train, X_test):\n",
    "    explainer = shap.Explainer(model)\n",
    "    shap_values = explainer.shap_values(X_test)\n",
    "    shap.initjs()\n",
    "    print(shap_values.shape)\n",
    "    shap.summary_plot(shap_values, X_test)\n",
    "    # shap.plots.waterfall(shap_values)\n",
    "    # shap.plots.bar(shap_values)\n",
    "    # shap.summary_plot(shap_values[0], X_test)\n",
    "    # shap.summary_plot(shap_values[1], X_test)\n",
    "\n",
    "# shapley(model, feats, X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232afb58-57b9-4b0e-97a7-af4375254b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "def anomaly_det(df):\n",
    "    train_df, test_df = split_train_test(d, '2019-01-01')\n",
    "    \n",
    "    train_df = train_df.toPandas()\n",
    "    test_df = test_df.toPandas()\n",
    "    print(\"Converted to Pandas\")\n",
    "    exclude_columns = ['fsym_id', 'label']\n",
    "    train_df['date'] = pd.to_datetime(train_df['date'])\n",
    "    train_df.set_index('date', inplace=True)\n",
    "    train_df.sort_index(inplace=True)\n",
    "    test_df['date'] = pd.to_datetime(test_df['date'])\n",
    "    test_df.set_index('date', inplace=True)\n",
    "    test_df.sort_index(inplace=True)\n",
    "    \n",
    "    X_train = train_df.drop(exclude_columns, axis=1)\n",
    "    y_train = train_df['label']\n",
    "    \n",
    "    isol_for = IsolationForest(contamination=0.1, random_state=42)\n",
    "    isol_for.fit(X_train)\n",
    "    X_train['anomaly_scores'] = isol_for.decision_function(X_train)\n",
    "    X_train['anomaly'] = isol_for.predict(X_train)\n",
    "    print(X_train)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef026f88-0ea7-4451-ad30-09a06e3f808d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-21 00:44:45.452302: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-21 00:44:45.452341: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-21 00:44:45.453840: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-21 00:44:45.464049: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-21 00:44:46.572114: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "\n",
    "def prepare_seqs(df):\n",
    "    train_df = df.toPandas()\n",
    "    # val_split_date = pd.to_datetime('2017-01-01')\n",
    "    # val_df = train_df[train_df['date'] >= val_split_date]\n",
    "    # print(len(val_df)/len(train_df))\n",
    "    # print(len(val_df[val_df['label']==1]))\n",
    "    # train_df = train_df[train_df['date'] < val_split_date]\n",
    "    # test_df = test_df.toPandas()\n",
    "    feature_cols = train_df.columns[2:-1]\n",
    "    # nn_train_feats = train_df[feature_cols].values\n",
    "    # nn_train_labels = train_df['label'].values\n",
    "    # nn_test_feats = test_df[feature_cols].values\n",
    "    # nn_test_labels = test_df['label'].values\n",
    "    # nn_val_feats = val_df[feature_cols].values\n",
    "    # nn_val_labels = val_df['label'].values\n",
    "    \n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(16, activation=\"tanh\"),\n",
    "        keras.layers.Dense(8, activation=\"relu\"),\n",
    "        keras.layers.Dense(1, activation = \"sigmoid\")\n",
    "    ])\n",
    "    loss_fn = keras.losses.BinaryCrossentropy()\n",
    "    optimizer = keras.optimizers.Adam(\n",
    "        learning_rate=0.005\n",
    "    )\n",
    "    \n",
    "    def plot_model_performance(mdl, loss, metric):\n",
    "        x = pd.DataFrame(mdl.history).reset_index()\n",
    "        x = pd.melt(x, id_vars='index')\n",
    "        x['validation'] = (x['variable'].str[:4] == 'val_').replace({True:'validation',False:'training'})\n",
    "        x['loss'] = (x['variable'].str[-4:] == 'loss').replace({True:loss,False:metric})\n",
    "        g = sns.FacetGrid(x, col='loss', hue='validation',sharey=False)\n",
    "        g.map(sns.lineplot, 'index','value')\n",
    "        g.add_legend()\n",
    "        return g\n",
    "    \n",
    "    # space = {\n",
    "    # 'units1': hp.choice('units1', np.arange(8, 64, dtype=int)),\n",
    "    # 'units2': hp.choice('units2', np.arange(8, 64, dtype=int)),\n",
    "    # 'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.1))\n",
    "    # }\n",
    "#     def objective(params):\n",
    "#         model = keras.Sequential([\n",
    "#             keras.layers.Dense(params['units1'], activation=\"tanh\"),\n",
    "#             keras.layers.Dense(params['units2'], activation=\"relu\"),\n",
    "#             keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "#         ])\n",
    "#         model.compile(optimizer=keras.optimizers.Adam(learning_rate=params['learning_rate']),\n",
    "#                       loss='binary_crossentropy',\n",
    "#                       metrics=['accuracy'])\n",
    "\n",
    "#         # Train and evaluate the model using cross-validation\n",
    "#         scores = cross_val_score(model, nn_train_feats, nn_train_labels, cv=tscv, scoring='f1')\n",
    "#         mean_f1 = -np.mean(scores)  # Negate to maximize F1 score\n",
    "#         return mean_f1\n",
    "    \n",
    "#     best_params = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=5)\n",
    "#     best_units1 = np.arange(8, 64, dtype=int)[best_params['units1']]\n",
    "#     best_units2 = np.arange(8, 64, dtype=int)[best_params['units2']]\n",
    "#     best_learning_rate = best_params['learning_rate']\n",
    "#     final_model = keras.Sequential([\n",
    "#         keras.layers.Dense(best_units1, activation=\"tanh\"),\n",
    "#         keras.layers.Dense(best_units2, activation=\"relu\"),\n",
    "#         keras.layers.Dense(1, activation=\"sigmoid\")])\n",
    "    model.compile(optimizer=optimizer,\n",
    "                        loss=loss_fn,\n",
    "                        metrics=['accuracy'])\n",
    "\n",
    "    # final_model.fit(X_train, y_train, epochs=10, batch_size=32, class_weight=class_weight_dict)\n",
    "    \n",
    "    i=1\n",
    "    for train_idx, val_idx in tscv.split(train_df):\n",
    "        train_fold = train_df.iloc[train_idx, :]\n",
    "        val_fold = train_df.iloc[val_idx, :]\n",
    "        nn_train_feats = train_fold[feature_cols].values\n",
    "        nn_train_labels = train_fold['label'].values\n",
    "        nn_val_feats = val_fold[feature_cols].values\n",
    "        nn_val_labels = val_fold['label'].values\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(nn_train_labels), y=nn_train_labels)\n",
    "        class_weight_dict = dict(enumerate(class_weights))\n",
    "        fit_model = model.fit(nn_train_feats, nn_train_labels, epochs=50, batch_size=32, class_weight=class_weight_dict)\n",
    "\n",
    "        plot_model_performance(fit_model, 'bin_cross_entropy','accuracy')\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        test_loss, test_acc = model.evaluate(nn_val_feats, nn_val_labels)\n",
    "        print(f'Test accuracy: {test_acc}')\n",
    "\n",
    "        # Make predictions on new data\n",
    "        predictions = model.predict(nn_val_feats)\n",
    "        for i in range(len(predictions)):\n",
    "            predictions[i] = 1 if predictions[i] >= 0.5 else 0\n",
    "        print(f\"Classification Report for Fold {i}:\")\n",
    "        print(classification_report(nn_val_labels, predictions))\n",
    "        i+=1\n",
    "        print(np.sum(nn_val_labels==1))\n",
    "        print(np.sum(predictions==1))\n",
    "        cm = confusion_matrix(nn_val_labels, predictions)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
    "        plt.xlabel(\"Predicted\")\n",
    "        plt.ylabel(\"Actual\")\n",
    "        plt.title(\"Confusion Matrix\")\n",
    "        plt.show()\n",
    "\n",
    "# prepare_seqs(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f51b0e-3083-43d6-beaa-eea3f6c5718f",
   "metadata": {},
   "source": [
    "## Cycling through each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "25aab8b6-d366-4465-887f-73f713d427ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+------------+------------------+------------------+----------+----------+----------------+----------+------------+-------+-----------+--------------------------+---------------+-----------+---------+----------------------+------------+-------------+---------+--------------------+----------------------+-----------------+------------+---------+-------------------+-------------------+---+------------------+---------------+-----+----+\n",
      "|fsym_id|date|ff_assets_gr|ff_net_inc_per_emp|ff_debt_entrpr_val|ff_fcf_yld|ff_sga_oth|ff_gross_cf_debt|ff_dil_adj|ff_shs_float|ff_xord|ff_inc_sund|ff_net_inc_basic_beft_xord|ff_non_oper_exp|ff_cf_ps_gr|ff_emp_gr|ff_net_inc_bef_xord_gr|ff_com_eq_gr|ff_mkt_val_gr|ff_zscore|ff_dfd_tax_assets_lt|ff_ut_non_oper_inc_oth|ff_mkt_val_public|ff_xord_disc|ff_bps_gr|ff_ut_operation_exp|ff_sales_fix_assets|CPI|ff_bk_non_oper_inc|ff_capex_assets|label|year|\n",
      "+-------+----+------------+------------------+------------------+----------+----------+----------------+----------+------------+-------+-----------+--------------------------+---------------+-----------+---------+----------------------+------------+-------------+---------+--------------------+----------------------+-----------------+------------+---------+-------------------+-------------------+---+------------------+---------------+-----+----+\n",
      "+-------+----+------------+------------------+------------------+----------+----------+----------------+----------+------------+-------+-----------+--------------------------+---------------+-----------+---------+----------------------+------------+-------------+---------+--------------------+----------------------+-----------------+------------+---------+-------------------+-------------------+---+------------------+---------------+-----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023]\n",
      "YEAR: 2001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 837:===================================================> (193 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to Pandas\n",
      "  0%|          | 0/5 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  4.39trial/s, best loss: -0.2018120169481122]\n",
      "Classification Report for Fold 0:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.87      0.93       731\n",
      "           1       0.11      0.69      0.18        16\n",
      "\n",
      "    accuracy                           0.87       747\n",
      "   macro avg       0.55      0.78      0.56       747\n",
      "weighted avg       0.97      0.87      0.91       747\n",
      "\n",
      "Classification Report for Fold 0:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.86      0.92       732\n",
      "           1       0.10      0.73      0.17        15\n",
      "\n",
      "    accuracy                           0.86       747\n",
      "   macro avg       0.55      0.80      0.55       747\n",
      "weighted avg       0.98      0.86      0.91       747\n",
      "\n",
      "Classification Report for Fold 0:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.93       737\n",
      "           1       0.09      0.90      0.17        10\n",
      "\n",
      "    accuracy                           0.88       747\n",
      "   macro avg       0.54      0.89      0.55       747\n",
      "weighted avg       0.99      0.88      0.92       747\n",
      "\n",
      "Classification Report for Fold 0:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.90      0.94       729\n",
      "           1       0.16      0.78      0.26        18\n",
      "\n",
      "    accuracy                           0.89       747\n",
      "   macro avg       0.58      0.84      0.60       747\n",
      "weighted avg       0.97      0.89      0.93       747\n",
      "\n",
      "Classification Report for Fold 0:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.93       731\n",
      "           1       0.13      0.81      0.22        16\n",
      "\n",
      "    accuracy                           0.88       747\n",
      "   macro avg       0.56      0.85      0.58       747\n",
      "weighted avg       0.98      0.88      0.92       747\n",
      "\n",
      "YEAR: 2002\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 860:====================================================>(197 + 3) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted to Pandas\n",
      "  0%|          | 0/5 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:00<00:00,  8.87trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "\n",
      "\n",
      "1 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "\n",
      "\n",
      "1 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  9.89trial/s, best loss=?]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "\n",
      "\n",
      "1 fits failed out of a total of 5.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"/opt/jupyterhub/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1253, in fit\n",
      "    raise ValueError(\n",
      "ValueError: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AllTrialsFailed",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAllTrialsFailed\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(recalls)\n\u001b[1;32m     14\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mcycle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[32], line 11\u001b[0m, in \u001b[0;36mcycle\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      9\u001b[0m     temp_df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mfilter(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m==\u001b[39my)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYEAR: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m     recall \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_testing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemp_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLogisticRegression\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     recalls\u001b[38;5;241m.\u001b[39mappend(recall)\n\u001b[1;32m     13\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(recalls)\n",
      "Cell \u001b[0;32mIn[31], line 125\u001b[0m, in \u001b[0;36mmodel_testing\u001b[0;34m(df, classifier)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m score\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m#     initial_model = classifier_instance\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;66;03m#     initial_model = initial_model.fit(X_train, y_train)\u001b[39;00m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m#     initial_preds = pd.DataFrame()\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m#     initial_preds['label'] = y_test\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m#     confusion_matrix_pandas(initial_preds)\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m \u001b[43mfmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msuggest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_evals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m     best_params \u001b[38;5;241m=\u001b[39m set_params(classifier, best_params)\n\u001b[1;32m    128\u001b[0m     classifier_instance\u001b[38;5;241m.\u001b[39mset_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_params)\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/hyperopt/fmin.py:593\u001b[0m, in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials\u001b[38;5;241m.\u001b[39mtrials) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    590\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[1;32m    591\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere are no evaluation tasks, cannot return argmin of task losses.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    592\u001b[0m         )\n\u001b[0;32m--> 593\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrials\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmin\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(trials) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    595\u001b[0m     \u001b[38;5;66;03m# Only if there are some successful trail runs, return the best point in\u001b[39;00m\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# the evaluation space\u001b[39;00m\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m space_eval(space, trials\u001b[38;5;241m.\u001b[39margmin)\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/hyperopt/base.py:620\u001b[0m, in \u001b[0;36mTrials.argmin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    619\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21margmin\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 620\u001b[0m     best_trial \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_trial\u001b[49m\n\u001b[1;32m    621\u001b[0m     vals \u001b[38;5;241m=\u001b[39m best_trial[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmisc\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvals\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;66;03m# unpack the one-element lists to values\u001b[39;00m\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;66;03m# and skip over the 0-element lists\u001b[39;00m\n",
      "File \u001b[0;32m/opt/jupyterhub/lib/python3.10/site-packages/hyperopt/base.py:611\u001b[0m, in \u001b[0;36mTrials.best_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    605\u001b[0m candidates \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    606\u001b[0m     t\n\u001b[1;32m    607\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrials\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m STATUS_OK \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    609\u001b[0m ]\n\u001b[1;32m    610\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m candidates:\n\u001b[0;32m--> 611\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m AllTrialsFailed\n\u001b[1;32m    612\u001b[0m losses \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mfloat\u001b[39m(t[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresult\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m candidates]\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(losses) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mAllTrialsFailed\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def cycle(df):\n",
    "    df = df.withColumn('year', F.year('date'))\n",
    "    years = df.select(\"year\").distinct().rdd.map(lambda row: row[0]).collect()\n",
    "    years = sorted(years)\n",
    "    recalls = []\n",
    "    for y in years:\n",
    "        temp_df = df.filter(F.col('year')==y)\n",
    "        print(f\"YEAR: {y}\\n\")\n",
    "        recall = model_testing(temp_df, 'LogisticRegression')\n",
    "        recalls.append(recall)\n",
    "    plt.plot(recalls)\n",
    "    plt.show()\n",
    "    \n",
    "cycle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dd66cf-5f79-479b-b435-4ffcf3473d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a722ab-ed0c-4696-882e-04028e46d870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #backup extra code:\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.model_selection import cross_validate\n",
    "\n",
    "# def cross_val(train_df, test_df):\n",
    "#     feature_cols = train_df.columns[2:-1]\n",
    "#     print(\"Features: \",feature_cols)\n",
    "#     assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "#     lr = LogisticRegression(labelCol='label', featuresCol = 'features')\n",
    "#     rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "#     gb = GBTClassifier(labelCol='label', featuresCol = 'features')\n",
    "    \n",
    "#     classifiers = [lr, rf, gb]\n",
    "    \n",
    "#     cv_models = []\n",
    "    \n",
    "#     evaluator = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol=\"rawPrediction\", metricName='areaUnderROC')\n",
    "    \n",
    "#     for classifier in classifiers:\n",
    "#         print(\"Starting...\")\n",
    "#         pipeline = Pipeline(stages=[assembler, classifier])\n",
    "        \n",
    "#         paramGrid = ParamGridBuilder() \\\n",
    "#         .addGrid(classifier.regParam, [0.01, 0.1]) \\\n",
    "#         .addGrid(classifier.elasticNetParam, [0.0, 0.5]) \\\n",
    "#         .addGrid(classifier.maxIter, [10, 20]) \\\n",
    "#         .build()\n",
    "        \n",
    "#         cross_val = CrossValidator(\n",
    "#             estimator=pipeline,\n",
    "#             estimatorParamMaps=paramGrid,\n",
    "#             evaluator=evaluator,\n",
    "#             numFolds=5\n",
    "#         )\n",
    "        \n",
    "#         cv_model = cross_val.fit(train_df)\n",
    "#         cv_models.append(cv_model)\n",
    "#         print(\"Working\")\n",
    "    \n",
    "#     for cv_model in cv_models:\n",
    "#         predictions = cv_model.transform(test_df)\n",
    "#         auc = evaluator.evaluate(predictions)\n",
    "#         print(f\"Model AUC: {auc}\")\n",
    "        \n",
    "# def basic_test(train_df, test_df):\n",
    "#     feature_cols = train_df.columns[2:-1]\n",
    "#     print(\"Features: \",feature_cols)\n",
    "#     assembler = VectorAssembler(inputCols=feature_cols, outputCol='features')\n",
    "#     train_df = assembler.transform(train_df)\n",
    "#     test_df = assembler.transform(test_df)\n",
    "#     lr = LogisticRegression(labelCol='label', featuresCol = 'features')\n",
    "#     rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\")\n",
    "#     gb = GBTClassifier(labelCol='label', featuresCol = 'features')\n",
    "    \n",
    "#     evaluator = BinaryClassificationEvaluator(labelCol='label', rawPredictionCol=\"rawPrediction\", metricName='areaUnderROC')\n",
    "    \n",
    "#     lr = lr.fit(train_df)\n",
    "#     preds_train_df = lr.transform(train_df)\n",
    "#     preds_test_df = lr.transform(test_df)\n",
    "    \n",
    "    \n",
    "# def cross_val_pandas(train_df, test_df):\n",
    "#     train_df = train_df.toPandas()\n",
    "#     test_df = test_df.toPandas()\n",
    "#     print(\"Converted to Pandas\")\n",
    "#     exclude_columns = ['fsym_id', 'date', 'label']\n",
    "    \n",
    "#     X_train = train_df.drop(exclude_columns, axis=1)\n",
    "#     y_train = train_df['label']\n",
    "#     X_test = test_df.drop(exclude_columns, axis=1)\n",
    "#     y_test = test_df['label']\n",
    "#     models = {\n",
    "#         'Logistic Regression': LogisticRegression(),\n",
    "#         'Random Forest': RandomForestClassifier(),\n",
    "#         'SVM': SVC(probability=True),\n",
    "#         'Gradient Boosting': GradientBoostingClassifier()\n",
    "#     }\n",
    "\n",
    "#     param_grids = {\n",
    "#         'Logistic Regression': {\n",
    "#             'C': [0.01, 0.1, 1.0, 10.0],\n",
    "#             'max_iter': [100, 200, 300]\n",
    "#         },\n",
    "#         'Random Forest': {\n",
    "#             'n_estimators': [50, 100, 200],\n",
    "#             'max_depth': [None, 10, 20]\n",
    "#         },\n",
    "#         'SVM': {\n",
    "#             'C': [0.1, 1.0, 10.0],\n",
    "#             'kernel': ['linear', 'rbf']\n",
    "#         },\n",
    "#         'Gradient Boosting': {\n",
    "#             'n_estimators': [50, 100, 200],\n",
    "#             'learning_rate': [0.01, 0.1, 0.2],\n",
    "#             'max_depth': [3, 5, 7]\n",
    "#         }\n",
    "#     }\n",
    "#     tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "#     for model_name, model in models.items():\n",
    "#         param_grid = param_grids[model_name]\n",
    "\n",
    "#         grid_search = GridSearchCV(model, param_grid, cv=tscv, scoring='roc_auc')\n",
    "\n",
    "#         grid_search.fit(X_train, y_train)\n",
    "        \n",
    "#         best_model = grid_search.best_estimator_\n",
    "\n",
    "#         predictions = best_model.predict(X_test)\n",
    "\n",
    "#         auc = roc_auc_score(y_test, predictions)\n",
    "\n",
    "#         print(f\"\\nResults for {model_name}:\")\n",
    "#         print(f\"Area under the ROC curve (AUC): {auc}\")\n",
    "#         print(\"Best model hyperparameters:\")\n",
    "#         print(grid_search.best_params_)\n",
    "        \n",
    "#         # Confusion matrix\n",
    "#         cm = confusion_matrix(y_test, predictions)\n",
    "#         plt.figure(figsize=(8, 6))\n",
    "#         sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
    "#         plt.title(f'Confusion Matrix - {model_name}')\n",
    "#         plt.xlabel('Predicted')\n",
    "#         plt.ylabel('Actual')\n",
    "#         plt.show()\n",
    "        \n",
    "\n",
    "# #model_testing(train_df, test_df)\n",
    "# #cross_val_pandas(train_df, test_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
