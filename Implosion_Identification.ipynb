{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1921cb03-1482-48a2-bbce-358a7fb395c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"Identification\").config(\"hive.metastore.uris\", \"thrift://bialobog:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "\n",
    "spark.sql(\"USE 2023_04_01\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a125b38f-a981-4af4-bddf-f1f8d8b2c801",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import norm\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, to_date, lit, udf, pandas_udf, PandasUDFType, coalesce, \\\n",
    "            month, year, concat, date_format, format_string, last_day, months_between, greatest, least, when, lag, count, desc, expr, log1p\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import timedelta\n",
    "from pyspark.sql.types import*\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d142faab-c022-45ef-87f3-703330240404",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_stocks_prices():\n",
    "    query = \"\"\"SELECT s.ticker_region, p.p_date, p.p_price, splits.p_split_date, splits.p_split_factor\n",
    "                FROM sym_ticker_region s \n",
    "                INNER JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                INNER JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                INNER JOIN fp_basic_prices p ON p.fsym_id = sc.fsym_regional_id\n",
    "                LEFT JOIN fp_basic_splits AS splits ON splits.p_split_date = p.p_date AND p.fsym_id = splits.fsym_id \n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\"\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\")\n",
    "                AND p.p_date >= '2000-01-01'\n",
    "                ORDER BY s.ticker_region, p.p_date\n",
    "                \"\"\"\n",
    "    adj = spark.sql(query)\n",
    "    \n",
    "    window_spec = Window.partitionBy('ticker_region').orderBy(col('p_date').desc())\n",
    "    adj = adj.withColumn(\"cum_split_factor\", lit(0.0))  # placeholders\n",
    "    adj = adj.withColumn(\"cum_split_factor\", when(col('p_date') == col('p_split_date'), col('p_split_factor')).otherwise(lit(1.0)))\n",
    "    adj = adj.withColumn(\"cum_split_factor\", lit(1.0) * lag(\"cum_split_factor\").over(window_spec)).fillna(1.0)\n",
    "\n",
    "    # Calculate split-adjusted price\n",
    "    adj = adj.withColumn(\"split_adj_price\", col(\"p_price\") * col(\"cum_split_factor\"))\n",
    "\n",
    "    # Sort the DataFrame\n",
    "    adj = adj.sort(col('ticker_region').asc(), col('p_date').asc())\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    # columns_to_drop = [\"fsym_id\", \"p_split_date\", 'p_split_factor']\n",
    "    # adj = adj.drop(*columns_to_drop)\n",
    "    \n",
    "    adj.filter(col('ticker_region')=='AAPL-US').show()\n",
    "    \n",
    "\n",
    "    \n",
    "#get_all_stocks_prices()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5ce84b2-54a1-4d7a-b155-bf5fff593fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_price_weekly(ticker):\n",
    "    # Suppress the progress message from yfinance\n",
    "    temp_df = yf.download(ticker, start='2000-01-01', end='2023-03-31', progress=False)\n",
    "    if temp_df.empty:\n",
    "        print(\"No data available for the specified date range.\")\n",
    "        return None\n",
    "    print(temp_df['Adj Close'].pct_change())\n",
    "\n",
    "#get_stock_price_weekly('AAPL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e2c4e93-ff74-4403-b4f1-eb9625eae815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj = spark.sql(\"\"\"SELECT s.fsym_id, str.ticker_region, sc.proper_name, p.p_date, p.p_price,\n",
    "#                 splits.p_split_date, splits.p_split_factor\n",
    "\n",
    "# FROM sym_entity_sector AS e\n",
    "\n",
    "# INNER JOIN fp_sec_entity AS s ON e.factset_entity_id = s.factset_entity_id\n",
    "# INNER JOIN sym_coverage AS sc ON s.fsym_id = sc.fsym_security_id \n",
    "# INNER JOIN fp_basic_prices AS p ON sc.fsym_regional_id = p.fsym_id\n",
    "# INNER JOIN factset_sector_map AS sm ON e.sector_code = sm.factset_sector_code\n",
    "# INNER JOIN ff_basic_qf AS ff ON p.fsym_id = ff.fsym_id\n",
    "# INNER JOIN ff_advanced_der_af adf ON p.fsym_id = adf.fsym_id\n",
    "# LEFT JOIN sym_ticker_region str ON str.fsym_id = sc.fsym_regional_id\n",
    "# LEFT JOIN fp_basic_dividends AS divs ON divs.p_divs_exdate = p.p_date AND p.fsym_id = divs.fsym_id\n",
    "# LEFT JOIN fp_basic_splits AS splits ON splits.p_split_date = p.p_date AND p.fsym_id = splits.fsym_id \n",
    "\n",
    "# WHERE sc.fsym_security_id IS NOT NULL\n",
    "#         AND sc.fref_security_type IS NOT NULL AND sc.fref_security_type = 'SHARE'\n",
    "#         AND sc.currency IS NOT NULL AND sc.currency = 'USD'\n",
    "#         AND s.factset_entity_id IS NOT NULL\n",
    "#         AND p.p_date IS NOT NULL\n",
    "#         AND (sc.fref_listing_exchange = 'NAS' OR sc.fref_listing_exchange = 'NYS')\n",
    "#         AND p.p_date >= '2000-01-01'\n",
    "#         AND str.ticker_region = 'AAPL-US'\n",
    "# ORDER BY s.fsym_id ASC, p_date DESC\n",
    "#             \"\"\")\n",
    "\n",
    "\n",
    "# adj = adj.withColumn(\"temp_cum_split_factor\", when(adj.p_date==adj.p_split_date, lit(adj.p_split_factor)).otherwise(lit(1.0)))\n",
    "# adj = adj.withColumn(\"div_split_factor\", lit(0.0)) # placeholders\n",
    "# adj = adj.withColumn(\"cum_split_factor\", lit(0.0)) # placeholders\n",
    "# adj = adj.withColumn(\"split_temp_i\", lit(0)) # placeholders - for ordering purposes\n",
    "\n",
    "# # creating udf to calculate cumulative split factor\n",
    "# @pandas_udf(adj.schema, FloatType(), PandasUDFType.GROUPED_MAP)\n",
    "# def calc_product_factor(df1):\n",
    "#     \"\"\"\n",
    "#     Calculates the cumulative split factor for each company based on unique fsym_id's,\n",
    "#     for both the price split and the dividend split.\n",
    "#     The data MUST be sorted within the function itself (no orderBy in the function call),\n",
    "#     and spin_temp_i must be set to i during each iteration of the loop to guarantee\n",
    "#     proper sorting - without these safeguards, the function is applied non-sequentially\n",
    "#     to the data.\n",
    "#     \"\"\"\n",
    "#     df1 = df1.sort_values(by='p_date', ascending=False)\n",
    "#     for i in range(0, len(df1)):\n",
    "#         df1.loc[i, 'split_temp_i'] = i\n",
    "#         if i == 0:\n",
    "#             df1.loc[i, 'cum_split_factor'] = 1.0\n",
    "#             df1.loc[i, 'div_split_factor'] = 1.0\n",
    "#             continue\n",
    "#         df1.loc[i-1, 'div_split_factor'] = df1.loc[i-1, 'cum_split_factor'] * df1.loc[i-1, 'temp_cum_split_factor']\n",
    "#         df1.loc[i, 'cum_split_factor'] = df1.loc[i-1, 'cum_split_factor'] * df1.loc[i-1, 'temp_cum_split_factor']\n",
    "#     return df1\n",
    "\n",
    "# adj = adj.groupBy('fsym_id').apply(calc_product_factor)\n",
    "# adj = adj.withColumn(\"split_adj_price\", (adj.p_price*adj.cum_split_factor))\n",
    "\n",
    "# adj.filter(col('ticker_region')=='AAPL-US').orderBy('fsym_id', 'p_date').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "225740ab-45e8-4a9f-8f7c-7e3493970cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+-------+-------------+---------------+----+------------+--------------------+------------------+------------------+------------------+------------------+\n",
      "|ticker_region| fsym_id|    p_date|p_price|p_com_shs_out|split_adj_price|year|week_of_year|Implosion_Start_Date|Implosion_End_Date|      Market_Value|      rolling_mean|       mv_per_year|\n",
      "+-------------+--------+----------+-------+-------------+---------------+----+------------+--------------------+------------------+------------------+------------------+------------------+\n",
      "|      PBFX-US|B00FG1-R|2014-05-09|  27.68|    15886.553|          27.68|2014|          19|                null|              null|439739.78703999997|             27.68|439739.78703999997|\n",
      "|      PBFX-US|B00FG1-R|2014-05-16|  26.92|      15887.0|          26.92|2014|          20|                null|              null|427678.04000000004|              27.3|      433708.91352|\n",
      "|      PBFX-US|B00FG1-R|2014-05-23|  26.74|      15887.0|          26.74|2014|          21|                null|              null|424818.37999999995|27.113333333333333|430745.40234666667|\n",
      "|      PBFX-US|B00FG1-R|2014-05-30|  26.75|      15887.0|          26.75|2014|          22|                null|              null|         424977.25|           27.0225|      429303.36426|\n",
      "|      PBFX-US|B00FG1-R|2014-06-06|  28.81|      15887.0|          28.81|2014|          23|                null|              null|         457704.47|27.380000000000003|     434983.585408|\n",
      "|      PBFX-US|B00FG1-R|2014-06-13|  27.19|      15887.0|          27.19|2014|          24|                null|              null|         431967.53|27.348333333333333|434480.90950666665|\n",
      "|      PBFX-US|B00FG1-R|2014-06-20|  27.63|      15887.0|          27.63|2014|          25|                null|              null|         438957.81|27.388571428571428|435120.46671999997|\n",
      "|      PBFX-US|B00FG1-R|2014-06-27|  26.77|      15887.0|          26.77|2014|          26|                null|              null|         425294.99|          27.31125|433892.28212999995|\n",
      "|      PBFX-US|B00FG1-R|2014-07-04|  26.95|      15887.0|          26.95|2014|          27|                null|              null|428154.64999999997| 27.27111111111111|433254.76744888886|\n",
      "|      PBFX-US|B00FG1-R|2014-07-11|26.7505|      15887.0|        26.7505|2014|          28|                null|              null|       424985.1935|          27.21905| 432427.8100539999|\n",
      "|      PBFX-US|B00FG1-R|2014-07-18|  27.38|      15887.0|          27.38|2014|          29|                null|              null|         434986.06|27.233681818181818|432660.37823090894|\n",
      "|      PBFX-US|B00FG1-R|2014-07-25|   26.9|      15887.0|           26.9|2014|          30|                null|              null|          427360.3|27.205874999999995| 432218.7050449999|\n",
      "|      PBFX-US|B00FG1-R|2014-08-01|  24.19|      15887.0|          24.19|2014|          31|                null|              null|         384306.53|26.973884615384613| 428533.1531184614|\n",
      "|      PBFX-US|B00FG1-R|2014-08-08|   24.0|      15887.0|           24.0|2014|          32|                null|              null|          381288.0|26.761464285714283| 425158.4993242856|\n",
      "|      PBFX-US|B00FG1-R|2014-08-15|  25.41|      15887.0|          25.41|2014|          33|                null|              null|         403688.67|26.671366666666664| 423727.1773693332|\n",
      "|      PBFX-US|B00FG1-R|2014-08-22|  25.82|      15887.0|          25.82|2014|          34|                null|              null|         410202.34|       26.61815625| 422881.8750337499|\n",
      "|      PBFX-US|B00FG1-R|2014-08-29|  24.96|      15887.0|          24.96|2014|          35|                null|              null|         396539.52| 26.52061764705882|  421332.324737647|\n",
      "|      PBFX-US|B00FG1-R|2014-09-05|  25.12|      15887.0|          25.12|2014|          36|                null|              null|         399081.44| 26.44280555555555| 420096.1644744444|\n",
      "|      PBFX-US|B00FG1-R|2014-09-12|  24.95|      15887.0|          24.95|2014|          37|                null|              null|396380.64999999997| 26.36423684210526|418847.97950210527|\n",
      "|      PBFX-US|B00FG1-R|2014-09-19|  25.75|      15887.0|          25.75|2014|          38|                null|              null|         409090.25|26.333524999999998|418360.09302699997|\n",
      "+-------------+--------+----------+-------+-------------+---------------+----+------------+--------------------+------------------+------------------+------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/group_ops.py:103: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+----------+-------+-------------+---------------+----+------------+--------------------+------------------+------------+------------+-----------+\n",
      "|ticker_region| fsym_id|    p_date|p_price|p_com_shs_out|split_adj_price|year|week_of_year|Implosion_Start_Date|Implosion_End_Date|Market_Value|rolling_mean|mv_per_year|\n",
      "+-------------+--------+----------+-------+-------------+---------------+----+------------+--------------------+------------------+------------+------------+-----------+\n",
      "|      BSFC-US|B06303-R|2020-02-21|  501.0|       6074.0|          501.0|2020|           8|                null|              null|   3043074.0|       501.0|  3043074.0|\n",
      "|      BSFC-US|B06303-R|2020-02-28|  501.0|      17558.0|          501.0|2020|           9|                null|              null|   8796558.0|       501.0|  5919816.0|\n",
      "|      BSFC-US|B06303-R|2020-03-06|  501.0|      17558.0|          501.0|2020|          10|                null|              null|   8796558.0|       501.0|  6878730.0|\n",
      "|      BSFC-US|B06303-R|2020-03-13|  501.0|      17558.0|          501.0|2020|          11|                null|              null|   8796558.0|       501.0|  7358187.0|\n",
      "|      BSFC-US|B06303-R|2020-03-20|  501.0|      17558.0|          501.0|2020|          12|                null|              null|   8796558.0|       501.0|  7645861.2|\n",
      "+-------------+--------+----------+-------+-------------+---------------+----+------------+--------------------+------------------+------------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_all_stocks_df():\n",
    "    query = f\"\"\"SELECT s.ticker_region, s.fsym_id,p.p_date, p.p_price, splits.p_split_date, splits.p_split_factor, ms.p_com_shs_out \n",
    "                FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                INNER JOIN fp_basic_prices p ON p.fsym_id = sc.fsym_regional_id\n",
    "                LEFT JOIN fp_basic_splits AS splits ON splits.p_split_date = p.p_date AND p.fsym_id = splits.fsym_id\n",
    "                LEFT JOIN (SELECT sf.fsym_id, mp.price_date, sf.p_com_shs_out, sf.p_date AS shares_hist_date\n",
    "                                        FROM fp_basic_shares_hist AS sf\n",
    "                                        JOIN (SELECT p2.fsym_id, p2.p_date AS price_date, max(h.p_date) AS max_shares_hist_date\n",
    "                                                FROM fp_basic_prices AS p2\n",
    "                                                JOIN fp_basic_shares_hist AS h ON h.p_date <= p2.p_date AND p2.fsym_id = h.fsym_id\n",
    "                                                GROUP BY p2.fsym_id, p2.p_date)\n",
    "                                        mp ON mp.fsym_id = sf.fsym_id AND mp.max_shares_hist_date = sf.p_date)\n",
    "                            ms ON ms.fsym_id = p.fsym_id AND ms.price_date = p.p_date\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\" \n",
    "                AND ms.p_com_shs_out IS NOT NULL\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\") AND p.p_date >= '2000-01-01'\n",
    "                ORDER BY s.fsym_id, p.p_date\"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    adj = spark.sql(query)\n",
    "    #adj.show()\n",
    "    adj = adj.withColumn(\"temp_cum_split_factor\", when(adj.p_date==adj.p_split_date, lit(adj.p_split_factor)).otherwise(lit(1.0)))\n",
    "    #adj = adj.withColumn(\"div_split_factor\", lit(0.0)) # placeholders\n",
    "    adj = adj.withColumn(\"cum_split_factor\", lit(0.0)) # placeholders\n",
    "    #adj = adj.withColumn(\"split_temp_i\", lit(0)) # placeholders - for ordering purposes\n",
    "\n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy(F.desc('p_date'))\n",
    "\n",
    "    # Calculate cumulative split factor and dividend split factor\n",
    "    #adj = adj.withColumn('split_temp_i', F.row_number().over(window_spec) - 1)\n",
    "    adj = adj.withColumn('cum_split_factor_no_lag', \n",
    "                        F.exp(F.sum(F.log('temp_cum_split_factor')).over(window_spec)))\n",
    "\n",
    "    # Apply lag to the calculated cumulative split factor\n",
    "    adj = adj.withColumn('cum_split_factor', \n",
    "                        F.when(F.row_number().over(window_spec) == 1, 1.0)\n",
    "                        .otherwise(F.lag('cum_split_factor_no_lag', default=1.0).over(window_spec)))\n",
    "\n",
    "    adj = adj.withColumn('split_adj_price', adj.p_price * adj.cum_split_factor)\n",
    "    \n",
    "    adj = adj.withColumn('year', F.year('p_date'))\n",
    "    \n",
    "    adj =adj.withColumn('week_of_year', F.weekofyear('p_date'))\n",
    "\n",
    "    window_spec = Window.partitionBy('fsym_id', 'year', 'week_of_year').orderBy(col('p_date').desc())\n",
    "\n",
    "    adj = adj.withColumn('row_num', F.row_number().over(window_spec))\n",
    "\n",
    "    adj = adj.filter(col('row_num') == 1)\n",
    "    \n",
    "    \n",
    "    # adj_pd = adj.toPandas()\n",
    "    # adj_pd['p_date'] = pd.to_datetime(adj_pd['p_date'])\n",
    "\n",
    "    # plt.figure(figsize=(15, 5))\n",
    "    # plt.plot(adj_pd['p_date'], adj_pd['split_adj_price'], label='AAPL adjusted')\n",
    "    # plt.plot(adj_pd['p_date'], adj_pd['p_price'], label='AAPL unadjusted')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "    adj = adj.drop('temp_cum_split_factor', 'cum_split_factor', 'cum_split_factor_no_lag', 'row_num', 'p_split_date', 'p_split_factor')\n",
    "    adj = adj.withColumn('Implosion_Start_Date', F.lit(None).cast(DateType()))\n",
    "    adj = adj.withColumn('Implosion_End_Date', F.lit(None).cast(DateType()))\n",
    "    adj = adj.withColumn('Market_Value', col('split_adj_price') * col('p_com_shs_out'))\n",
    "    \n",
    "    window_spec2 = Window().partitionBy('fsym_id').orderBy('p_date').rowsBetween(-52, Window.currentRow)\n",
    "    adj = adj.withColumn('rolling_mean', F.avg('split_adj_price').over(window_spec2))\n",
    "    window_spec3 = Window().partitionBy('fsym_id', 'year').orderBy('p_date')\n",
    "    adj = adj.withColumn('mv_per_year', F.avg('Market_Value').over(window_spec3))\n",
    "    \n",
    "    \n",
    "    adj.orderBy('fsym_id', 'p_date').show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    @pandas_udf(adj.schema, FloatType(), PandasUDFType.GROUPED_MAP)\n",
    "    def find_implosion(df1):\n",
    "        fsym_id = df1.loc[0,'fsym_id']\n",
    "        df1 = df1.sort_values(by='p_date')\n",
    "        i = 52\n",
    "        price_drop_thresh = -0.6\n",
    "        period_thresh = 104\n",
    "        imp_dates = []\n",
    "        while i < len(df1):\n",
    "            current_date = df1.loc[i, 'p_date']\n",
    "            current_price = df1.loc[i, 'split_adj_price']\n",
    "            mean_price = df1.loc[i, 'rolling_mean']\n",
    "            if (current_price - mean_price)/mean_price < price_drop_thresh:\n",
    "                j = i\n",
    "                start_price=current_price\n",
    "                j+=1\n",
    "                imp_period=0\n",
    "                while j < len(df1) and (df1.loc[j, 'split_adj_price'] - start_price) / start_price <= 0.2:\n",
    "                    imp_period+=1\n",
    "                    j+=1\n",
    "                if imp_period > period_thresh:\n",
    "                    imp_dates.append((current_date, df1.loc[i+imp_period, 'p_date']))\n",
    "                i+=imp_period\n",
    "            i+=1\n",
    "        if len(imp_dates)>0:\n",
    "            df1.loc[df1['fsym_id']==fsym_id, 'Implosion_Start_Date'] = imp_dates[0][0]\n",
    "            df1.loc[df1['fsym_id']==fsym_id, 'Implosion_End_Date'] = imp_dates[0][1]\n",
    "        \n",
    "        return df1\n",
    "    \n",
    "    @pandas_udf(adj.schema, FloatType(), PandasUDFType.GROUPED_MAP)\n",
    "    def find_implosion_mv(df1):\n",
    "        fsym_id = df1.loc[0,'fsym_id']\n",
    "        df1 = df1.sort_values(by='p_date')\n",
    "        i = 52\n",
    "        mv_drop_thresh = -0.6\n",
    "        period_thresh = 104\n",
    "        imp_dates = []\n",
    "        while i < len(df1):\n",
    "            current_date = df1.loc[i, 'p_date']\n",
    "            current_mv = df1.loc[i, 'Market_Value']\n",
    "            prev_year_mv = df1.loc[i, 'mv_per_year']\n",
    "            if (current_mv - prev_year_mv)/prev_year_mv < mv_drop_thresh:\n",
    "                j = i\n",
    "                start_mv = current_mv\n",
    "                j+=1\n",
    "                imp_period=0\n",
    "                while j < len(df1) and (df1.loc[j, 'Market_Value'] - start_mv) / start_mv <= 0.2:\n",
    "                    imp_period+=1\n",
    "                    j+=1\n",
    "                if imp_period > period_thresh:\n",
    "                    imp_dates.append((current_date, df1.loc[i+imp_period, 'p_date']))\n",
    "                i+=imp_period\n",
    "            i+=1\n",
    "        if len(imp_dates)>0:\n",
    "            df1.loc[df1['fsym_id']==fsym_id, 'Implosion_Start_Date'] = imp_dates[0][0]\n",
    "            df1.loc[df1['fsym_id']==fsym_id, 'Implosion_End_Date'] = imp_dates[0][1]\n",
    "        \n",
    "        return df1\n",
    "\n",
    "    \n",
    "\n",
    "    # adj = adj.filter(col('ticker_region')=='AAPL-US').orderBy('fsym_id', 'p_date')\n",
    "    # find_implosion2(ps.DataFrame(adj))\n",
    "    # adj = adj.groupBy('fsym_id').apply(find_implosion)\n",
    "    # result_df = adj.groupBy('ticker_region').agg(F.first('Implosion_Start_Date').alias('Implosion_Start_Date'), F.first('Implosion_End_Date').alias('Implosion_End_Date'))\n",
    "    # result_df.show()\n",
    "    # result_df.toPandas().to_csv('imploded_stocks3.csv')\n",
    "    \n",
    "    adj = adj.groupBy('fsym_id').apply(find_implosion_mv)\n",
    "    adj.show(5)\n",
    "    result_df = adj.groupBy('ticker_region').agg(F.first('Implosion_Start_Date').alias('Implosion_Start_Date'), F.first('Implosion_End_Date').alias('Implosion_End_Date'))\n",
    "    result_df.toPandas().to_csv('imploded_stocks_mv.csv')\n",
    "    \n",
    "    \n",
    "\n",
    "    #adj.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "get_all_stocks_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ce9b515-98a7-469f-a097-03d81cf7b875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prices(imp_df, pic_name):\n",
    "    imp_df.createOrReplaceTempView(\"temp_table\")\n",
    "    \n",
    "    query2 = f\"\"\"SELECT t.ticker_region, s.fsym_id,p.p_date, p.p_price, splits.p_split_date, splits.p_split_factor, \n",
    "                divs.p_divs_exdate, divs.p_divs_s_pd, divs.p_divs_pd, ms.p_com_shs_out \n",
    "                FROM temp_table t\n",
    "                LEFT JOIN sym_ticker_region s ON s.ticker_region = t.ticker_region\n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                INNER JOIN fp_basic_prices p ON p.fsym_id = sc.fsym_regional_id\n",
    "                LEFT JOIN fp_basic_dividends divs ON divs.p_divs_exdate = p.p_date AND p.fsym_id = divs.fsym_id\n",
    "                LEFT JOIN fp_basic_splits AS splits ON splits.p_split_date = p.p_date AND p.fsym_id = splits.fsym_id\n",
    "                LEFT JOIN (SELECT sf.fsym_id, mp.price_date, sf.p_com_shs_out, sf.p_date AS shares_hist_date\n",
    "                                        FROM fp_basic_shares_hist AS sf\n",
    "                                        JOIN (SELECT p2.fsym_id, p2.p_date AS price_date, max(h.p_date) AS max_shares_hist_date\n",
    "                                                FROM fp_basic_prices AS p2\n",
    "                                                JOIN fp_basic_shares_hist AS h ON h.p_date <= p2.p_date AND p2.fsym_id = h.fsym_id\n",
    "                                                GROUP BY p2.fsym_id, p2.p_date)\n",
    "                                        mp ON mp.fsym_id = sf.fsym_id AND mp.max_shares_hist_date = sf.p_date)\n",
    "                            ms ON ms.fsym_id = p.fsym_id AND ms.price_date = p.p_date\n",
    "                WHERE p.p_date >= '2000-01-01'\n",
    "                ORDER BY t.ticker_region, p.p_date\n",
    "                \"\"\"\n",
    "    \n",
    "    adj = spark.sql(query2)\n",
    "    imp_df = imp_df.toPandas()\n",
    "    adj.show()\n",
    "    adj = adj.withColumn(\"temp_cum_split_factor\", when(adj.p_date==adj.p_split_date, lit(adj.p_split_factor)).otherwise(lit(1.0)))\n",
    "    adj = adj.withColumn(\"cum_split_factor\", lit(0.0))\n",
    "\n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy(F.desc('p_date'))\n",
    "\n",
    "    adj = adj.withColumn('cum_split_factor_no_lag', \n",
    "                        F.exp(F.sum(F.log('temp_cum_split_factor')).over(window_spec)))\n",
    "\n",
    "    adj = adj.withColumn('cum_split_factor', \n",
    "                        F.when(F.row_number().over(window_spec) == 1, 1.0)\n",
    "                        .otherwise(F.lag('cum_split_factor_no_lag', default=1.0).over(window_spec)))\n",
    "    \n",
    "    adj = adj.withColumn('split_adj_price', adj.p_price * adj.cum_split_factor)\n",
    "#     adj = adj.withColumn(\"split_adj_div\", (adj.p_divs_pd*adj.cum_split_factor))\n",
    "    \n",
    "#     adj = adj.withColumn(\"div_factor\", when(((adj.p_date == adj.p_divs_exdate) & (adj.p_divs_s_pd == 1)), lit(adj.split_adj_div)).otherwise(lit(0.0)))\n",
    "#     adj = adj.withColumn(\"temp_cum_spin_factor\", when((adj.split_adj_price - adj.div_factor <= 0), lit(1.0)) \\\n",
    "#                      .otherwise(lit((adj.split_adj_price - adj.div_factor)/adj.split_adj_price)))\n",
    "#     adj = adj.withColumn(\"cum_spin_factor\", lit(0.0))\n",
    "    \n",
    "#     adj = adj.withColumn('cum_spin_factor_no_lag', \n",
    "#                         F.exp(F.sum(F.log('temp_cum_spin_factor')).over(window_spec)))\n",
    "    \n",
    "#     adj = adj.withColumn('cum_spin_factor', \n",
    "#                         F.when(F.row_number().over(window_spec) == 1, 1.0)\n",
    "#                         .otherwise(F.lag('cum_spin_factor_no_lag').over(window_spec)))\n",
    "    \n",
    "#     adj = adj.withColumn(\"adj_price\", adj.split_adj_price * adj.cum_spin_factor)\n",
    "    \n",
    "    adj = adj.withColumn('Market_Value', col('split_adj_price') * col('p_com_shs_out'))\n",
    "    \n",
    "    adj = adj.withColumn('year', F.year('p_date'))\n",
    "    \n",
    "    adj =adj.withColumn('week_of_year', F.weekofyear('p_date'))\n",
    "\n",
    "    window_spec = Window.partitionBy('fsym_id', 'year', 'week_of_year').orderBy(col('p_date').desc())\n",
    "\n",
    "    adj = adj.withColumn('row_num', F.row_number().over(window_spec))\n",
    "\n",
    "    adj = adj.filter(col('row_num') == 1).orderBy('p_date')\n",
    "    adj = adj.drop('temp_cum_split_factor', 'cum_split_factor', 'cum_split_factor_no_lag', 'row_num', 'p_split_date', 'p_split_factor')\n",
    "    \n",
    "    adj_pd = adj.toPandas()\n",
    "    adj_pd['p_date'] = pd.to_datetime(adj_pd['p_date'])\n",
    "    list_to_plot = sorted(adj_pd['ticker_region'].unique().tolist())\n",
    "    \n",
    "    columns = 3\n",
    "    num_rows = math.ceil(len(list_to_plot) / columns)\n",
    "    fig, axs = plt.subplots(nrows=num_rows, ncols=columns, figsize=(15, 5*num_rows))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    i = 0\n",
    "    for t in list_to_plot:\n",
    "        temp_df = adj_pd[adj_pd['ticker_region']==t]\n",
    "        axs[i].plot(temp_df['p_date'], temp_df['split_adj_price'], label=t)\n",
    "        \n",
    "        imp_start_date = imp_df.loc[imp_df['ticker_region']==t, 'Implosion_Start_Date']\n",
    "        imp_end_date = imp_df.loc[imp_df['ticker_region']==t, 'Implosion_End_Date']\n",
    "        if imp_start_date is not None:\n",
    "            # filtered_temp_df = temp_df[(temp_df['p_date'] >= imp_start_date) & (temp_df['p_date'] <= imp_end_date)]\n",
    "            # vol = filtered_temp_df['split_adj_price'].std()\n",
    "            imp_start_date = pd.to_datetime(imp_start_date)\n",
    "            imp_end_date = pd.to_datetime(imp_end_date)\n",
    "            #filtered_temp_df = temp_df[(temp_df['p_date'] >= imp_start_date) & (temp_df['p_date'] <= imp_end_date)]\n",
    "            #print(filtered_temp_df.head())\n",
    "            axs[i].axvspan(imp_start_date, imp_end_date, alpha=0.5, color='blue')\n",
    "        axs[i].legend()\n",
    "        #axs[i].text(0.5, -0.1, f'Volatility: {vol:.2f}', ha='center', transform=axs[i].transAxes)\n",
    "        i+=1\n",
    "        \n",
    "    for i in range(len(list_to_plot), num_rows * columns):\n",
    "        fig.delaxes(axs.flatten()[i])\n",
    "    \n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(pic_name)\n",
    "    \n",
    "    \n",
    "def sample_plots():\n",
    "    imp_df = pd.read_csv('imploded_stocks_mv.csv', index_col=False)\n",
    "    imps_only = imp_df.loc[imp_df['Implosion_Start_Date'].notnull()]\n",
    "    imps_only = spark.createDataFrame(imps_only.tail(20))\n",
    "    plot_prices(imps_only, 'all_implosions_mv_subplots.png')\n",
    "    \n",
    "    \n",
    "\n",
    "#sample_plots()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f6b219a-a80f-4e06-b034-968b543ee281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_methods():\n",
    "    imp_df_price = pd.read_csv('imploded_stocks3.csv', index_col=False)\n",
    "    imp_df_mv = pd.read_csv('imploded_stocks_mv.csv', index_col=False)\n",
    "    imp_df_price = imp_df_price.loc[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "    imp_df_mv =  imp_df_mv.loc[imp_df_mv['Implosion_Start_Date'].notnull()]\n",
    "    print(\"Number imploded via Market Value: \", len(imp_df_mv))\n",
    "    print(\"Number imploded via Price: \", len(imp_df_price))\n",
    "    not_in_price = imp_df_mv[~imp_df_mv['ticker_region'].isin(imp_df_price['ticker_region'])]\n",
    "    not_in_mv = imp_df_price[~imp_df_price['ticker_region'].isin(imp_df_mv['ticker_region'])]\n",
    "    print(\"Number of stocks in price not in mv: \", len(not_in_mv))\n",
    "    print(\"Number of stocks in mv not in price: \", len(not_in_price))\n",
    "    print(\"Number of stocks in both: \", len(imp_df_price) - len(not_in_mv))\n",
    "    plot_prices(spark.createDataFrame(not_in_price.head(20)), 'imploded_mv_only.png')\n",
    "    plot_prices(spark.createDataFrame(not_in_mv.head(20)), 'imploded_price_only.png')\n",
    "    \n",
    "def compare_lengths():\n",
    "    imp_df_price = pd.read_csv('imploded_stocks3.csv', index_col=False)\n",
    "    imp_df_mv = pd.read_csv('imploded_stocks_mv.csv', index_col=False)\n",
    "    imp_df_price = imp_df_price.loc[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "    imp_df_mv =  imp_df_mv.loc[imp_df_mv['Implosion_Start_Date'].notnull()]\n",
    "    imp_df_price['Implosion_Start_Date'] = pd.to_datetime(imp_df_price['Implosion_Start_Date'])\n",
    "    imp_df_mv['Implosion_Start_Date'] = pd.to_datetime(imp_df_mv['Implosion_Start_Date'])\n",
    "    imp_df_price['Implosion_End_Date'] = pd.to_datetime(imp_df_price['Implosion_End_Date'])\n",
    "    imp_df_mv['Implosion_End_Date'] = pd.to_datetime(imp_df_mv['Implosion_End_Date'])\n",
    "    imp_df_price['imp_length'] = imp_df_price['Implosion_End_Date']-  imp_df_price['Implosion_Start_Date']\n",
    "    imp_df_mv['imp_length'] = imp_df_mv['Implosion_End_Date']-  imp_df_mv['Implosion_Start_Date']\n",
    "    print(\"Average implosion length (price): \", imp_df_price['imp_length'].mean())\n",
    "    print(\"Average implosion length (market val): \", imp_df_mv['imp_length'].mean())\n",
    "\n",
    "def create_implosion_price_plots():\n",
    "    imp_df_price = pd.read_csv('imploded_stocks3.csv', index_col=False)\n",
    "    test_stocks = ['CTRM-US', 'MNMD-US', 'CLDX-US', 'PHIO-US', 'EGOV-US', 'RRD-US', 'GFED-US', 'PTEK-US', \n",
    "                   'VHC-US', 'STRZB-US', 'NXPL-US', 'TANH-US', 'CTIC-US', 'DTSX-US', 'BLNK-US', 'XRTX-US', \n",
    "                   'ATOS-US', 'APAGF-US', 'VTNR-US', 'UONE-US', 'PIXY-US', 'SLRX-US', 'METG-US', 'CLZR-US', 'FET-US']\n",
    "    test_df = imp_df_price[imp_df_price['ticker_region'].isin(test_stocks)]\n",
    "    plot_prices(spark.createDataFrame(test_df), 'imploded_test_104p_52lb.png')\n",
    "    \n",
    "\n",
    "    \n",
    "# compare_methods()\n",
    "# compare_lengths()\n",
    "#create_implosion_price_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67904e3c-e2e3-46b9-8712-eb00fac01cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6a583-dda1-4294-8c8d-5814de60d4bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
