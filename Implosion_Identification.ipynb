{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1921cb03-1482-48a2-bbce-358a7fb395c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/opt/hadoop-3.2.2/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.7-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "2024-01-25 20:46:37,359 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2024-01-25 20:46:40,377 WARN spark.ExecutorAllocationManager: Dynamic allocation without a shuffle service is an experimental feature.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "\n",
    "# for shared metastore (shared across all users)\n",
    "spark = SparkSession.builder.appName(\"Identification\").config(\"hive.metastore.uris\", \"thrift://amok:9083\", conf=SparkConf()).getOrCreate() \\\n",
    "\n",
    "# for local metastore (your private, invidivual database) add the following config to spark session\n",
    "\n",
    "#spark.sql(\"USE 2023_04_01\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f5e73b-b8f6-47a7-9df0-9ec9e573bb5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"USE 2023_11_02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a125b38f-a981-4af4-bddf-f1f8d8b2c801",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-25 20:46:45.847362: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-01-25 20:46:45.847397: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-01-25 20:46:45.849291: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-01-25 20:46:45.858811: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-25 20:46:47.086004: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yfinance as yf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import norm\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, to_date, lit, udf, pandas_udf, PandasUDFType, coalesce, \\\n",
    "            month, year, concat, date_format, format_string, last_day, months_between, greatest, least, when, lag, count, desc, expr, log1p\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import timedelta\n",
    "from pyspark.sql.types import*\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import math\n",
    "#import pyspark.pandas as ps\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0ce3be3-0801-4665-bade-234c88d10ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from CreateDataset import get_fund_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "225740ab-45e8-4a9f-8f7c-7e3493970cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/group_ops.py:81: UserWarning: It is preferred to use 'applyInPandas' over this API. This API will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n",
      "                                                                                ]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff\n",
    "\n",
    "def get_all_stocks_df():\n",
    "    query = f\"\"\"SELECT s.ticker_region, s.fsym_id,p.p_date AS date, p.p_price AS price, splits.p_split_date, IF(ISNULL(splits.p_split_factor),1,splits.p_split_factor) AS split_factor, ms.p_com_shs_out\n",
    "                FROM sym_ticker_region s \n",
    "                LEFT JOIN FF_SEC_COVERAGE c ON c.fsym_id = s.fsym_id\n",
    "                LEFT JOIN sym_coverage sc ON sc.fsym_id = s.fsym_id\n",
    "                INNER JOIN fp_basic_prices p ON p.fsym_id = sc.fsym_regional_id\n",
    "                LEFT JOIN fp_basic_splits AS splits ON splits.p_split_date = p.p_date AND p.fsym_id = splits.fsym_id\n",
    "                LEFT JOIN fp_basic_dividends AS divs ON divs.p_divs_exdate = p.p_date AND p.fsym_id = divs.fsym_id\n",
    "                LEFT JOIN (SELECT sf.fsym_id, mp.price_date, sf.p_com_shs_out, sf.p_date AS shares_hist_date\n",
    "                                        FROM fp_basic_shares_hist AS sf\n",
    "                                        JOIN (SELECT p2.fsym_id, p2.p_date AS price_date, max(h.p_date) AS max_shares_hist_date\n",
    "                                                FROM fp_basic_prices AS p2\n",
    "                                                JOIN fp_basic_shares_hist AS h ON h.p_date <= p2.p_date AND p2.fsym_id = h.fsym_id\n",
    "                                                GROUP BY p2.fsym_id, p2.p_date)\n",
    "                                        mp ON mp.fsym_id = sf.fsym_id AND mp.max_shares_hist_date = sf.p_date)\n",
    "                            ms ON ms.fsym_id = p.fsym_id AND ms.price_date = p.p_date\n",
    "                WHERE s.ticker_region LIKE \"%-US\" AND s.ticker_region NOT LIKE '%.%' AND c.CURRENCY = \"USD\" \n",
    "                AND ms.p_com_shs_out IS NOT NULL\n",
    "                AND (sc.fref_listing_exchange = \"NAS\" OR sc.fref_listing_exchange = \"NYS\") AND p.p_date >= '2000-01-01'\n",
    "                ORDER BY s.fsym_id, p.p_date\"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "    adj = spark.sql(query)\n",
    "    div_query = \"\"\"SELECT fsym_id, p_divs_exdate AS date, SUM(p_divs_pd) AS div FROM fp_basic_dividends \n",
    "                    GROUP BY fsym_id, p_divs_exdate\n",
    "                    ORDER BY fsym_id, p_divs_exdate\"\"\"\n",
    "    df_div = spark.sql(div_query)\n",
    "    df_div = df_div.withColumn('weekday', F.dayofweek('date'))\n",
    "    dic = {1:2, 2:0, 3:0, 4:0, 5:0, 6:0, 7:1}\n",
    "    df_div = df_div.na.replace(dic, subset=\"weekday\")\n",
    "    df_div = df_div.withColumn('date_adj', F.expr(\"date_sub(date, weekday)\"))\n",
    "    df = adj.join(df_div, (adj.fsym_id == df_div.fsym_id) & (adj.date == df_div.date_adj), how='left') \\\n",
    "          .select(adj.fsym_id, adj.date, adj.price, df_div.div, adj.split_factor, adj.p_com_shs_out) \\\n",
    "          .na.fill(0, subset='div') \\\n",
    "          .orderBy(F.col('fsym_id').asc(), F.col('date').desc())\n",
    "    window_spec = Window.partitionBy(\"fsym_id\").orderBy(F.desc(\"date\"))\n",
    "\n",
    "    df = df.withColumn(\"cum_split\", F.lag(F.exp(F.sum(F.log(\"split_factor\")).over(window_spec))).over(window_spec))\n",
    "    df = df.na.fill(1, subset='cum_split') # Set cumulative split factor of latest date to 1\n",
    "\n",
    "    # Split-adjusted price and dividends\n",
    "    df = df.withColumn(\"price_split_adj\", F.col('price') * F.col('cum_split'))\n",
    "    df = df.withColumn(\"div_split_adj\", F.col('div') * F.col('cum_split'))\n",
    "\n",
    "    # Dividend factor\n",
    "    df = df.withColumn('div_factor', (F.col('price_split_adj') - F.lag('div_split_adj').over(window_spec))/F.col('price_split_adj'))\n",
    "    df = df.na.fill(1, subset='div_factor') # Set dividend factor of latest date to 1\n",
    "\n",
    "    # Cumulative dividend factor\n",
    "    df = df.withColumn(\"cum_div\", F.exp(F.sum(F.log(\"div_factor\")).over(window_spec)))\n",
    "    df = df.na.fill(1, subset='cum_div') # Set cumulative dividend factor of latest date to 1\n",
    "\n",
    "    # Price adjusted for splits and dividends\n",
    "    df = df.withColumn('adj_price', F.col('price_split_adj') * F.col('cum_div'))\n",
    "    df = df.withColumn('market_value', F.col('adj_price') * F.col('p_com_shs_out'))\n",
    "    df = df.withColumn('weekly_return', df.adj_price / F.lead(df.adj_price).over(window_spec))\n",
    "    df = df.orderBy('fsym_id','date')\n",
    "    \n",
    "\n",
    "                         \n",
    "    \n",
    "    df = df.withColumn('year', F.year('date'))\n",
    "    \n",
    "    df = df.withColumn('week_of_year', F.weekofyear('date'))\n",
    "\n",
    "    window_spec = Window.partitionBy('fsym_id', 'year', 'week_of_year').orderBy(col('date').desc())\n",
    "\n",
    "    df = df.withColumn('row_num', F.row_number().over(window_spec))\n",
    "\n",
    "    df = df.filter(col('row_num') == 1)\n",
    "    df = df.select('fsym_id', 'date', 'adj_price', 'weekly_return', 'market_value').orderBy('fsym_id','date')\n",
    "    # df.show()\n",
    "    \n",
    "#     adj = adj.drop('temp_cum_split_factor', 'cum_split_factor', 'cum_split_factor_no_lag', 'row_num', 'p_split_date', 'p_split_factor', 'split_adj_price', \n",
    "#               'cum_spin_factor', 'temp_cum_spin_factor', 'div_factor', 'split_adj_div', 'div_split_factor')\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    df = df.withColumn('Implosion_Start_Date', F.lit(None).cast(DateType()))\n",
    "    df = df.withColumn('Implosion_End_Date', F.lit(None).cast(DateType()))\n",
    "    df = df.withColumn('Num_Implosions', F.lit(None).cast(\"int\"))\n",
    "    # adj = adj.withColumn('Market_Value', col('split_adj_price') * col('p_com_shs_out'))\n",
    "    \n",
    "    window_spec2 = Window().partitionBy('fsym_id').orderBy('date').rowsBetween(-52, Window.currentRow)\n",
    "    df = df.withColumn('rolling_mean', F.avg('adj_price').over(window_spec2))\n",
    "#     window_spec3 = Window().partitionBy('fsym_id', 'year').orderBy('p_date')\n",
    "#     adj = adj.withColumn('mv_per_year', F.avg('Market_Value').over(window_spec3))\n",
    "#     adj = adj.withColumn('mv_prev_year', F.lag('Market_Value').over(window_spec3))\n",
    "    \n",
    "    \n",
    "    def implosion_wrapper(adj, price_drop_thresh=-0.6, period_thresh=78, increase_thresh=0.0):\n",
    "    \n",
    "        @pandas_udf(adj.schema, FloatType(), PandasUDFType.GROUPED_MAP)\n",
    "        def find_implosion_price(df1):\n",
    "            max_price = df1['adj_price'].max()\n",
    "            if max_price < 100:\n",
    "                return df1\n",
    "            fsym_id = df1.loc[0,'fsym_id']\n",
    "            df1 = df1.sort_values(by='date')\n",
    "            i = 52 #lookback\n",
    "            imp_dates = []\n",
    "            while i < len(df1):\n",
    "                current_date = df1.loc[i, 'date']\n",
    "                current_price = df1.loc[i, 'adj_price']\n",
    "                mean_price = df1.loc[i, 'rolling_mean']\n",
    "                if (current_price - mean_price)/mean_price < price_drop_thresh:\n",
    "                    j = i\n",
    "                    start_price=current_price\n",
    "                    j+=1\n",
    "                    imp_period=0\n",
    "                    while j < len(df1) and (df1.loc[j, 'adj_price'] - start_price) / start_price <= increase_thresh:\n",
    "                        imp_period+=1\n",
    "                        j+=1\n",
    "                    if imp_period > period_thresh:\n",
    "                        imp_dates.append((current_date, df1.loc[i+imp_period, 'date']))\n",
    "                    i+=imp_period\n",
    "                i+=1\n",
    "            if len(imp_dates)>0:\n",
    "                df1.loc[df1['fsym_id']==fsym_id, 'Implosion_Start_Date'] = imp_dates[0][0]\n",
    "                df1.loc[df1['fsym_id']==fsym_id, 'Implosion_End_Date'] = imp_dates[0][1]\n",
    "                df1.loc[df1['fsym_id']==fsym_id, 'Num_Implosions'] = len(imp_dates)\n",
    "\n",
    "            return df1\n",
    "        adj_price = adj.groupBy('fsym_id').apply(find_implosion_price)\n",
    "        return adj_price\n",
    "    \n",
    "    def crash_wrapper(adj, price_drop_thresh=-0.8, period_thresh=78, increase_thresh=0.5):\n",
    "    \n",
    "        @pandas_udf(adj.schema, FloatType(), PandasUDFType.GROUPED_MAP)\n",
    "        def find_crash(df1):\n",
    "            \n",
    "            max_price = df1['adj_price'].max()\n",
    "            fsym_id = df1.loc[0,'fsym_id']\n",
    "            df1 = df1.sort_values(by='date')\n",
    "            i = 1 #lookback\n",
    "            imp_dates = []\n",
    "            while i < len(df1):\n",
    "                current_date = df1.loc[i, 'date']\n",
    "                current_return = df1.loc[i, 'weekly_return']\n",
    "                current_price = df1.loc[i, 'adj_price']\n",
    "                if current_return < -0.6:\n",
    "                    j = i\n",
    "                    start_price=current_price\n",
    "                    j+=1\n",
    "                    imp_period=0\n",
    "                    while j < len(df1) and (df1.loc[j, 'adj_price'] - start_price) / start_price <= increase_thresh:\n",
    "                        imp_period+=1\n",
    "                        j+=1\n",
    "                    if imp_period > period_thresh:\n",
    "                        imp_dates.append((current_date, df1.loc[i+imp_period, 'date']))\n",
    "                    i+=imp_period\n",
    "                i+=1\n",
    "            if len(imp_dates)>0:\n",
    "                df1.loc[df1['fsym_id']==fsym_id, 'Implosion_Start_Date'] = imp_dates[0][0]\n",
    "                df1.loc[df1['fsym_id']==fsym_id, 'Implosion_End_Date'] = imp_dates[0][1]\n",
    "                df1.loc[df1['fsym_id']==fsym_id, 'Num_Implosions'] = len(imp_dates)\n",
    "\n",
    "            return df1\n",
    "        adj_price = adj.groupBy('fsym_id').apply(find_crash)\n",
    "        return adj_price\n",
    "    \n",
    "#     def mkt_val_wrapper(adj, mv_drop_thresh=-0.6, period_thresh = 78, inc_thresh = 0.0):\n",
    "\n",
    "#         @pandas_udf(adj.schema, FloatType(), PandasUDFType.GROUPED_MAP)\n",
    "#         def find_implosion_mv(df1):\n",
    "#             fsym_id = df1.loc[0,'fsym_id']\n",
    "#             df1 = df1.sort_values(by='date')\n",
    "#             max_price = df1['adj_price'].max()\n",
    "#             if max_price < 100:\n",
    "#                 return df1\n",
    "#             i = 0\n",
    "#             imp_dates = []\n",
    "#             while i < len(df1)-52:\n",
    "#                 current_date = df1.loc[i, 'date']\n",
    "#                 current_mv = df1.loc[i, 'market_value']\n",
    "#                 next_year_mv = df1.loc[i+52, 'market_value']\n",
    "#                 if (next_year_mv - curr_mv)/curr_mv < mv_drop_thresh:\n",
    "#                     j = i\n",
    "#                     start_mv = next_year_mv\n",
    "#                     j+=1\n",
    "#                     imp_period=0\n",
    "#                     while j < len(df1) and (df1.loc[j, 'Market_Value'] - start_mv) / start_mv <= inc_thresh:\n",
    "#                         imp_period+=1\n",
    "#                         j+=1\n",
    "#                     if imp_period > period_thresh:\n",
    "#                         imp_dates.append((current_date, df1.loc[i+imp_period, 'date']))\n",
    "#                     i+=imp_period\n",
    "#                 i+=1\n",
    "#             if len(imp_dates)>0:\n",
    "#                 df1.loc[df1['fsym_id']==fsym_id, 'Implosion_Start_Date'] = imp_dates[0][0]\n",
    "#                 df1.loc[df1['fsym_id']==fsym_id, 'Implosion_End_Date'] = imp_dates[0][1]\n",
    "#                 df1.loc[df1['fsym_id']==fsym_id, 'Num_Implosions'] = len(imp_dates)\n",
    "\n",
    "#             return df1\n",
    "        \n",
    "    def drawdown_wrapper(adj, dd_drop = -0.7, period = 78, inc_thresh = 0.0):\n",
    "        @pandas_udf(adj.schema, FloatType(), PandasUDFType.GROUPED_MAP)\n",
    "        def get_stock_dds(ds):\n",
    "            max_price = ds['adj_price'].max()\n",
    "            if len(ds)==0 or max_price < 100:\n",
    "                return ds\n",
    "            fsym_id = ds.loc[0,'fsym_id']\n",
    "            ds=ds.set_index('date')\n",
    "            pmin_pmax = (ds['adj_price'].diff(-1) > 0).astype(int).diff() #<- -1 indicates pmin, +1 indicates pmax\n",
    "            pmax = pmin_pmax[pmin_pmax == 1]\n",
    "            pmin = pmin_pmax[pmin_pmax == -1]\n",
    "            if len(pmin) == 0 or len(pmax)==0:\n",
    "                ds.reset_index(inplace=True)\n",
    "                return ds\n",
    "            if pmin.index[0] < pmax.index[0]:\n",
    "                pmin = pmin.drop(pmin.index[0])\n",
    "            if pmin.index[-1] < pmax.index[-1]:\n",
    "                pmax = pmax.drop(pmax.index[-1])\n",
    "\n",
    "            dd = (np.array(ds['adj_price'][pmin.index]) - np.array(ds['adj_price'][pmax.index])) \\\n",
    "                / np.array(ds['adj_price'][pmax.index])\n",
    "\n",
    "            dur = [np.busday_count(p1, p2) for p1, p2 in zip(pmax.index, pmin.index)]\n",
    "            d = {'Date':pmax.index, 'drawdown':dd, 'd_start': pmax.index, 'd_end': pmin.index, \\\n",
    "                    'duration': dur}  \n",
    "\n",
    "            df_d = pd.DataFrame(d).set_index('Date')\n",
    "            df_d.index = pd.to_datetime(df_d.index, format='%Y/%m/%d')\n",
    "            df_d = df_d.sort_values(by='drawdown')\n",
    "            df_d['rank'] = list(range(1,df_d.shape[0]+1))\n",
    "\n",
    "            major_dds = df_d[df_d['drawdown'] < dd_drop]\n",
    "            imp_dates = []\n",
    "            if len(major_dds)==0:\n",
    "                ds.reset_index(inplace=True)\n",
    "                return ds\n",
    "                \n",
    "            for i in range(len(major_dds)):\n",
    "                dd = major_dds.iloc[i]\n",
    "                impl_start = dd['d_start']\n",
    "                current_date = dd['d_end']\n",
    "                start_price = ds.loc[dd['d_end'], 'adj_price']\n",
    "                imp_period = 0\n",
    "    \n",
    "                curr_idx = ds.index.get_loc(current_date)\n",
    "                start_idx = curr_idx\n",
    "            \n",
    "                for idx in ds.index:\n",
    "                    row = ds.loc[idx]\n",
    "                    if (row['adj_price'] - start_price) / start_price <= inc_thresh:\n",
    "                        imp_period+=1\n",
    "                if imp_period > period:\n",
    "                    imp_dates.append((impl_start, ds.index[start_idx+min(imp_period,len(ds)-start_idx-1)]))\n",
    "                    \n",
    "            if len(imp_dates)>0:\n",
    "                ds.loc[ds['fsym_id']==fsym_id, 'Implosion_Start_Date'] = imp_dates[0][0]\n",
    "                ds.loc[ds['fsym_id']==fsym_id, 'Implosion_End_Date'] = imp_dates[0][1]\n",
    "                ds.loc[ds['fsym_id']==fsym_id, 'Num_Implosions'] = len(imp_dates)\n",
    "            ds.reset_index(inplace=True)\n",
    "            return ds\n",
    "                \n",
    "                \n",
    "        adj_price = adj.groupBy('fsym_id').apply(get_stock_dds)\n",
    "        return adj_price\n",
    "    \n",
    "   \n",
    "                \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "#     df = implosion_wrapper(df)\n",
    "#     result_df = df.groupBy('fsym_id').agg(F.first('Implosion_Start_Date').alias('Implosion_Start_Date'), F.first('Implosion_End_Date').alias('Implosion_End_Date'))\n",
    "#     result_df.show(2)\n",
    "#     result_df.toPandas().to_csv('imploded_stocks_price.csv')\n",
    "    \n",
    "#     df = mkt_val_wrapper(df)\n",
    "#     result_df = df.groupBy('fsym_id').agg(F.first('Implosion_Start_Date').alias('Implosion_Start_Date'), F.first('Implosion_End_Date').alias('Implosion_End_Date'))\n",
    "#     result_df.show(2)\n",
    "#     result_df.toPandas().to_csv('imploded_stocks_mv.csv')\n",
    "\n",
    "    df = drawdown_wrapper(df)\n",
    "    result_df = df.groupBy('fsym_id').agg(F.first('Implosion_Start_Date').alias('Implosion_Start_Date'), F.first('Implosion_End_Date').alias('Implosion_End_Date'))\n",
    "    filtered_df = result_df.filter(col(\"Implosion_Start_Date\").isNotNull())\n",
    "    # result_df.show(2)\n",
    "    result_df.toPandas().to_csv('imploded_stocks_dd.csv')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def find_optimal_params(method):\n",
    "        price_drop_threshs = [-0.6, -0.8]\n",
    "        period_threshs= [78, 104]\n",
    "        increase_threshs = [0.0, 0.2]\n",
    "        results = []\n",
    "        for price_drop in price_drop_threshs:\n",
    "            for period in period_threshs:\n",
    "                for inc in increase_threshs:\n",
    "                    if method == 'price_drop':\n",
    "                        adj_price = implosion_wrapper(df, price_drop, period, inc)\n",
    "                    elif method == 'mv_drop':\n",
    "                        adj_price = mkt_val_wrapper(df, price_drop, period, inc)\n",
    "                    elif method == 'dd':\n",
    "                        adj_price = drawdown_wrapper(df, price_drop, period, inc)\n",
    "                    result_df_price = adj_price.groupBy('fsym_id').agg(F.first('Implosion_Start_Date').alias('Implosion_Start_Date'), F.first('Implosion_End_Date').alias('Implosion_End_Date'), F.first('Num_Implosions').alias('Num_Implosions'))\n",
    "                    filtered_df = result_df_price.filter(col(\"Implosion_Start_Date\").isNotNull())\n",
    "                    filtered_df = filtered_df.withColumn('imp_length', datediff(col('Implosion_Start_Date'), col('Implosion_End_Date')))\n",
    "                    mean_length = filtered_df.agg(F.mean('imp_length')).collect()[0][0]\n",
    "                    mean_implosions = filtered_df.agg(F.mean('Num_Implosions')).collect()[0][0]\n",
    "                    results.append({'price_drop':price_drop, 'period_thresh':period, 'increase_thresh':inc, \"num_imploded\":filtered_df.count(), 'avg_imp_len' : mean_length, 'imp_per_stock':mean_implosions})\n",
    "                    print(\"Working\\n\")\n",
    "                    \n",
    "                    \n",
    "        return results\n",
    "    \n",
    "    \n",
    "    #adj_mv = adj.groupBy('fsym_id').apply(find_implosion_mv)\n",
    "    #result_df_mv = adj_mv.groupBy('fsym_id').agg(F.first('Implosion_Start_Date').alias('Implosion_Start_Date'), F.first('Implosion_End_Date').alias('Implosion_End_Date'))\n",
    "    #filtered_df2 = result_df_mv.filter(col(\"Implosion_Start_Date\").isNull())\n",
    "    #joined_df = filtered_df1.join(filtered_df2, \"fsym_id\", \"inner\")\n",
    "    \n",
    "    \n",
    "#     print(\"Number imploded by price metric: \", filtered_df1.count())\n",
    "#     print(\"Imploded market val: \", filtered_df2.count()) \n",
    "#     print(\"Common stocks: \", joined_df.count())\n",
    "#     print(\"Imploded only (price): \", filtered_df1.count() - joined_df.count())\n",
    "#     print(\"Imploded only (market val): \", filtered_df2.count() - joined_df.count())\n",
    "    \n",
    "#     result_df_price.toPandas().to_csv('imploded_stocks_price.csv')\n",
    "#     result_df_mv.toPandas().to_csv('imploded_stocks_mv.csv')\n",
    "    \n",
    "    \n",
    "\n",
    "    # exp1_results = find_optimal_params('dd')\n",
    "    # results_df = pd.DataFrame(exp1_results)\n",
    "    # print(results_df.head())\n",
    "    # results_df.to_csv('exp1_results_dd.csv')\n",
    "    \n",
    "    \n",
    "    \n",
    "get_all_stocks_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "040885d9-6cf9-4ef5-a572-84487a765118",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10826\n",
      "936\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "print(len(df))\n",
    "df = df[df['Implosion_Start_Date'].notnull()]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b38b5b76-9d0e-4307-b8f0-01550d28807f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10826\n",
      "824\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('imploded_stocks_dd.csv', index_col=False)\n",
    "print(len(df))\n",
    "df = df[df['Implosion_Start_Date'].notnull()]\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce9b515-98a7-469f-a097-03d81cf7b875",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "[Stage 1837:====================================>               (139 + 9) / 200]]\r"
     ]
    }
   ],
   "source": [
    "def plot_prices(imp_df, pic_name):\n",
    "    adj = get_fund_data(imp_df)\n",
    "    imp_df = imp_df.toPandas()\n",
    "    adj_pd = adj.toPandas()\n",
    "    adj_pd['date'] = pd.to_datetime(adj_pd['date'])\n",
    "    list_to_plot = sorted(adj_pd['fsym_id'].unique().tolist())\n",
    "    \n",
    "    columns = 8\n",
    "    num_rows = math.ceil(len(list_to_plot) / columns)\n",
    "    fig, axs = plt.subplots(nrows=num_rows, ncols=columns, figsize=(35, 5*num_rows))\n",
    "    axs = axs.flatten()\n",
    "    \n",
    "    i = 0\n",
    "    for t in list_to_plot:\n",
    "        temp_df = adj_pd[adj_pd['fsym_id']==t]\n",
    "        axs[i].plot(temp_df['date'], temp_df['adj_price'], label=t)\n",
    "        # axs[i].plot(temp_df['date'], temp_df['adj_price'].rolling(52).mean(), label=f'{t} (52-week)')\n",
    "        # axs[i].plot(temp_df['date'], temp_df['adj_price'].rolling(5).mean(), label=f'{t} (5-week)')\n",
    "        \n",
    "        # axs[i].plot(temp_df['date'], temp_df['adj_price'].ewm(span=7, adjust=False).mean())\n",
    "        # axs[i].plot(temp_df['date'], temp_df['adj_price'].ewm(span=52, adjust=False).mean())\n",
    "        \n",
    "        imp_start_date = imp_df.loc[imp_df['fsym_id']==t, 'Implosion_Start_Date'].iloc[0]\n",
    "        imp_end_date = imp_df.loc[imp_df['fsym_id']==t, 'Implosion_End_Date'].iloc[0]\n",
    "        if imp_start_date is not None:\n",
    "            # filtered_temp_df = temp_df[(temp_df['p_date'] >= imp_start_date) & (temp_df['p_date'] <= imp_end_date)]\n",
    "            # vol = filtered_temp_df['split_adj_price'].std()\n",
    "            imp_start_date = pd.to_datetime(imp_start_date)\n",
    "            imp_end_date = pd.to_datetime(imp_end_date)\n",
    "            #filtered_temp_df = temp_df[(temp_df['p_date'] >= imp_start_date) & (temp_df['p_date'] <= imp_end_date)]\n",
    "            #print(filtered_temp_df.head())\n",
    "            axs[i].axvspan(imp_start_date, imp_end_date, alpha=0.5, color='blue')\n",
    "        axs[i].legend()\n",
    "        #axs[i].text(0.5, -0.1, f'Volatility: {vol:.2f}', ha='center', transform=axs[i].transAxes)\n",
    "        i+=1\n",
    "        \n",
    "    for i in range(len(list_to_plot), num_rows * columns):\n",
    "        fig.delaxes(axs.flatten()[i])\n",
    "    \n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(pic_name)\n",
    "    \n",
    "    \n",
    "def sample_plots():\n",
    "    imp_df = pd.read_csv('imploded_stocks_dd.csv', index_col=False)\n",
    "    imps_only = imp_df.loc[imp_df['Implosion_Start_Date'].notnull()]\n",
    "    imps_only = spark.createDataFrame(imps_only.head(20))\n",
    "    plot_prices(imps_only, 'sample_implosions_dd.png')\n",
    "    \n",
    "def sample_plots_2022():\n",
    "    imp_df = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "    imps_only = imp_df.loc[imp_df['Implosion_Start_Date'].notnull()]\n",
    "    imps_only['Implosion_Start_Date'] = pd.to_datetime(imps_only['Implosion_Start_Date'])\n",
    "    imps_only['year'] = imps_only['Implosion_Start_Date'].dt.year\n",
    "    imps_only = imps_only[imps_only['year']==2022]\n",
    "    imps_only = spark.createDataFrame(imps_only.tail(40))\n",
    "    plot_prices(imps_only, 'sample_implosions_2022.png')\n",
    "    compare_with_yahoo(imps_only)\n",
    "\n",
    "def number_imploded():\n",
    "    imp_df = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "    imps_only = imp_df.loc[imp_df['Implosion_Start_Date'].notnull()]\n",
    "    print(\"Number imploded: \", len(imps_only))\n",
    "    \n",
    "def visualize_implosions():\n",
    "    df = pd.read_csv('imploded_stocks_dd.csv', index_col=False)\n",
    "\n",
    "    # Check for missing values in 'Implosion_Start_Date' column\n",
    "    if df['Implosion_Start_Date'].isnull().any():\n",
    "        # Drop rows with missing values in 'Implosion_Start_Date'\n",
    "        df = df.dropna(subset=['Implosion_Start_Date'])\n",
    "    \n",
    "    # Convert 'Implosion_Start_Date' to datetime and extract only the year\n",
    "    df['Implosion_Date'] = pd.to_datetime(df['Implosion_Start_Date']).dt.year\n",
    "\n",
    "    implosions_per_year = df.groupby('Implosion_Date').size()\n",
    "\n",
    "    implosions_per_year.plot(kind='bar', color='skyblue')\n",
    "\n",
    "    plt.title('Number of Implosions per Year')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Implosions')\n",
    "    plt.savefig('Implosions_per_year.png')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "def compare_with_yahoo(df):\n",
    "    df.createOrReplaceTempView('temp')\n",
    "    q = \"\"\"SELECT str.ticker_region, t.fsym_id FROM temp t LEFT JOIN sym_ticker_region str ON str.fsym_id = t.fsym_id\"\"\"\n",
    "    ticker_df = spark.sql(q)\n",
    "    ticker_df = ticker_df.toPandas()\n",
    "    list_to_plot = sorted(ticker_df['ticker_region'].tolist())\n",
    "    \n",
    "    columns = 3\n",
    "    num_rows = math.ceil(len(list_to_plot) / columns)\n",
    "    fig, axs = plt.subplots(nrows=num_rows, ncols=columns, figsize=(15, 5*num_rows))\n",
    "    axs = axs.flatten()\n",
    "    i=0\n",
    "    for t in list_to_plot:\n",
    "        stock_data = yf.download(t[:-3], start='2001-01-01', end='2023-11-02')\n",
    "        if not stock_data.empty:\n",
    "            stock_data = stock_data['Adj Close'].resample('W').last()\n",
    "            axs[i].plot(stock_data.index, stock_data, label=ticker_df.loc[ticker_df['ticker_region'] == t, 'fsym_id'])\n",
    "            axs[i].legend()\n",
    "        i+=1\n",
    "            \n",
    "    for i in range(len(list_to_plot), num_rows * columns):\n",
    "        fig.delaxes(axs.flatten()[i])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('yahoo_stocks.png')\n",
    "    \n",
    "\n",
    "sample_plots()\n",
    "visualize_implosions()\n",
    "# sample_plots_2022()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f6b219a-a80f-4e06-b034-968b543ee281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_methods():\n",
    "    imp_df_price = pd.read_csv('imploded_stock4.csv', index_col=False)\n",
    "    imp_df_mv = pd.read_csv('imploded_stocks_mv.csv', index_col=False)\n",
    "    imp_df_price = imp_df_price.loc[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "    imp_df_mv =  imp_df_mv.loc[imp_df_mv['Implosion_Start_Date'].notnull()]\n",
    "    print(\"Number imploded via Market Value: \", len(imp_df_mv))\n",
    "    print(\"Number imploded via Price: \", len(imp_df_price))\n",
    "    not_in_price = imp_df_mv[~imp_df_mv['ticker_region'].isin(imp_df_price['ticker_region'])]\n",
    "    not_in_mv = imp_df_price[~imp_df_price['ticker_region'].isin(imp_df_mv['ticker_region'])]\n",
    "    print(\"Number of stocks in price not in mv: \", len(not_in_mv))\n",
    "    print(\"Number of stocks in mv not in price: \", len(not_in_price))\n",
    "    print(\"Number of stocks in both: \", len(imp_df_price) - len(not_in_mv))\n",
    "    plot_prices(spark.createDataFrame(not_in_price.head(20)), 'imploded_mv_mv_only.png')\n",
    "    plot_prices(spark.createDataFrame(not_in_mv.head(20)), 'imploded_mv_price_only.png')\n",
    "    \n",
    "def compare_lengths():\n",
    "    imp_df_price = pd.read_csv('imploded_stocks3.csv', index_col=False)\n",
    "    imp_df_mv = pd.read_csv('imploded_stocks_mv.csv', index_col=False)\n",
    "    imp_df_price = imp_df_price.loc[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "    imp_df_mv =  imp_df_mv.loc[imp_df_mv['Implosion_Start_Date'].notnull()]\n",
    "    imp_df_price['Implosion_Start_Date'] = pd.to_datetime(imp_df_price['Implosion_Start_Date'])\n",
    "    imp_df_mv['Implosion_Start_Date'] = pd.to_datetime(imp_df_mv['Implosion_Start_Date'])\n",
    "    imp_df_price['Implosion_End_Date'] = pd.to_datetime(imp_df_price['Implosion_End_Date'])\n",
    "    imp_df_mv['Implosion_End_Date'] = pd.to_datetime(imp_df_mv['Implosion_End_Date'])\n",
    "    imp_df_price['imp_length'] = imp_df_price['Implosion_End_Date']-  imp_df_price['Implosion_Start_Date']\n",
    "    imp_df_mv['imp_length'] = imp_df_mv['Implosion_End_Date']-  imp_df_mv['Implosion_Start_Date']\n",
    "    print(\"Average implosion length (price): \", imp_df_price['imp_length'].mean())\n",
    "    print(\"Average implosion length (market val): \", imp_df_mv['imp_length'].mean())\n",
    "\n",
    "def create_implosion_price_plots():\n",
    "    imp_df_price = pd.read_csv('imploded_stocks_price.csv', index_col=False)\n",
    "    imp_df_price = imp_df_price.loc[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "    imp_df_price['Implosion_Start_Date'] = pd.to_datetime(imp_df_price['Implosion_Start_Date'])\n",
    "    imp_df_2001 = imp_df_price[imp_df_price['Implosion_Start_Date'].dt.year==2001]\n",
    "    imp_df_2018 = imp_df_price[imp_df_price['Implosion_Start_Date'].dt.year==2018]\n",
    "    plot_prices(spark.createDataFrame(imp_df_2001.head(50)), '2001_implosions.png')\n",
    "    plot_prices(spark.createDataFrame(imp_df_2018.head(50)), '2018_implosions.png')\n",
    "    \n",
    "def compare_type_of_stocks_captured():\n",
    "    imp_df_price = pd.read_csv('imploded_stock4.csv', index_col=False)\n",
    "    imp_df_mv = pd.read_csv('imploded_stocks_mv.csv', index_col=False)\n",
    "    imp_df_price = imp_df_price.loc[imp_df_price['Implosion_Start_Date'].notnull()]\n",
    "    imp_df_mv =  imp_df_mv.loc[imp_df_mv['Implosion_Start_Date'].notnull()]\n",
    "    imp_df_price_series = get_fund_data(imp_df_price)\n",
    "    imp_df_mv_series = get_fund_data(imp_df_mv)\n",
    "    window_spec = Window.partitionBy('fsym_id').orderBy(col('p_date').desc())\n",
    "    imp_df_price_series = imp_df_price_series.withColumn('row_num', F.row_number().over(window_spec))\n",
    "    imp_df_mv_series = imp_df_mv_series.withColumn('row_num', F.row_number().over(window_spec))\n",
    "    \n",
    "    first_per_stock_price = imp_df_price_series.filter(col('row_num') == 1)\n",
    "    num_small_cap_price = first_per_stock_price.filter(col('Market_Value') <= 2000000).count()\n",
    "    num_small_cap_mv = first_per_stock_mv.filter(col('Market_Value') <= 2000000).count()\n",
    "    \n",
    "def stock_start_analysis():\n",
    "    stocks_df = pd.read_csv('imploded_stocks_price.csv')\n",
    "    stocks_df['Implosion_Start_Date'] = pd.to_datetime(stocks_df['Implosion_Start_Date'])\n",
    "    stocks_df['Implosion_End_Date'] = pd.to_datetime(stocks_df['Implosion_End_Date'])\n",
    "    stocks_df = spark.createDataFrame(stocks_df)\n",
    "    stocks_df.createOrReplaceTempView(\"temp_table\")\n",
    "    query = f\"\"\"SELECT t.fsym_id, c.p_first_date FROM temp_table t LEFT JOIN fp_sec_coverage c ON c.fsym_id=t.fsym_id\n",
    "                    ORDER BY t.fsym_id\n",
    "            \"\"\"\n",
    "    start_df = spark.sql(query)\n",
    "    start_df = start_df.toPandas()\n",
    "    start_df['p_first_date'] = pd.to_datetime(start_df['p_first_date'])\n",
    "    start_df['Year'] = start_df['p_first_date'].dt.year\n",
    "    print(start_df.head())\n",
    "    starts_per_year = start_df.groupby('Year').size()\n",
    "    print(starts_per_year)\n",
    "    \n",
    "    plt.figure(figsize=(10,5))\n",
    "    starts_per_year.plot(kind='bar')\n",
    "    plt.title('Earliest dates of stocks')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Stocks')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# stock_start_analysis()  \n",
    "# compare_methods()\n",
    "# compare_lengths()\n",
    "#create_implosion_price_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2259edbd-7316-41b1-a382-dd8f765671c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import calendar\n",
    "\n",
    "def industry_analysis_all():\n",
    "    #stock_df = get_all_stocks_df()\n",
    "    stock_df = pd.read_csv('imploded_stocks_price.csv')\n",
    "    stock_df = stock_df.loc[stock_df['Implosion_Start_Date'].notnull()]\n",
    "    print(len(stock_df))\n",
    "    stock_df = spark.createDataFrame(stock_df)\n",
    "    stock_df.createOrReplaceTempView(\"temp_table\")\n",
    "    q2 = \"\"\"SELECT t.fsym_id, t.Implosion_Start_Date, e.factset_industry_desc FROM temp_table t\n",
    "    LEFT JOIN sym_coverage sc ON sc.fsym_id = t.fsym_id\n",
    "    LEFT JOIN ff_sec_entity_hist c on c.fsym_id=sc.fsym_security_id\n",
    "    LEFT JOIN sym_entity_sector d on d.factset_entity_id=c.factset_entity_id\n",
    "    LEFT JOIN factset_industry_map e on e.factset_industry_code=d.industry_code\n",
    "    ORDER BY t.fsym_id\n",
    "    \"\"\"\n",
    "    ind_df = spark.sql(q2)\n",
    "    ind_df = ind_df.toPandas()\n",
    "    ind_df['Implosion_Date'] = pd.to_datetime(ind_df['Implosion_Start_Date'])\n",
    "    ind_df['year'] = ind_df['Implosion_Date'].dt.year\n",
    "    null_count = ind_df['factset_industry_desc'].isnull().sum()\n",
    "    print(\"Number with no industry label: \", null_count)\n",
    "    \n",
    "    ind_df_grp = ind_df.groupby(['year', 'factset_industry_desc']).size().reset_index(name='count')\n",
    "\n",
    "    ind_df_grp = ind_df_grp[ind_df_grp['count'] >= 10]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='year', y='count', hue='factset_industry_desc', data=ind_df_grp)\n",
    "\n",
    "    plt.title('Imploded Stocks by Industry and Year')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Stocks')\n",
    "    plt.legend(title='Industry', bbox_to_anchor=(1, 1), loc='upper left')\n",
    "    \n",
    "    ind_df_year = ind_df.groupby('year').size().reset_index(name='count')\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='year', y='count', data=ind_df_year)\n",
    "\n",
    "    plt.title('Imploded Stocks by Industry and Year')\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Stocks')\n",
    "    plt.show()\n",
    "\n",
    "def industry_plots(industry):\n",
    "    stock_df = pd.read_csv('imploded_stocks_price.csv')\n",
    "    stock_df = stock_df.loc[stock_df['Implosion_Start_Date'].notnull()]\n",
    "    print(len(stock_df))\n",
    "    stock_df = spark.createDataFrame(stock_df)\n",
    "    stock_df.createOrReplaceTempView(\"temp_table\")\n",
    "    q2 = f\"\"\"SELECT t.fsym_id, t.Implosion_Start_Date, t.Implosion_End_Date, e.factset_industry_desc FROM temp_table t\n",
    "    LEFT JOIN sym_coverage sc ON sc.fsym_id = t.fsym_id\n",
    "    LEFT JOIN ff_sec_entity_hist c on c.fsym_id=sc.fsym_security_id\n",
    "    LEFT JOIN sym_entity_sector d on d.factset_entity_id=c.factset_entity_id\n",
    "    LEFT JOIN factset_industry_map e on e.factset_industry_code=d.industry_code\n",
    "    WHERE e.factset_industry_desc = \"{industry}\"\n",
    "    ORDER BY t.fsym_id\n",
    "    \"\"\"\n",
    "    ind_df = spark.sql(q2)\n",
    "    plot_prices(ind_df.limit(100), f\"{industry}_imploded.png\")\n",
    "    \n",
    "    \n",
    "def examine_year():\n",
    "    stock_df = pd.read_csv('imploded_stocks_price.csv')\n",
    "    stock_df = stock_df.loc[stock_df['Implosion_Start_Date'].notnull()]\n",
    "    stock_df = spark.createDataFrame(stock_df)\n",
    "    stock_df.createOrReplaceTempView(\"temp_table\")\n",
    "    q2 = \"\"\"SELECT t.fsym_id, t.Implosion_Start_Date, e.factset_industry_desc FROM temp_table t\n",
    "    LEFT JOIN sym_coverage sc ON sc.fsym_id = t.fsym_id\n",
    "    LEFT JOIN ff_sec_entity_hist c on c.fsym_id=sc.fsym_security_id\n",
    "    LEFT JOIN sym_entity_sector d on d.factset_entity_id=c.factset_entity_id\n",
    "    LEFT JOIN factset_industry_map e on e.factset_industry_code=d.industry_code\n",
    "    ORDER BY t.fsym_id\n",
    "    \"\"\"\n",
    "    ind_df = spark.sql(q2)\n",
    "    ind_df = ind_df.toPandas()\n",
    "    ind_df['Implosion_Start_Date'] = pd.to_datetime(ind_df['Implosion_Start_Date'])\n",
    "    ind_df['year'] = ind_df['Implosion_Start_Date'].dt.year\n",
    "    ind_df_year = ind_df[ind_df['year'] == 2022]\n",
    "    ind_df_year['month'] = ind_df_year['Implosion_Start_Date'].dt.month\n",
    "    ind_df_year['month_name'] = ind_df_year['Implosion_Start_Date'].dt.month_name()\n",
    "\n",
    "    # Specify the order of months\n",
    "    month_order = list(calendar.month_name)[1:]  # List of month names excluding the empty string\n",
    "\n",
    "    ind_df_grp = ind_df_year.groupby(['month_name']).size().reindex(month_order, fill_value=0).reset_index(name='count')\n",
    "    print(ind_df_grp.head())\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='month_name', y='count', data=ind_df_grp, color='skyblue', order=month_order)\n",
    "\n",
    "    plt.title('Imploded Stocks in 2022')\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Number of Stocks')\n",
    "    plt.show()\n",
    "\n",
    "def mkt_val_per_year():\n",
    "    stock_df = pd.read_csv('imploded_stocks_price.csv')\n",
    "    stock_df = stock_df.loc[stock_df['Implosion_Start_Date'].notnull()]\n",
    "    stock_df = spark.createDataFrame(stock_df)\n",
    "    stock_series = get_fund_data(stock_df)\n",
    "    stock_series.show()\n",
    "    stock_series = stock_series.join(stock_df.select(\"fsym_id\", \"Implosion_Start_Date\"), \"fsym_id\", \"inner\")\n",
    "    stock_series = stock_series.withColumn('year',F.year('Implosion_Start_Date'))\n",
    "    window_spec = Window.partitionBy('fsym_id', 'year').orderBy(col('date'))\n",
    "    stock_series = stock_series.withColumn('Max_Market_Value', F.max('Market_Value').over(window_spec))\n",
    "    \n",
    "    \n",
    "    filtered_df = stock_series.filter(F.col('Max_Market_Value') < 2e9)\n",
    "\n",
    "    grouped_df = filtered_df.groupBy('year').agg(F.countDistinct('fsym_id').alias('num_stocks'))\n",
    "\n",
    "    pandas_df = grouped_df.toPandas()\n",
    "\n",
    "    plt.bar(pandas_df['year'], pandas_df['num_stocks'])\n",
    "    plt.xlabel('Year')\n",
    "    plt.ylabel('Number of Stocks with Market Val < $2bn')\n",
    "    plt.title('Number of Stocks with Market Val < $2bn')\n",
    "    plt.show()\n",
    "\n",
    "# industry_analysis_all()\n",
    "# examine_year()\n",
    "# mkt_val_per_year()\n",
    "# industry_plots('Pharmaceuticals: Major')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "153ea4a7-d25c-4963-8d64-7d07913be2b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_stock_price(ticker):\n",
    "    # Fetching data from Yahoo Finance\n",
    "    print(ticker[:-3])\n",
    "    stock_data = yf.download(ticker[:-3], start='2001-01-01', end='2023-11-02')\n",
    "    if not stock_data.empty:\n",
    "        stock_data = stock_data['Adj Close'].resample('W').last()\n",
    "        print(stock_data.head())\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(stock_data.index, stock_data)\n",
    "        plt.title(f'{ticker} Stock Price Over Time')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Price')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "def compare_prices():\n",
    "    unique_values = ['AAPL-US', 'ABIO-US', 'PTEK-US', 'AIG-US']\n",
    "    for u in unique_values:\n",
    "        q = f\"\"\"SELECT str.fsym_id FROM sym_ticker_region str WHERE str.ticker_region = \"{u}\" \"\"\"\n",
    "        qdf = spark.sql(q)\n",
    "        price_df = get_fund_data(qdf)\n",
    "        price_df.show()\n",
    "        #id_t = yf_ticker_df.select('ticker_region').first()[0]\n",
    "        stock_data = price_df.select('p_date', 'adj_price').orderBy('p_date').toPandas()\n",
    "\n",
    "        # Convert 'p_date' to datetime if needed\n",
    "        stock_data['p_date'] = pd.to_datetime(stock_data['p_date'])\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(stock_data['p_date'], stock_data['adj_price'])\n",
    "        plt.title('Stock Price Over Time')\n",
    "        plt.xlabel('Date')\n",
    "        plt.ylabel('Price')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        plot_stock_price(u)\n",
    "\n",
    "# tickers_feats = spark.createDataFrame(df[:5])\n",
    "# sp_df = get_fund_data(tickers_feats)\n",
    "#compare_prices()\n",
    "#plot_stock_price('BHZYXG-R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6658f02-5d01-421f-9140-63f5be576d44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/pandas/conversion.py:331: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     plt\u001b[38;5;241m.\u001b[39mgrid(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m---> 27\u001b[0m \u001b[43mzscore_plots\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 6\u001b[0m, in \u001b[0;36mzscore_plots\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m stock_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(stock_df)\n\u001b[1;32m      5\u001b[0m stock_series \u001b[38;5;241m=\u001b[39m get_fund_data(stock_df)\n\u001b[0;32m----> 6\u001b[0m \u001b[43mstock_series\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m stock_series \u001b[38;5;241m=\u001b[39m stock_series\u001b[38;5;241m.\u001b[39mjoin(stock_df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfsym_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImplosion_Start_Date\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfsym_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m stock_series\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:484\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    442\u001b[0m \n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;124;03m name | Bob\u001b[39;00m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;28mint\u001b[39m(truncate), vertical))\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1303\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1296\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1033\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1033\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py:1200\u001b[0m, in \u001b[0;36mGatewayConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[1;32m   1197\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError while sending\u001b[39m\u001b[38;5;124m\"\u001b[39m, e, proto\u001b[38;5;241m.\u001b[39mERROR_ON_SEND)\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1200\u001b[0m     answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m   1201\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m   1202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m answer\u001b[38;5;241m.\u001b[39mstartswith(proto\u001b[38;5;241m.\u001b[39mRETURN_MESSAGE):\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def zscore_plots():\n",
    "    stock_df = pd.read_csv('imploded_stocks_price.csv')\n",
    "    stock_df = stock_df.loc[stock_df['Implosion_Start_Date'].notnull()]\n",
    "    stock_df = spark.createDataFrame(stock_df)\n",
    "    stock_series = get_fund_data(stock_df)\n",
    "    stock_series.show()\n",
    "    stock_series = stock_series.join(stock_df.select(\"fsym_id\", \"Implosion_Start_Date\"), \"fsym_id\", \"inner\")\n",
    "    stock_series.show()\n",
    "    stock_series.createOrReplaceTempView('temp')\n",
    "    q =f\"\"\"SELECT t.fsym_id, t.date, t.Implosion_Start_Date, t.adj_price, a.date, a.FF_ZSCORE FROM temp t LEFT JOIN FF_ADVANCED_DER_QF a ON YEAR(a.date) = YEAR(t.date)\n",
    "                AND MONTH(t.date) = MONTH(a.date) AND t.fsym_id = a.fsym_id\"\"\"\n",
    "    z_df = spark.sql(q)\n",
    "    z_df.show()\n",
    "    \n",
    "#     # Convert 'p_date' to datetime if needed\n",
    "    z_df.toPandas()\n",
    "    z_df['date'] = pd.to_datetime(stock_data['p_date'])\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(stock_data['p_date'], stock_data['adj_price'])\n",
    "    plt.title('Stock Price Over Time')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Price')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "# zscore_plots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67904e3c-e2e3-46b9-8712-eb00fac01cf4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae6a583-dda1-4294-8c8d-5814de60d4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#idk where to put this back up code:\n",
    "#     adj = adj.withColumn(\"temp_cum_split_factor\", when(adj.p_date==adj.p_split_date, lit(adj.p_split_factor)).otherwise(lit(1.0)))\n",
    "#     adj = adj.withColumn(\"div_split_factor\", lit(0.0)) # placeholders\n",
    "#     adj = adj.withColumn(\"cum_split_factor\", lit(0.0)) # placeholders\n",
    "\n",
    "#     window_spec = Window.partitionBy('fsym_id').orderBy(F.desc('p_date'))\n",
    "\n",
    "#     # Calculate cumulative split factor and dividend split factor\n",
    "#     #adj = adj.withColumn('split_temp_i', F.row_number().over(window_spec) - 1)\n",
    "#     adj = adj.withColumn('cum_split_factor_no_lag', \n",
    "#                         F.exp(F.sum(F.log('temp_cum_split_factor')).over(window_spec)))\n",
    "#     adj = adj.withColumn('div_split_factor', \n",
    "#                         F.exp(F.sum(F.log('temp_cum_split_factor')).over(window_spec)))\n",
    "#     adj = adj.withColumn('div_split_factor', \n",
    "#                         F.when(F.row_number().over(window_spec) == 1, 1.0)\n",
    "#                         .otherwise('div_split_factor'))\n",
    "#     adj = adj.withColumn('cum_split_factor', \n",
    "#                         F.when(F.row_number().over(window_spec) == 1, 1.0)\n",
    "#                         .otherwise(F.lag('cum_split_factor_no_lag', default=1.0).over(window_spec)))\n",
    "    \n",
    "#     adj = adj.withColumn('split_adj_price', adj.p_price * adj.cum_split_factor)\n",
    "#     adj = adj.withColumn('split_adj_div', adj.p_divs_pd * adj.cum_split_factor)\n",
    "    \n",
    "#     adj = adj.withColumn(\"div_factor\", when(((adj.p_date == adj.p_divs_exdate) & (adj.p_divs_s_pd == 1)), lit(adj.split_adj_div)).otherwise(lit(0.0)))\n",
    "    \n",
    "#     adj = adj.withColumn(\"temp_cum_spin_factor\", when((adj.split_adj_price - adj.div_factor <= 0), lit(1.0)).otherwise(lit((adj.split_adj_price - adj.div_factor)/adj.split_adj_price)))\n",
    "    \n",
    "#     adj = adj.withColumn(\"cum_spin_factor\", lit(0.0))\n",
    "    \n",
    "#     adj = adj.withColumn('cum_spin_factor', \n",
    "#                         F.exp(F.sum(F.log('temp_cum_spin_factor')).over(window_spec)))\n",
    "    \n",
    "#     adj = adj.withColumn('cum_spin_factor', \n",
    "#                         F.when(F.row_number().over(window_spec) == 1, 1.0)\n",
    "#                         .otherwise(F.lag('cum_spin_factor', default=1.0).over(window_spec)))\n",
    "    \n",
    "#     adj = adj.withColumn(\"adj_price\", adj.split_adj_price * adj.cum_spin_factor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
